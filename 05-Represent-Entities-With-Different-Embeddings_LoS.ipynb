{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tvTwvOrlwvyW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATAPATH = \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\""],"metadata":{"id":"xGTAurDETQIW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S01QXtUwwdfv"},"outputs":[],"source":["import pandas as pd\n","import os\n","import numpy as np\n","from gensim.models import Word2Vec, FastText\n","#import glove\n","#from glove import Corpus\n","\n","import collections\n","import gc \n","\n","import keras\n","from keras import backend as K\n","from keras import regularizers\n","from keras.models import Sequential, Model\n","from keras.layers import Flatten, Dense, Dropout, Input, concatenate, merge, Activation, Concatenate, LSTM, GRU\n","from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\n","from keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D, merge\n","\n","#from keras.optimizers import Adam\n","\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n","from keras.utils import np_utils\n","#from keras.backend.tensorflow_backend import set_session, clear_session, get_session\n","from keras.backend import set_session, clear_session, get_session\n","import tensorflow as tf\n","\n","\n","from sklearn.utils import class_weight\n","from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6oormoyqwdf1"},"outputs":[],"source":["\n","new_notes = pd.read_pickle(os.path.join(DATAPATH, \"ner_df_final_los.p\")) # med7\n","w2vec = Word2Vec.load(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/embeddings/word2vec.model\")\n","fasttext = FastText.load(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/embeddings/fasttext.model\")\n","#word2vec = gensim.models.KeyedVectors.load('word2vec.model')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzJxksN9wdf2","executionInfo":{"status":"ok","timestamp":1651116120484,"user_tz":300,"elapsed":581,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}},"outputId":"22c22637-865c-4a70-9bbd-4a2b58de80e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["119519\n"]}],"source":["null_index_list = []\n","for i in new_notes.itertuples():\n","    \n","    if len(i.ner) == 0:\n","    #if not i.ner:\n","        null_index_list.append(i.Index)\n","new_notes.drop(null_index_list, inplace=True)\n","print(len(new_notes))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E95rIQpXwdf3","executionInfo":{"status":"ok","timestamp":1651116141032,"user_tz":300,"elapsed":20552,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}},"outputId":"7d60dfb4-c8eb-494b-8394-422ce645f890"},"outputs":[{"output_type":"stream","name":"stdout","text":["21837\n"]}],"source":["med7_ner_data = {}\n","\n","for ii in new_notes.itertuples():\n","    \n","    p_id = ii.SUBJECT_ID\n","    ind = ii.Index\n","    \n","    try:\n","        new_ner = new_notes.loc[ind].ner\n","    except:\n","        new_ner = []\n","            \n","    unique = set()\n","    new_temp = []\n","    \n","    for j in new_ner:\n","        for k in j:\n","            \n","            unique.add(k[0])\n","            new_temp.append(k)\n","\n","    if p_id in med7_ner_data:\n","        for i in new_temp:\n","            med7_ner_data[p_id].append(i)\n","    else:\n","        med7_ner_data[p_id] = new_temp\n","print(len(med7_ner_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGO-3Xy-wdf4"},"outputs":[],"source":["pd.to_pickle(med7_ner_data, (os.path.join(DATAPATH, \"new_ner_word_dict_los.pkl\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3SbPzMowdf5"},"outputs":[],"source":["def mean(a):\n","    return sum(a) / len(a)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06wOJ1U4wdf6","executionInfo":{"status":"ok","timestamp":1651116358349,"user_tz":300,"elapsed":215827,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}},"outputId":"5decefa8-2146-44b8-eb3b-52ec6ee2c7c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["w2vec starting..\n","21731\n","fasttext starting..\n","21809\n","combined starting..\n","21731 21809 21837\n"]}],"source":["data_types = [med7_ner_data]\n","data_names = [\"new_ner\"]\n","\n","for data, names in zip(data_types, data_names):\n","    new_word2vec = {}\n","    print(\"w2vec starting..\")\n","    for k,v in data.items():\n","\n","        patient_temp = []\n","        for i in v:\n","            try:\n","                patient_temp.append(w2vec[i[0]])\n","            except:\n","                avg = []\n","                num = 0\n","                temp = []\n","\n","                if len(i[0].split(\" \")) > 1:\n","                    for each_word in i[0].split(\" \"):\n","                        try:\n","                            temp = w2vec[each_word]\n","                            avg.append(temp)\n","                            num += 1\n","                        except:\n","                            pass\n","                    if num == 0: continue\n","                    avg = np.asarray(avg)\n","                    #t = np.asarray(map(mean, zip(*avg)))\n","                    t = np.asarray(list(map(mean, zip(*avg))))\n","                    patient_temp.append(t)\n","        if len(patient_temp) == 0: continue\n","        new_word2vec[k] = patient_temp\n","    print(len(new_word2vec))\n","    #############################################################################\n","    print(\"fasttext starting..\")\n","        \n","    new_fasttextvec = {}\n","\n","    for k,v in data.items():\n","\n","        patient_temp = []\n","\n","        for i in v:\n","            try:\n","                patient_temp.append(fasttext[i[0]])\n","            except:\n","                pass\n","        if len(patient_temp) == 0: continue\n","        new_fasttextvec[k] = patient_temp\n","    print(len(new_fasttextvec))\n","    #############################################################################    \n","\n","    print(\"combined starting..\")\n","    new_concatvec = {}\n","\n","    for k,v in data.items():\n","        patient_temp = []\n","    #     if k != 6: continue\n","        for i in v:\n","            w2vec_temp = []\n","            fasttemp = [] # nitin added \n","            try:\n","                w2vec_temp = w2vec[i[0]]\n","            except:\n","                avg = []\n","                num = 0\n","                temp = []\n","\n","                if len(i[0].split(\" \")) > 1:\n","                    for each_word in i[0].split(\" \"):\n","                        try:\n","                            temp = w2vec[each_word]\n","                            avg.append(temp)\n","                            num += 1\n","                        except:\n","                            pass\n","                    if num == 0: \n","                        #w2vec_temp = [0] * 100\n","                        w2vec_temp = [0] * 200\n","                    else:\n","                        avg = np.asarray(avg)\n","                        w2vec_temp = np.asarray(list(map(mean, zip(*avg))))  \n","                else:\n","                    #w2vec_temp = [0] * 100\n","                    w2vec_temp = [0] * 200\n","            #print('value for i',i)    \n","            try:\n","                fasttemp = fasttext[i[0]]\n","            except:\n","                #fasttemp = [0] * 100\n","                fasttemp = [0] * 200\n","            #print('fasttemp {0}, w2vec_temp{1},'.format(fasttemp,w2vec_temp))\n","            appended = np.append(fasttemp, w2vec_temp, 0)\n","            patient_temp.append(appended)\n","        if len(patient_temp) == 0: continue\n","        new_concatvec[k] = patient_temp\n","\n","\n","\n","    #print(len(new_word2vec), len(new_fasttextvec))\n","    print(len(new_word2vec), len(new_fasttextvec), len(new_concatvec))\n","    \n","    pd.to_pickle(new_word2vec, (os.path.join(DATAPATH, \"new_ner_word2vec_dict_los.pkl\")))\n","    pd.to_pickle(new_fasttextvec, (os.path.join(DATAPATH, \"new_ner_fasttext_dict_los.pkl\")))\n","    pd.to_pickle(new_concatvec, (os.path.join(DATAPATH, \"new_ner_combined_dict_los.pkl\")))\n"]},{"cell_type":"code","source":["new_word2vec_dict = pd.read_pickle(os.path.join(DATAPATH, \"new_ner_word2vec_dict_los.pkl\"))\n","new_fasttext_dict = pd.read_pickle(os.path.join(DATAPATH, \"new_ner_fasttext_dict_los.pkl\"))\n","new_combined_dict = pd.read_pickle(os.path.join(DATAPATH, \"new_ner_combined_dict_los.pkl\"))\n","\n","diff = set(new_fasttext_dict.keys()).difference(set(new_word2vec_dict))\n","for i in diff:\n","    del new_fasttext_dict[i]\n","    del new_combined_dict[i]\n","print (len(new_word2vec_dict), len(new_fasttext_dict), len(new_combined_dict))\n","\n","pd.to_pickle(new_word2vec_dict, (os.path.join(DATAPATH, \"new_ner_word2vec_limited_dict_los.pkl\")))\n","pd.to_pickle(new_fasttext_dict, (os.path.join(DATAPATH, \"new_ner_fasttext_limited_dict_los.pkl\")))\n","pd.to_pickle(new_combined_dict, (os.path.join(DATAPATH, \"new_ner_combined_limited_dict_los.pkl\")))"],"metadata":{"id":"YvCenPzdDk7D","executionInfo":{"status":"ok","timestamp":1651116403907,"user_tz":300,"elapsed":45567,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"51dc70cf-a8b8-4384-8c39-c2a317b0fdfe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21731 21731 21759\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"05-Represent-Entities-With-Different-Embeddings_LoS.ipynb","provenance":[{"file_id":"1RaRHM1cJhL79J2XMAFDOKmz_p2dRCuWP","timestamp":1651206739602},{"file_id":"1q77LzBKaSFNVxY4KJGCIuABuCbz0qkJL","timestamp":1650732034329},{"file_id":"1p7XCSF22Xv0rAUBi5XshXtqDhowvu4ks","timestamp":1649799600818},{"file_id":"1OAgfEwQSo5jSse74gsIH4cv3aVD3RVIw","timestamp":1649619327189}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}