{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tvTwvOrlwvyW","executionInfo":{"status":"ok","timestamp":1649988765922,"user_tz":300,"elapsed":16089,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}},"outputId":"752e6daa-b9cc-486a-c030-359a050014e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S01QXtUwwdfv"},"outputs":[],"source":["import pandas as pd\n","import os\n","import numpy as np\n","from gensim.models import Word2Vec, FastText\n","#import glove\n","#from glove import Corpus\n","\n","import collections\n","import gc \n","\n","import keras\n","from keras import backend as K\n","from keras import regularizers\n","from keras.models import Sequential, Model\n","from keras.layers import Flatten, Dense, Dropout, Input, concatenate, merge, Activation, Concatenate, LSTM, GRU\n","from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\n","from keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D, merge\n","\n","#from keras.optimizers import Adam\n","\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n","from keras.utils import np_utils\n","#from keras.backend.tensorflow_backend import set_session, clear_session, get_session\n","from keras.backend import set_session, clear_session, get_session\n","import tensorflow as tf\n","\n","\n","from sklearn.utils import class_weight\n","from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6oormoyqwdf1"},"outputs":[],"source":["new_notes = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/ner_df_final.p\") # med7\n","w2vec = Word2Vec.load(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/embeddings/word2vec.model\")\n","fasttext = FastText.load(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/embeddings/fasttext.model\")\n","#word2vec = gensim.models.KeyedVectors.load('word2vec.model')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzJxksN9wdf2","executionInfo":{"status":"ok","timestamp":1649988798773,"user_tz":300,"elapsed":630,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}},"outputId":"365fa5f4-386d-4337-ca82-e5a5d08f1725"},"outputs":[{"output_type":"stream","name":"stdout","text":["119519\n"]}],"source":["null_index_list = []\n","for i in new_notes.itertuples():\n","    \n","    if len(i.ner) == 0:\n","    #if not i.ner:\n","        null_index_list.append(i.Index)\n","new_notes.drop(null_index_list, inplace=True)\n","print(len(new_notes))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E95rIQpXwdf3","executionInfo":{"status":"ok","timestamp":1649988821765,"user_tz":300,"elapsed":20912,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}},"outputId":"d6527ecb-ded5-44ea-fe88-07ec0b230050"},"outputs":[{"output_type":"stream","name":"stdout","text":["21837\n"]}],"source":["med7_ner_data = {}\n","\n","for ii in new_notes.itertuples():\n","    \n","    p_id = ii.SUBJECT_ID\n","    ind = ii.Index\n","    \n","    try:\n","        new_ner = new_notes.loc[ind].ner\n","    except:\n","        new_ner = []\n","            \n","    unique = set()\n","    new_temp = []\n","    \n","    for j in new_ner:\n","        for k in j:\n","            \n","            unique.add(k[0])\n","            new_temp.append(k)\n","\n","    if p_id in med7_ner_data:\n","        for i in new_temp:\n","            med7_ner_data[p_id].append(i)\n","    else:\n","        med7_ner_data[p_id] = new_temp\n","print(len(med7_ner_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGO-3Xy-wdf4"},"outputs":[],"source":["pd.to_pickle(med7_ner_data, \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner_word_dict.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3SbPzMowdf5"},"outputs":[],"source":["def mean(a):\n","    return sum(a) / len(a)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06wOJ1U4wdf6","executionInfo":{"status":"ok","timestamp":1649993538681,"user_tz":300,"elapsed":219290,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}},"outputId":"6402e943-ab5c-4ba4-b2c3-9ad9660b144a"},"outputs":[{"output_type":"stream","name":"stdout","text":["w2vec starting..\n","21731\n","fasttext starting..\n","21809\n","combined starting..\n","21731 21809 21837\n"]}],"source":["data_types = [med7_ner_data]\n","data_names = [\"new_ner\"]\n","\n","for data, names in zip(data_types, data_names):\n","    new_word2vec = {}\n","    print(\"w2vec starting..\")\n","    for k,v in data.items():\n","\n","        patient_temp = []\n","        for i in v:\n","            try:\n","                patient_temp.append(w2vec[i[0]])\n","            except:\n","                avg = []\n","                num = 0\n","                temp = []\n","\n","                if len(i[0].split(\" \")) > 1:\n","                    for each_word in i[0].split(\" \"):\n","                        try:\n","                            temp = w2vec[each_word]\n","                            avg.append(temp)\n","                            num += 1\n","                        except:\n","                            pass\n","                    if num == 0: continue\n","                    avg = np.asarray(avg)\n","                    #t = np.asarray(map(mean, zip(*avg)))\n","                    t = np.asarray(list(map(mean, zip(*avg))))\n","                    patient_temp.append(t)\n","        if len(patient_temp) == 0: continue\n","        new_word2vec[k] = patient_temp\n","    print(len(new_word2vec))\n","    #############################################################################\n","    print(\"fasttext starting..\")\n","        \n","    new_fasttextvec = {}\n","\n","    for k,v in data.items():\n","\n","        patient_temp = []\n","\n","        for i in v:\n","            try:\n","                patient_temp.append(fasttext[i[0]])\n","            except:\n","                pass\n","        if len(patient_temp) == 0: continue\n","        new_fasttextvec[k] = patient_temp\n","    print(len(new_fasttextvec))\n","    #############################################################################    \n","\n","    print(\"combined starting..\")\n","    new_concatvec = {}\n","\n","    for k,v in data.items():\n","        patient_temp = []\n","    #     if k != 6: continue\n","        for i in v:\n","            w2vec_temp = []\n","            fasttemp = [] # nitin added \n","            try:\n","                w2vec_temp = w2vec[i[0]]\n","            except:\n","                avg = []\n","                num = 0\n","                temp = []\n","\n","                if len(i[0].split(\" \")) > 1:\n","                    for each_word in i[0].split(\" \"):\n","                        try:\n","                            temp = w2vec[each_word]\n","                            avg.append(temp)\n","                            num += 1\n","                        except:\n","                            pass\n","                    if num == 0: \n","                        #w2vec_temp = [0] * 100\n","                        w2vec_temp = [0] * 200\n","                    else:\n","                        avg = np.asarray(avg)\n","                        w2vec_temp = np.asarray(list(map(mean, zip(*avg)))) # nitin added list after map for fixing the return of map as hash \n","                else:\n","                    #w2vec_temp = [0] * 100\n","                    w2vec_temp = [0] * 200\n","            #print('value for i',i)    # nitin added the try pass to eliminate the error if i[[0]] doesn't exist\n","            try:\n","                fasttemp = fasttext[i[0]]\n","            except:\n","                #fasttemp = [0] * 100\n","                fasttemp = [0] * 200\n","            #print('fasttemp {0}, w2vec_temp{1},'.format(fasttemp,w2vec_temp))\n","            appended = np.append(fasttemp, w2vec_temp, 0)\n","            patient_temp.append(appended)\n","        if len(patient_temp) == 0: continue\n","        new_concatvec[k] = patient_temp\n","\n","\n","\n","    #print(len(new_word2vec), len(new_fasttextvec))\n","    print(len(new_word2vec), len(new_fasttextvec), len(new_concatvec))\n","    #new_word2vec\n","    #new_fasttextvec\n","    pd.to_pickle(new_word2vec, \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner5_word2vec_dict.pkl\")\n","    pd.to_pickle(new_fasttextvec, \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner5_fasttext_dict.pkl\")\n","    pd.to_pickle(new_concatvec, \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner5_combined_dict.pkl\")"]},{"cell_type":"code","source":["#print(\"new_word2vec\", new_word2vec[0])\n","new_word2vec_dict = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner5_word2vec_dict.pkl\")\n","new_fasttext_dict = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner5_fasttext_dict.pkl\")\n","new_combined_dict = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner5_combined_dict.pkl\")\n","diff = set(new_fasttext_dict.keys()).difference(set(new_word2vec_dict))\n","for i in diff:\n","    del new_fasttext_dict[i]\n","    del new_combined_dict[i]\n","print (len(new_word2vec_dict), len(new_fasttext_dict), len(new_combined_dict))\n","#print (len(new_word2vec_dict), len(new_fasttext_dict))\n","\n","\n","pd.to_pickle(new_word2vec_dict, \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+\"new_ner5_word2vec_limited_dict.pkl\")\n","pd.to_pickle(new_fasttext_dict, \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+\"new_ner5_fasttext_limited_dict.pkl\")\n","pd.to_pickle(new_fasttext_dict, \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+\"new_ner5_combined_limited_dict.pkl\")\n","#pd.to_pickle(new_combined_dict, \"data/\"+\"new_ner\"+\"_combined_limited_dict.pkl\")"],"metadata":{"id":"YvCenPzdDk7D","executionInfo":{"status":"ok","timestamp":1649993625312,"user_tz":300,"elapsed":45100,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c25d656d-81ab-4add-b977-7369b863be6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["21731 21731 21759\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"pKwdcxSwaFn3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVpt6AQ2EGdS"},"outputs":[],"source":["lvl2_train =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/lvl2_imputer_train.pkl\")\n","lvl2_dev =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/lvl2_imputer_dev.pkl\")\n","lvl2_test =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/lvl2_imputer_test.pkl\")\n","\n","Ys =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/Ys.pkl\")\n","Ys_train =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/Ys_train.pkl\")\n","Ys_dev =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/Ys_dev.pkl\")\n","Ys_test =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/Ys_test.pkl\")"]},{"cell_type":"code","source":[""],"metadata":{"id":"fRxg6SsSaISu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uxy2Bi0aEGdT"},"outputs":[],"source":["all_train_ids = set()\n","for i in Ys_train.itertuples():\n","    all_train_ids.add( i.Index[0] )\n","    \n","all_dev_ids = set()\n","for i in Ys_dev.itertuples():\n","    all_dev_ids.add( i.Index[0] )\n","    \n","all_test_ids = set()\n","for i in Ys_test.itertuples():\n","    all_test_ids.add( i.Index[0] )"]},{"cell_type":"code","source":[""],"metadata":{"id":"NYRBDYzjaL0N"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"juzKhgPyEGdV","executionInfo":{"status":"ok","timestamp":1649989239241,"user_tz":300,"elapsed":191,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}},"outputId":"1739b12c-3f6d-44fd-f985-74c955bc6f87"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.07159904534606205\n","0.06725146198830409\n","0.07432150313152401\n","====\n","0.10680190930787589\n","0.09649122807017543\n","0.10855949895615867\n","====\n","0.43323389021479713\n","0.42105263157894735\n","0.4246346555323591\n","====\n","0.07696897374701671\n","0.07268170426065163\n","0.07954070981210856\n"]}],"source":["print (sum(Ys_train.mort_icu.values)*1.0 / len(Ys_train.mort_icu.values))\n","print (sum(Ys_dev.mort_icu.values)*1.0 / len(Ys_dev.mort_icu.values))\n","print (sum(Ys_test.mort_icu.values)*1.0 / len(Ys_test.mort_icu.values))\n","print (\"====\")\n","print (sum(Ys_train.mort_hosp.values)*1.0 / len(Ys_train.mort_hosp.values))\n","print (sum(Ys_dev.mort_hosp.values)*1.0 / len(Ys_dev.mort_hosp.values))\n","print (sum(Ys_test.mort_hosp.values)*1.0 / len(Ys_test.mort_hosp.values))\n","print (\"====\")\n","print (sum(Ys_train.los_3.values)*1.0 / len(Ys_train.los_3.values))\n","print (sum(Ys_dev.los_3.values)*1.0 / len(Ys_dev.los_3.values))\n","print (sum(Ys_test.los_3.values)*1.0 / len(Ys_test.los_3.values))\n","print (\"====\")\n","print (sum(Ys_train.los_7.values)*1.0 / len(Ys_train.los_7.values))\n","print (sum(Ys_dev.los_7.values)*1.0 / len(Ys_dev.los_7.values))\n","print (sum(Ys_test.los_7.values)*1.0 / len(Ys_test.los_7.values))"]},{"cell_type":"code","source":[""],"metadata":{"id":"cW_FhW1taP2G"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQFydDIdEGdX"},"outputs":[],"source":["new_word2vec_dict = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner4_word2vec_dict.pkl\")\n","#new_word2vec_dict = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner_word_dict.pkl\")\n","new_keys = set(new_word2vec_dict.keys())\n","new_train_ids = sorted(all_train_ids.intersection(new_keys))\n","new_dev_ids = sorted(all_dev_ids.intersection(new_keys))\n","new_test_ids = sorted(all_test_ids.intersection(new_keys))"]},{"cell_type":"code","source":[""],"metadata":{"id":"xS4wmiZLaV5Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iXxm_PjvEGdZ"},"outputs":[],"source":["#new_train_ids = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_train_ids.pkl\")\n","#new_dev_ids = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_dev_ids.pkl\")\n","#new_test_ids = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_test_ids.pkl\")"]},{"cell_type":"code","source":["print(\"new_train_ids = \", new_train_ids)\n","print(\"new_dev_ids = \", new_dev_ids)\n","print(\"new_test_ids = \", new_test_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lS7cbRm0aWi8","executionInfo":{"status":"ok","timestamp":1649989291328,"user_tz":300,"elapsed":181,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}},"outputId":"082cb0c9-84ea-4d95-db2c-6b0c2bac30fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["new_train_ids =  [3, 9, 12, 13, 17, 19, 21, 25, 30, 31, 35, 41, 45, 52, 55, 56, 61, 62, 64, 65, 68, 71, 78, 81, 83, 85, 88, 97, 99, 100, 101, 103, 105, 106, 114, 115, 123, 124, 127, 129, 130, 133, 134, 135, 137, 140, 141, 143, 144, 146, 147, 149, 152, 160, 163, 169, 170, 171, 173, 174, 175, 177, 178, 186, 191, 195, 201, 202, 205, 208, 209, 211, 212, 214, 222, 225, 226, 228, 234, 238, 242, 245, 249, 251, 253, 255, 256, 261, 262, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 279, 281, 287, 290, 298, 301, 302, 305, 306, 307, 309, 310, 313, 314, 315, 318, 319, 321, 323, 326, 329, 330, 335, 344, 345, 346, 347, 350, 351, 353, 354, 356, 360, 364, 366, 367, 370, 371, 379, 383, 389, 391, 394, 395, 396, 397, 400, 402, 406, 407, 409, 417, 418, 420, 421, 422, 424, 429, 434, 436, 437, 438, 439, 440, 445, 448, 452, 453, 458, 462, 464, 466, 468, 471, 472, 477, 481, 482, 485, 487, 489, 491, 492, 498, 503, 507, 510, 518, 523, 533, 536, 538, 539, 540, 543, 545, 554, 557, 558, 559, 561, 564, 569, 577, 580, 586, 590, 593, 599, 600, 601, 603, 605, 612, 614, 616, 624, 634, 638, 639, 642, 650, 653, 654, 655, 657, 663, 667, 671, 672, 674, 676, 678, 680, 681, 689, 690, 695, 699, 700, 705, 707, 708, 710, 715, 716, 719, 721, 723, 727, 729, 733, 740, 741, 743, 745, 747, 749, 750, 757, 766, 767, 771, 772, 774, 776, 779, 786, 787, 788, 793, 794, 797, 799, 805, 806, 808, 811, 813, 815, 816, 819, 820, 822, 831, 841, 843, 854, 859, 860, 867, 870, 871, 875, 878, 880, 883, 884, 886, 888, 891, 894, 897, 899, 902, 903, 909, 913, 921, 925, 928, 931, 935, 943, 945, 949, 950, 957, 960, 971, 975, 976, 977, 978, 980, 984, 989, 990, 992, 998, 1004, 1009, 1010, 1012, 1016, 1017, 1021, 1026, 1027, 1028, 1030, 1032, 1033, 1038, 1040, 1041, 1042, 1046, 1049, 1050, 1053, 1059, 1060, 1066, 1073, 1076, 1077, 1084, 1088, 1091, 1092, 1093, 1095, 1096, 1102, 1106, 1113, 1119, 1120, 1121, 1124, 1125, 1127, 1128, 1137, 1140, 1143, 1144, 1148, 1158, 1162, 1165, 1166, 1167, 1168, 1173, 1178, 1182, 1183, 1187, 1190, 1192, 1196, 1197, 1198, 1199, 1209, 1214, 1217, 1223, 1224, 1227, 1231, 1232, 1239, 1244, 1245, 1247, 1251, 1256, 1257, 1258, 1262, 1263, 1264, 1265, 1266, 1267, 1272, 1276, 1282, 1283, 1286, 1293, 1299, 1300, 1305, 1309, 1311, 1314, 1317, 1321, 1325, 1326, 1329, 1332, 1333, 1337, 1343, 1344, 1347, 1348, 1351, 1353, 1355, 1357, 1358, 1367, 1372, 1374, 1375, 1377, 1378, 1385, 1387, 1395, 1396, 1397, 1406, 1410, 1417, 1419, 1430, 1438, 1439, 1443, 1446, 1449, 1457, 1459, 1462, 1463, 1469, 1471, 1472, 1473, 1476, 1485, 1486, 1491, 1496, 1497, 1498, 1500, 1501, 1502, 1503, 1513, 1515, 1518, 1522, 1524, 1527, 1528, 1530, 1531, 1533, 1534, 1535, 1540, 1541, 1543, 1544, 1546, 1550, 1554, 1559, 1565, 1568, 1569, 1574, 1575, 1581, 1583, 1586, 1588, 1601, 1604, 1606, 1610, 1614, 1615, 1623, 1627, 1629, 1634, 1639, 1642, 1645, 1648, 1649, 1652, 1653, 1658, 1660, 1663, 1665, 1676, 1679, 1685, 1691, 1694, 1699, 1701, 1704, 1707, 1714, 1719, 1720, 1724, 1727, 1729, 1738, 1739, 1746, 1747, 1751, 1761, 1766, 1777, 1778, 1782, 1786, 1788, 1791, 1794, 1795, 1796, 1797, 1798, 1802, 1803, 1805, 1807, 1808, 1809, 1813, 1815, 1816, 1819, 1820, 1822, 1825, 1829, 1832, 1833, 1837, 1839, 1840, 1842, 1845, 1848, 1849, 1856, 1858, 1859, 1860, 1861, 1866, 1873, 1879, 1882, 1885, 1888, 1890, 1892, 1895, 1898, 1899, 1900, 1901, 1902, 1905, 1907, 1908, 1909, 1913, 1915, 1918, 1919, 1931, 1932, 1936, 1937, 1939, 1941, 1944, 1945, 1946, 1948, 1949, 1968, 1969, 1970, 1972, 1973, 1979, 1980, 1981, 1982, 1984, 1985, 1988, 1989, 1990, 1991, 1994, 1995, 1996, 2001, 2002, 2004, 2006, 2011, 2012, 2015, 2016, 2019, 2025, 2033, 2035, 2040, 2045, 2047, 2048, 2049, 2051, 2053, 2054, 2056, 2057, 2070, 2074, 2078, 2079, 2082, 2086, 2090, 2091, 2093, 2096, 2098, 2099, 2103, 2104, 2113, 2115, 2119, 2121, 2123, 2124, 2128, 2132, 2133, 2136, 2143, 2146, 2149, 2150, 2155, 2158, 2159, 2170, 2171, 2176, 2177, 2179, 2180, 2183, 2185, 2186, 2190, 2194, 2195, 2196, 2200, 2205, 2206, 2208, 2212, 2215, 2220, 2222, 2224, 2225, 2228, 2229, 2234, 2235, 2238, 2239, 2240, 2247, 2253, 2256, 2261, 2262, 2263, 2264, 2274, 2281, 2284, 2289, 2291, 2292, 2304, 2309, 2320, 2326, 2330, 2337, 2340, 2343, 2344, 2345, 2346, 2348, 2349, 2353, 2354, 2355, 2356, 2362, 2364, 2365, 2366, 2369, 2371, 2372, 2373, 2374, 2377, 2383, 2392, 2395, 2398, 2399, 2403, 2405, 2406, 2411, 2417, 2423, 2425, 2427, 2432, 2434, 2438, 2441, 2445, 2446, 2451, 2452, 2455, 2456, 2462, 2470, 2476, 2477, 2478, 2482, 2483, 2484, 2489, 2491, 2492, 2493, 2497, 2498, 2505, 2510, 2518, 2522, 2533, 2534, 2538, 2540, 2544, 2548, 2552, 2553, 2558, 2562, 2568, 2573, 2578, 2579, 2581, 2590, 2598, 2599, 2603, 2605, 2608, 2610, 2613, 2616, 2619, 2622, 2623, 2625, 2637, 2639, 2640, 2641, 2642, 2644, 2648, 2649, 2650, 2652, 2658, 2666, 2670, 2674, 2679, 2684, 2688, 2691, 2694, 2700, 2702, 2703, 2707, 2710, 2718, 2722, 2723, 2726, 2727, 2729, 2735, 2741, 2742, 2744, 2754, 2756, 2757, 2758, 2763, 2769, 2773, 2779, 2786, 2787, 2790, 2791, 2793, 2797, 2798, 2802, 2805, 2806, 2809, 2810, 2811, 2817, 2819, 2821, 2827, 2840, 2841, 2842, 2850, 2852, 2853, 2859, 2863, 2874, 2878, 2879, 2882, 2894, 2896, 2904, 2905, 2912, 2915, 2916, 2917, 2921, 2924, 2925, 2934, 2937, 2938, 2940, 2941, 2943, 2944, 2945, 2946, 2947, 2948, 2952, 2956, 2957, 2958, 2963, 2968, 2969, 2973, 2974, 2975, 2978, 2979, 2986, 2988, 2996, 3002, 3018, 3020, 3026, 3029, 3030, 3031, 3032, 3035, 3036, 3038, 3042, 3044, 3045, 3049, 3052, 3055, 3065, 3073, 3081, 3084, 3087, 3090, 3092, 3100, 3102, 3103, 3110, 3111, 3116, 3132, 3135, 3137, 3138, 3139, 3143, 3150, 3151, 3154, 3155, 3156, 3161, 3165, 3172, 3174, 3176, 3177, 3178, 3180, 3181, 3182, 3189, 3190, 3194, 3196, 3199, 3201, 3202, 3203, 3205, 3214, 3216, 3217, 3218, 3220, 3221, 3225, 3226, 3227, 3232, 3234, 3236, 3238, 3239, 3245, 3248, 3252, 3253, 3254, 3257, 3265, 3266, 3272, 3273, 3274, 3275, 3276, 3277, 3279, 3283, 3286, 3287, 3288, 3289, 3292, 3298, 3301, 3304, 3306, 3307, 3309, 3312, 3320, 3323, 3326, 3327, 3333, 3335, 3347, 3356, 3357, 3359, 3360, 3361, 3362, 3364, 3373, 3377, 3380, 3388, 3390, 3393, 3395, 3399, 3400, 3415, 3419, 3426, 3430, 3435, 3436, 3440, 3449, 3456, 3457, 3458, 3459, 3460, 3461, 3466, 3471, 3474, 3475, 3477, 3482, 3483, 3485, 3489, 3490, 3491, 3494, 3496, 3498, 3515, 3523, 3530, 3533, 3538, 3539, 3542, 3547, 3549, 3552, 3553, 3555, 3556, 3557, 3559, 3560, 3564, 3566, 3569, 3571, 3572, 3574, 3575, 3586, 3587, 3588, 3590, 3591, 3593, 3601, 3620, 3622, 3623, 3630, 3633, 3637, 3644, 3649, 3650, 3651, 3652, 3653, 3656, 3657, 3659, 3666, 3673, 3674, 3676, 3677, 3678, 3680, 3691, 3699, 3702, 3705, 3706, 3711, 3715, 3716, 3718, 3727, 3734, 3736, 3737, 3738, 3740, 3743, 3744, 3745, 3746, 3747, 3751, 3759, 3760, 3763, 3765, 3771, 3772, 3780, 3781, 3784, 3794, 3796, 3810, 3811, 3814, 3815, 3816, 3818, 3820, 3824, 3826, 3827, 3830, 3831, 3835, 3836, 3838, 3839, 3840, 3847, 3848, 3850, 3852, 3854, 3855, 3856, 3867, 3871, 3874, 3876, 3877, 3879, 3881, 3882, 3883, 3887, 3889, 3896, 3897, 3910, 3912, 3913, 3919, 3924, 3926, 3929, 3935, 3945, 3948, 3949, 3951, 3954, 3955, 3957, 3958, 3964, 3965, 3968, 3969, 3973, 3977, 3978, 3981, 3986, 3987, 3988, 3989, 3990, 3992, 3993, 3995, 3996, 3998, 3999, 4001, 4002, 4003, 4005, 4006, 4013, 4016, 4018, 4022, 4030, 4032, 4038, 4039, 4041, 4042, 4043, 4048, 4052, 4058, 4060, 4062, 4063, 4067, 4068, 4070, 4072, 4079, 4082, 4097, 4098, 4104, 4112, 4113, 4115, 4118, 4121, 4127, 4132, 4135, 4137, 4140, 4142, 4147, 4148, 4151, 4152, 4155, 4166, 4171, 4175, 4183, 4184, 4185, 4187, 4191, 4194, 4195, 4200, 4208, 4211, 4220, 4223, 4228, 4230, 4236, 4239, 4240, 4242, 4245, 4246, 4248, 4249, 4252, 4254, 4255, 4257, 4259, 4263, 4264, 4265, 4266, 4271, 4272, 4273, 4274, 4277, 4279, 4285, 4288, 4290, 4291, 4297, 4299, 4301, 4307, 4312, 4318, 4328, 4329, 4330, 4331, 4336, 4343, 4344, 4347, 4348, 4350, 4356, 4357, 4359, 4362, 4367, 4369, 4374, 4388, 4395, 4397, 4398, 4401, 4403, 4404, 4405, 4407, 4408, 4409, 4416, 4420, 4421, 4422, 4424, 4429, 4435, 4439, 4444, 4449, 4452, 4456, 4463, 4464, 4465, 4467, 4469, 4472, 4474, 4477, 4478, 4480, 4481, 4484, 4487, 4490, 4491, 4495, 4496, 4497, 4498, 4499, 4500, 4504, 4509, 4518, 4521, 4522, 4536, 4537, 4538, 4546, 4550, 4552, 4557, 4566, 4568, 4570, 4575, 4577, 4579, 4584, 4589, 4591, 4592, 4593, 4594, 4597, 4598, 4600, 4606, 4611, 4614, 4618, 4619, 4623, 4624, 4627, 4633, 4637, 4646, 4657, 4669, 4670, 4671, 4677, 4687, 4690, 4695, 4697, 4699, 4700, 4705, 4712, 4719, 4720, 4725, 4729, 4735, 4737, 4740, 4741, 4742, 4745, 4757, 4760, 4762, 4765, 4766, 4771, 4784, 4785, 4786, 4791, 4792, 4796, 4797, 4798, 4799, 4803, 4805, 4807, 4809, 4811, 4812, 4814, 4822, 4823, 4828, 4833, 4835, 4842, 4848, 4849, 4852, 4857, 4861, 4862, 4866, 4873, 4874, 4875, 4879, 4891, 4893, 4894, 4897, 4899, 4901, 4906, 4907, 4910, 4916, 4918, 4920, 4923, 4924, 4927, 4929, 4947, 4948, 4949, 4951, 4954, 4955, 4957, 4958, 4961, 4963, 4965, 4966, 4967, 4973, 4976, 4979, 4980, 4987, 4990, 4993, 5000, 5004, 5006, 5013, 5018, 5021, 5026, 5029, 5031, 5033, 5034, 5035, 5037, 5039, 5042, 5044, 5045, 5046, 5050, 5051, 5052, 5056, 5058, 5070, 5071, 5074, 5077, 5080, 5085, 5087, 5091, 5096, 5097, 5107, 5109, 5112, 5114, 5120, 5124, 5127, 5128, 5129, 5135, 5136, 5137, 5140, 5145, 5147, 5155, 5161, 5164, 5168, 5169, 5170, 5175, 5188, 5190, 5195, 5201, 5211, 5215, 5223, 5236, 5237, 5238, 5239, 5241, 5244, 5250, 5254, 5255, 5258, 5261, 5263, 5275, 5277, 5282, 5285, 5302, 5310, 5313, 5317, 5327, 5336, 5337, 5340, 5348, 5353, 5357, 5358, 5360, 5364, 5366, 5376, 5377, 5383, 5385, 5389, 5391, 5393, 5395, 5400, 5407, 5410, 5411, 5417, 5425, 5428, 5430, 5434, 5442, 5445, 5446, 5450, 5454, 5460, 5461, 5462, 5465, 5466, 5467, 5470, 5471, 5472, 5476, 5481, 5482, 5485, 5487, 5489, 5490, 5493, 5494, 5495, 5500, 5502, 5503, 5504, 5509, 5512, 5516, 5518, 5532, 5537, 5542, 5544, 5547, 5548, 5553, 5554, 5561, 5562, 5564, 5574, 5575, 5583, 5587, 5594, 5597, 5598, 5599, 5603, 5606, 5607, 5608, 5609, 5610, 5611, 5613, 5616, 5617, 5620, 5623, 5629, 5631, 5634, 5636, 5637, 5639, 5641, 5643, 5661, 5664, 5666, 5668, 5675, 5676, 5678, 5690, 5694, 5697, 5701, 5705, 5710, 5712, 5713, 5719, 5722, 5724, 5726, 5729, 5737, 5740, 5742, 5750, 5753, 5754, 5760, 5761, 5765, 5770, 5774, 5775, 5778, 5784, 5785, 5788, 5789, 5791, 5795, 5799, 5801, 5808, 5809, 5814, 5820, 5823, 5824, 5841, 5844, 5845, 5846, 5847, 5849, 5850, 5857, 5862, 5863, 5866, 5867, 5878, 5879, 5880, 5881, 5885, 5886, 5888, 5897, 5908, 5909, 5911, 5921, 5922, 5928, 5929, 5930, 5932, 5936, 5938, 5939, 5949, 5953, 5958, 5962, 5964, 5966, 5967, 5968, 5975, 5976, 5982, 5986, 5990, 5993, 5995, 5996, 6001, 6002, 6005, 6008, 6012, 6014, 6024, 6027, 6037, 6038, 6043, 6047, 6049, 6052, 6053, 6054, 6061, 6064, 6066, 6067, 6068, 6069, 6070, 6072, 6073, 6080, 6081, 6086, 6088, 6089, 6092, 6093, 6097, 6107, 6108, 6109, 6114, 6115, 6123, 6124, 6125, 6129, 6138, 6141, 6151, 6155, 6158, 6160, 6166, 6168, 6170, 6171, 6176, 6178, 6180, 6181, 6183, 6184, 6185, 6189, 6190, 6191, 6195, 6196, 6201, 6204, 6212, 6215, 6216, 6217, 6224, 6228, 6233, 6237, 6239, 6251, 6254, 6255, 6256, 6258, 6268, 6271, 6272, 6275, 6276, 6279, 6284, 6287, 6289, 6290, 6291, 6309, 6311, 6313, 6315, 6318, 6321, 6331, 6338, 6340, 6345, 6346, 6348, 6349, 6356, 6358, 6360, 6362, 6365, 6368, 6369, 6375, 6383, 6387, 6392, 6396, 6397, 6398, 6399, 6407, 6414, 6416, 6421, 6424, 6426, 6429, 6436, 6437, 6440, 6441, 6447, 6448, 6461, 6466, 6468, 6471, 6472, 6473, 6475, 6477, 6479, 6481, 6482, 6489, 6497, 6505, 6510, 6511, 6513, 6515, 6516, 6520, 6522, 6525, 6526, 6527, 6535, 6537, 6546, 6552, 6553, 6557, 6558, 6563, 6569, 6570, 6574, 6576, 6578, 6580, 6581, 6582, 6583, 6588, 6598, 6604, 6605, 6609, 6624, 6625, 6628, 6630, 6649, 6652, 6653, 6657, 6661, 6670, 6673, 6682, 6685, 6686, 6687, 6688, 6690, 6691, 6692, 6693, 6695, 6700, 6703, 6704, 6706, 6708, 6712, 6713, 6715, 6718, 6720, 6722, 6729, 6730, 6732, 6733, 6735, 6739, 6741, 6745, 6746, 6749, 6755, 6758, 6760, 6761, 6770, 6772, 6773, 6783, 6797, 6799, 6800, 6802, 6803, 6806, 6809, 6810, 6817, 6823, 6825, 6828, 6830, 6834, 6845, 6847, 6854, 6856, 6862, 6863, 6867, 6868, 6871, 6873, 6874, 6875, 6876, 6877, 6880, 6881, 6883, 6890, 6891, 6894, 6898, 6901, 6903, 6904, 6906, 6908, 6910, 6914, 6916, 6917, 6918, 6920, 6924, 6927, 6928, 6930, 6938, 6940, 6942, 6950, 6951, 6952, 6953, 6958, 6959, 6973, 6979, 6982, 6983, 6984, 6986, 6989, 6999, 7001, 7002, 7008, 7010, 7015, 7016, 7023, 7024, 7030, 7031, 7033, 7036, 7038, 7042, 7045, 7046, 7049, 7050, 7056, 7060, 7062, 7063, 7064, 7065, 7068, 7075, 7077, 7080, 7084, 7086, 7087, 7094, 7095, 7097, 7101, 7102, 7103, 7105, 7110, 7112, 7115, 7118, 7121, 7123, 7125, 7133, 7134, 7138, 7139, 7142, 7144, 7147, 7148, 7151, 7152, 7155, 7159, 7164, 7167, 7171, 7172, 7173, 7177, 7178, 7182, 7183, 7184, 7187, 7188, 7189, 7190, 7192, 7197, 7201, 7209, 7212, 7213, 7214, 7216, 7217, 7221, 7224, 7227, 7228, 7231, 7235, 7246, 7247, 7248, 7254, 7255, 7258, 7259, 7260, 7265, 7267, 7272, 7276, 7281, 7282, 7283, 7284, 7287, 7291, 7292, 7294, 7296, 7297, 7304, 7309, 7314, 7316, 7319, 7323, 7326, 7327, 7333, 7334, 7335, 7336, 7338, 7339, 7340, 7352, 7356, 7357, 7363, 7365, 7368, 7371, 7375, 7379, 7387, 7388, 7390, 7391, 7394, 7396, 7397, 7400, 7401, 7402, 7405, 7406, 7414, 7415, 7420, 7422, 7427, 7432, 7433, 7438, 7440, 7442, 7454, 7455, 7463, 7464, 7470, 7472, 7473, 7475, 7479, 7481, 7488, 7489, 7490, 7493, 7494, 7499, 7507, 7514, 7516, 7519, 7521, 7525, 7532, 7533, 7536, 7537, 7539, 7543, 7549, 7565, 7567, 7570, 7571, 7572, 7573, 7580, 7583, 7584, 7586, 7589, 7595, 7596, 7599, 7600, 7602, 7603, 7605, 7606, 7615, 7621, 7628, 7630, 7632, 7636, 7640, 7641, 7652, 7654, 7656, 7659, 7664, 7665, 7668, 7669, 7671, 7672, 7673, 7675, 7681, 7693, 7694, 7697, 7698, 7700, 7703, 7705, 7708, 7709, 7710, 7711, 7724, 7728, 7731, 7735, 7738, 7742, 7744, 7749, 7753, 7755, 7758, 7763, 7774, 7775, 7781, 7782, 7784, 7795, 7801, 7804, 7816, 7817, 7818, 7825, 7826, 7828, 7831, 7840, 7843, 7848, 7853, 7855, 7856, 7874, 7878, 7882, 7883, 7887, 7891, 7894, 7897, 7900, 7901, 7904, 7909, 7910, 7913, 7915, 7920, 7921, 7923, 7925, 7928, 7948, 7952, 7958, 7964, 7966, 7968, 7973, 7977, 7979, 7987, 7989, 7992, 8001, 8008, 8010, 8013, 8016, 8018, 8020, 8023, 8025, 8028, 8034, 8036, 8040, 8047, 8050, 8051, 8054, 8062, 8063, 8066, 8067, 8070, 8072, 8076, 8081, 8084, 8095, 8096, 8097, 8099, 8103, 8104, 8105, 8107, 8109, 8110, 8114, 8116, 8118, 8120, 8121, 8122, 8127, 8128, 8134, 8138, 8141, 8142, 8143, 8145, 8152, 8153, 8154, 8163, 8170, 8172, 8173, 8183, 8185, 8188, 8195, 8196, 8200, 8201, 8203, 8205, 8206, 8210, 8213, 8214, 8215, 8216, 8221, 8227, 8232, 8237, 8243, 8244, 8249, 8259, 8271, 8272, 8274, 8276, 8278, 8282, 8285, 8286, 8288, 8290, 8291, 8296, 8297, 8298, 8303, 8311, 8312, 8313, 8317, 8318, 8322, 8332, 8333, 8336, 8337, 8340, 8342, 8343, 8344, 8346, 8347, 8348, 8359, 8363, 8364, 8366, 8369, 8371, 8377, 8380, 8409, 8410, 8415, 8416, 8418, 8419, 8422, 8424, 8427, 8428, 8432, 8439, 8440, 8443, 8445, 8446, 8447, 8452, 8458, 8459, 8463, 8467, 8474, 8477, 8481, 8482, 8483, 8486, 8488, 8489, 8492, 8494, 8498, 8499, 8501, 8502, 8503, 8504, 8505, 8506, 8512, 8513, 8516, 8518, 8519, 8525, 8526, 8532, 8534, 8541, 8542, 8543, 8545, 8549, 8550, 8554, 8557, 8561, 8563, 8567, 8570, 8571, 8573, 8575, 8576, 8579, 8580, 8581, 8587, 8591, 8597, 8605, 8609, 8616, 8619, 8622, 8629, 8636, 8645, 8649, 8654, 8656, 8660, 8661, 8662, 8663, 8665, 8666, 8674, 8676, 8677, 8678, 8679, 8686, 8687, 8690, 8692, 8697, 8698, 8701, 8703, 8705, 8709, 8714, 8719, 8721, 8724, 8727, 8730, 8734, 8740, 8746, 8747, 8749, 8753, 8757, 8758, 8762, 8765, 8767, 8769, 8773, 8774, 8776, 8777, 8781, 8784, 8789, 8790, 8791, 8793, 8794, 8796, 8798, 8799, 8806, 8810, 8811, 8812, 8813, 8817, 8818, 8820, 8822, 8823, 8825, 8828, 8830, 8831, 8833, 8841, 8846, 8848, 8857, 8859, 8861, 8866, 8869, 8870, 8872, 8873, 8879, 8882, 8888, 8889, 8890, 8891, 8894, 8896, 8899, 8911, 8916, 8917, 8918, 8921, 8926, 8928, 8929, 8931, 8941, 8942, 8943, 8949, 8952, 8959, 8961, 8962, 8963, 8965, 8968, 8969, 8977, 8981, 8985, 8986, 8987, 8990, 8991, 8996, 8998, 8999, 9001, 9007, 9012, 9013, 9016, 9019, 9026, 9027, 9032, 9034, 9038, 9045, 9053, 9056, 9057, 9061, 9062, 9063, 9064, 9068, 9072, 9075, 9076, 9078, 9086, 9088, 9091, 9094, 9097, 9100, 9104, 9117, 9119, 9127, 9129, 9141, 9143, 9144, 9146, 9152, 9157, 9158, 9160, 9161, 9162, 9163, 9165, 9168, 9175, 9177, 9190, 9194, 9195, 9200, 9201, 9202, 9204, 9213, 9215, 9216, 9225, 9226, 9231, 9232, 9233, 9236, 9240, 9246, 9251, 9252, 9253, 9258, 9259, 9260, 9262, 9263, 9266, 9268, 9269, 9271, 9272, 9273, 9277, 9279, 9281, 9294, 9297, 9299, 9302, 9303, 9309, 9310, 9312, 9313, 9316, 9319, 9321, 9325, 9331, 9335, 9338, 9342, 9354, 9356, 9359, 9363, 9366, 9370, 9376, 9377, 9384, 9389, 9393, 9396, 9397, 9402, 9407, 9412, 9413, 9415, 9419, 9423, 9424, 9434, 9437, 9444, 9459, 9460, 9461, 9462, 9466, 9473, 9475, 9478, 9480, 9482, 9484, 9486, 9490, 9493, 9494, 9498, 9503, 9505, 9508, 9517, 9518, 9521, 9523, 9526, 9533, 9538, 9539, 9549, 9552, 9553, 9555, 9561, 9562, 9566, 9569, 9576, 9579, 9580, 9584, 9586, 9594, 9597, 9599, 9600, 9602, 9604, 9605, 9607, 9611, 9615, 9616, 9619, 9620, 9622, 9628, 9639, 9640, 9644, 9647, 9661, 9662, 9665, 9667, 9669, 9670, 9672, 9674, 9677, 9680, 9685, 9691, 9696, 9705, 9708, 9710, 9712, 9713, 9714, 9715, 9722, 9725, 9726, 9727, 9729, 9730, 9736, 9737, 9739, 9741, 9744, 9747, 9752, 9753, 9754, 9763, 9765, 9768, 9771, 9784, 9787, 9788, 9798, 9799, 9801, 9802, 9803, 9804, 9805, 9809, 9810, 9812, 9818, 9819, 9825, 9826, 9828, 9829, 9832, 9833, 9835, 9836, 9842, 9844, 9846, 9847, 9852, 9857, 9863, 9865, 9870, 9871, 9873, 9876, 9877, 9882, 9883, 9885, 9886, 9887, 9889, 9891, 9892, 9895, 9896, 9900, 9901, 9905, 9910, 9913, 9915, 9921, 9923, 9924, 9933, 9936, 9937, 9942, 9943, 9944, 9950, 9951, 9952, 9954, 9958, 9960, 9965, 9968, 9970, 9972, 9973, 9974, 9980, 9983, 9984, 9987, 9991, 9994, 10006, 10007, 10010, 10012, 10015, 10017, 10019, 10022, 10026, 10027, 10029, 10030, 10032, 10035, 10036, 10038, 10040, 10042, 10043, 10044, 10045, 10046, 10049, 10050, 10063, 10065, 10075, 10077, 10080, 10085, 10088, 10089, 10097, 10099, 10102, 10105, 10106, 10110, 10115, 10117, 10122, 10123, 10130, 10131, 10136, 10140, 10144, 10149, 10150, 10153, 10154, 10155, 10160, 10165, 10167, 10171, 10173, 10174, 10177, 10179, 10180, 10184, 10188, 10204, 10207, 10222, 10224, 10231, 10234, 10235, 10241, 10243, 10248, 10250, 10254, 10255, 10264, 10266, 10272, 10277, 10278, 10281, 10286, 10287, 10289, 10292, 10294, 10302, 10304, 10313, 10320, 10322, 10324, 10325, 10326, 10327, 10332, 10334, 10337, 10339, 10348, 10352, 10355, 10357, 10359, 10360, 10364, 10368, 10369, 10370, 10374, 10379, 10385, 10390, 10392, 10394, 10397, 10400, 10414, 10420, 10422, 10423, 10427, 10428, 10432, 10434, 10440, 10443, 10449, 10453, 10455, 10464, 10467, 10478, 10482, 10484, 10496, 10501, 10502, 10505, 10506, 10509, 10510, 10512, 10513, 10518, 10527, 10529, 10531, 10547, 10559, 10561, 10564, 10565, 10567, 10568, 10586, 10600, 10601, 10602, 10607, 10609, 10615, 10616, 10617, 10618, 10623, 10624, 10625, 10633, 10639, 10646, 10649, 10650, 10651, 10652, 10653, 10654, 10656, 10658, 10659, 10662, 10664, 10665, 10666, 10667, 10668, 10669, 10670, 10674, 10675, 10677, 10681, 10686, 10692, 10693, 10695, 10696, 10705, 10710, 10714, 10717, 10738, 10741, 10742, 10745, 10748, 10749, 10750, 10751, 10758, 10765, 10771, 10776, 10779, 10780, 10783, 10784, 10785, 10799, 10800, 10804, 10806, 10807, 10808, 10809, 10811, 10814, 10817, 10818, 10822, 10823, 10829, 10830, 10833, 10834, 10839, 10842, 10843, 10847, 10849, 10856, 10859, 10861, 10874, 10880, 10884, 10889, 10890, 10893, 10898, 10899, 10901, 10906, 10909, 10911, 10912, 10920, 10924, 10928, 10932, 10933, 10937, 10941, 10948, 10951, 10954, 10956, 10957, 10963, 10965, 10967, 10969, 10971, 10972, 10975, 10976, 10978, 10980, 10981, 10983, 10985, 10987, 10989, 10990, 10997, 10999, 11003, 11012, 11013, 11018, 11019, 11020, 11021, 11023, 11027, 11028, 11030, 11032, 11036, 11037, 11040, 11047, 11049, 11050, 11055, 11056, 11059, 11063, 11067, 11074, 11078, 11080, 11081, 11082, 11090, 11091, 11092, 11095, 11096, 11112, 11116, 11122, 11123, 11133, 11134, 11139, 11142, 11143, 11149, 11150, 11155, 11162, 11167, 11172, 11177, 11181, 11183, 11189, 11194, 11197, 11199, 11202, 11204, 11205, 11207, 11210, 11212, 11214, 11220, 11226, 11230, 11232, 11233, 11235, 11236, 11242, 11247, 11255, 11262, 11264, 11265, 11269, 11270, 11275, 11279, 11282, 11286, 11292, 11294, 11295, 11300, 11305, 11311, 11312, 11315, 11318, 11322, 11327, 11328, 11330, 11335, 11336, 11338, 11339, 11340, 11346, 11347, 11348, 11349, 11350, 11354, 11356, 11358, 11362, 11369, 11371, 11372, 11375, 11381, 11382, 11384, 11404, 11406, 11408, 11409, 11412, 11413, 11423, 11426, 11427, 11431, 11433, 11437, 11438, 11440, 11442, 11448, 11451, 11454, 11455, 11456, 11460, 11461, 11467, 11470, 11472, 11476, 11477, 11478, 11479, 11483, 11484, 11486, 11490, 11498, 11503, 11516, 11517, 11518, 11523, 11524, 11526, 11529, 11533, 11535, 11536, 11542, 11544, 11550, 11552, 11557, 11559, 11561, 11563, 11565, 11566, 11567, 11568, 11569, 11575, 11577, 11579, 11580, 11581, 11583, 11585, 11587, 11590, 11592, 11593, 11603, 11604, 11609, 11614, 11617, 11623, 11624, 11626, 11634, 11635, 11640, 11641, 11643, 11644, 11646, 11647, 11656, 11659, 11663, 11664, 11667, 11675, 11680, 11683, 11684, 11686, 11687, 11688, 11689, 11697, 11700, 11701, 11706, 11707, 11710, 11712, 11714, 11717, 11718, 11719, 11723, 11725, 11728, 11729, 11732, 11737, 11740, 11744, 11745, 11747, 11748, 11752, 11753, 11757, 11758, 11759, 11763, 11770, 11771, 11778, 11782, 11784, 11785, 11787, 11797, 11798, 11800, 11802, 11803, 11805, 11808, 11813, 11817, 11818, 11820, 11829, 11837, 11842, 11844, 11849, 11850, 11853, 11855, 11858, 11863, 11867, 11869, 11870, 11871, 11877, 11880, 11885, 11890, 11891, 11892, 11897, 11899, 11901, 11905, 11907, 11908, 11916, 11917, 11918, 11920, 11926, 11927, 11932, 11938, 11941, 11944, 11945, 11947, 11948, 11951, 11952, 11959, 11962, 11971, 11976, 11978, 11982, 11985, 11991, 11995, 11996, 11997, 11999, 12001, 12003, 12005, 12008, 12011, 12012, 12013, 12017, 12020, 12021, 12025, 12032, 12040, 12045, 12050, 12056, 12058, 12061, 12063, 12064, 12065, 12068, 12070, 12075, 12077, 12078, 12082, 12085, 12086, 12087, 12089, 12093, 12094, 12095, 12103, 12110, 12115, 12116, 12119, 12120, 12124, 12125, 12130, 12131, 12136, 12138, 12140, 12141, 12142, 12143, 12145, 12146, 12151, 12152, 12153, 12156, 12161, 12164, 12167, 12169, 12170, 12173, 12175, 12177, 12178, 12179, 12182, 12183, 12184, 12185, 12186, 12188, 12189, 12192, 12207, 12215, 12216, 12218, 12219, 12221, 12222, 12223, 12226, 12228, 12233, 12234, 12235, 12241, 12242, 12243, 12244, 12246, 12249, 12250, 12251, 12252, 12253, 12254, 12257, 12258, 12259, 12261, 12265, 12267, 12268, 12269, 12271, 12274, 12277, 12278, 12281, 12282, 12284, 12287, 12289, 12290, 12294, 12295, 12302, 12305, 12306, 12307, 12310, 12314, 12320, 12324, 12327, 12329, 12330, 12337, 12342, 12344, 12345, 12348, 12351, 12355, 12367, 12371, 12372, 12373, 12376, 12377, 12379, 12380, 12381, 12387, 12388, 12392, 12395, 12397, 12398, 12400, 12401, 12402, 12407, 12410, 12412, 12413, 12423, 12425, 12432, 12434, 12435, 12438, 12442, 12445, 12446, 12448, 12450, 12454, 12457, 12461, 12467, 12471, 12472, 12474, 12476, 12479, 12480, 12481, 12482, 12483, 12489, 12492, 12496, 12498, 12501, 12511, 12517, 12518, 12519, 12521, 12522, 12524, 12530, 12531, 12532, 12536, 12537, 12540, 12546, 12553, 12561, 12565, 12566, 12572, 12575, 12584, 12585, 12588, 12589, 12592, 12594, 12595, 12599, 12605, 12606, 12611, 12618, 12622, 12623, 12625, 12626, 12627, 12628, 12633, 12634, 12635, 12637, 12639, 12643, 12648, 12657, 12659, 12660, 12661, 12663, 12664, 12665, 12675, 12685, 12692, 12695, 12704, 12708, 12712, 12715, 12718, 12724, 12726, 12732, 12736, 12737, 12740, 12741, 12744, 12746, 12748, 12752, 12753, 12759, 12760, 12761, 12762, 12767, 12769, 12770, 12776, 12777, 12779, 12780, 12784, 12788, 12797, 12803, 12807, 12809, 12810, 12812, 12819, 12823, 12824, 12826, 12828, 12830, 12833, 12834, 12839, 12840, 12841, 12847, 12856, 12865, 12868, 12869, 12870, 12871, 12872, 12875, 12876, 12878, 12880, 12882, 12883, 12896, 12899, 12901, 12906, 12907, 12908, 12915, 12916, 12921, 12923, 12927, 12933, 12941, 12942, 12946, 12948, 12955, 12957, 12958, 12964, 12965, 12968, 12970, 12980, 12982, 12983, 12990, 12991, 12993, 12995, 13002, 13004, 13005, 13007, 13014, 13018, 13020, 13021, 13027, 13029, 13035, 13036, 13038, 13041, 13049, 13052, 13055, 13057, 13061, 13064, 13069, 13071, 13074, 13076, 13078, 13079, 13080, 13082, 13084, 13086, 13091, 13095, 13098, 13099, 13100, 13101, 13109, 13111, 13116, 13124, 13125, 13127, 13133, 13136, 13137, 13138, 13140, 13145, 13146, 13153, 13161, 13165, 13167, 13169, 13171, 13174, 13177, 13179, 13183, 13185, 13186, 13190, 13195, 13200, 13207, 13212, 13214, 13220, 13226, 13229, 13231, 13236, 13238, 13241, 13242, 13244, 13249, 13250, 13252, 13258, 13259, 13267, 13268, 13272, 13274, 13276, 13281, 13286, 13288, 13289, 13301, 13303, 13305, 13306, 13308, 13314, 13321, 13323, 13327, 13329, 13330, 13333, 13340, 13353, 13354, 13357, 13359, 13360, 13361, 13362, 13364, 13367, 13376, 13378, 13379, 13382, 13385, 13391, 13395, 13401, 13403, 13406, 13417, 13418, 13419, 13420, 13421, 13422, 13428, 13431, 13436, 13438, 13439, 13441, 13445, 13452, 13459, 13462, 13463, 13464, 13465, 13469, 13470, 13475, 13478, 13489, 13490, 13493, 13494, 13495, 13500, 13502, 13505, 13507, 13508, 13515, 13523, 13530, 13532, 13536, 13541, 13542, 13548, 13552, 13554, 13556, 13559, 13562, 13564, 13565, 13566, 13574, 13575, 13579, 13582, 13587, 13589, 13597, 13598, 13600, 13601, 13606, 13607, 13610, 13618, 13620, 13622, 13623, 13624, 13627, 13628, 13633, 13635, 13637, 13642, 13644, 13647, 13648, 13653, 13661, 13675, 13677, 13682, 13683, 13689, 13690, 13693, 13695, 13699, 13711, 13714, 13716, 13718, 13720, 13732, 13733, 13735, 13736, 13742, 13745, 13748, 13751, 13753, 13754, 13755, 13757, 13759, 13765, 13772, 13782, 13787, 13788, 13792, 13795, 13800, 13809, 13812, 13813, 13815, 13817, 13825, 13835, 13836, 13837, 13838, 13841, 13842, 13844, 13845, 13847, 13848, 13850, 13852, 13853, 13857, 13864, 13866, 13877, 13881, 13882, 13884, 13888, 13895, 13905, 13906, 13912, 13919, 13920, 13922, 13926, 13927, 13930, 13931, 13932, 13934, 13935, 13936, 13938, 13941, 13946, 13947, 13949, 13950, 13951, 13953, 13956, 13958, 13961, 13969, 13976, 13978, 13979, 13980, 13983, 13985, 13986, 13988, 13993, 13995, 14002, 14003, 14005, 14012, 14014, 14016, 14025, 14026, 14035, 14038, 14041, 14042, 14044, 14047, 14049, 14050, 14054, 14057, 14058, 14059, 14060, 14061, 14063, 14065, 14069, 14071, 14073, 14078, 14082, 14083, 14084, 14087, 14089, 14093, 14100, 14104, 14116, 14118, 14126, 14129, 14130, 14140, 14143, 14149, 14152, 14154, 14156, 14157, 14159, 14160, 14165, 14170, 14173, 14177, 14184, 14185, 14186, 14189, 14190, 14196, 14197, 14202, 14203, 14209, 14211, 14217, 14218, 14219, 14223, 14227, 14228, 14229, 14230, 14233, 14238, 14240, 14241, 14245, 14256, 14259, 14263, 14268, 14269, 14271, 14279, 14282, 14285, 14286, 14293, 14294, 14298, 14300, 14301, 14304, 14309, 14310, 14311, 14314, 14315, 14319, 14320, 14322, 14323, 14325, 14329, 14334, 14337, 14338, 14345, 14346, 14347, 14348, 14352, 14354, 14357, 14358, 14366, 14370, 14379, 14380, 14385, 14387, 14389, 14391, 14393, 14402, 14404, 14405, 14407, 14409, 14413, 14415, 14420, 14432, 14434, 14435, 14440, 14441, 14445, 14448, 14449, 14450, 14465, 14466, 14467, 14477, 14480, 14482, 14484, 14492, 14495, 14497, 14507, 14511, 14514, 14515, 14517, 14520, 14522, 14524, 14531, 14535, 14539, 14543, 14545, 14550, 14555, 14558, 14561, 14563, 14564, 14568, 14571, 14572, 14573, 14574, 14578, 14579, 14583, 14585, 14587, 14592, 14593, 14607, 14611, 14614, 14615, 14623, 14624, 14626, 14629, 14633, 14635, 14640, 14642, 14651, 14660, 14662, 14663, 14664, 14667, 14673, 14674, 14679, 14683, 14688, 14689, 14690, 14695, 14700, 14701, 14702, 14709, 14710, 14711, 14713, 14714, 14715, 14719, 14727, 14735, 14737, 14741, 14743, 14748, 14749, 14750, 14753, 14754, 14755, 14756, 14757, 14759, 14763, 14772, 14774, 14777, 14779, 14784, 14787, 14796, 14800, 14805, 14806, 14810, 14814, 14815, 14816, 14818, 14825, 14827, 14830, 14833, 14835, 14836, 14837, 14838, 14840, 14841, 14847, 14851, 14853, 14860, 14863, 14864, 14869, 14870, 14873, 14874, 14875, 14879, 14881, 14883, 14884, 14885, 14886, 14887, 14892, 14897, 14898, 14899, 14901, 14905, 14908, 14912, 14921, 14923, 14928, 14932, 14935, 14938, 14942, 14944, 14945, 14949, 14953, 14954, 14955, 14956, 14957, 14958, 14964, 14965, 14966, 14967, 14972, 14973, 14974, 14975, 14976, 14977, 14982, 14985, 14987, 14990, 14994, 14998, 14999, 15001, 15002, 15003, 15004, 15008, 15010, 15015, 15019, 15020, 15022, 15026, 15027, 15028, 15030, 15036, 15039, 15042, 15047, 15051, 15052, 15056, 15057, 15062, 15066, 15081, 15083, 15085, 15091, 15098, 15100, 15103, 15105, 15106, 15115, 15118, 15119, 15120, 15121, 15126, 15127, 15134, 15138, 15141, 15143, 15149, 15150, 15153, 15157, 15160, 15161, 15166, 15168, 15174, 15179, 15182, 15183, 15187, 15190, 15191, 15192, 15193, 15200, 15201, 15206, 15208, 15216, 15217, 15219, 15220, 15227, 15228, 15231, 15232, 15234, 15235, 15236, 15239, 15243, 15246, 15247, 15250, 15252, 15255, 15258, 15259, 15263, 15266, 15275, 15276, 15278, 15279, 15280, 15283, 15285, 15287, 15291, 15294, 15304, 15314, 15315, 15325, 15328, 15329, 15333, 15336, 15337, 15340, 15352, 15356, 15357, 15359, 15363, 15365, 15367, 15370, 15376, 15380, 15385, 15386, 15387, 15390, 15392, 15395, 15401, 15403, 15407, 15409, 15411, 15414, 15415, 15420, 15426, 15427, 15431, 15433, 15434, 15438, 15440, 15441, 15445, 15447, 15452, 15454, 15455, 15456, 15458, 15460, 15461, 15464, 15469, 15472, 15473, 15475, 15484, 15489, 15490, 15491, 15495, 15506, 15509, 15513, 15521, 15525, 15527, 15530, 15531, 15535, 15537, 15541, 15542, 15544, 15545, 15548, 15553, 15554, 15560, 15562, 15566, 15568, 15569, 15570, 15571, 15573, 15578, 15582, 15584, 15585, 15589, 15594, 15599, 15602, 15607, 15608, 15612, 15615, 15619, 15621, 15629, 15631, 15636, 15639, 15644, 15647, 15650, 15652, 15653, 15656, 15658, 15664, 15665, 15669, 15672, 15675, 15679, 15683, 15684, 15685, 15686, 15687, 15690, 15694, 15697, 15703, 15704, 15709, 15727, 15730, 15733, 15736, 15739, 15745, 15747, 15749, 15756, 15760, 15761, 15763, 15767, 15774, 15775, 15776, 15777, 15779, 15780, 15783, 15787, 15796, 15801, 15805, 15807, 15808, 15809, 15811, 15814, 15821, 15824, 15825, 15831, 15838, 15841, 15842, 15843, 15846, 15850, 15852, 15853, 15856, 15858, 15864, 15872, 15876, 15881, 15882, 15885, 15887, 15888, 15891, 15894, 15899, 15902, 15903, 15912, 15914, 15917, 15918, 15919, 15922, 15923, 15924, 15925, 15926, 15927, 15933, 15936, 15941, 15943, 15944, 15946, 15948, 15959, 15961, 15963, 15971, 15976, 15977, 15982, 15984, 15986, 15987, 15990, 15993, 15995, 15996, 15998, 16002, 16004, 16010, 16013, 16014, 16021, 16023, 16026, 16030, 16033, 16034, 16035, 16041, 16042, 16044, 16045, 16049, 16051, 16053, 16058, 16060, 16061, 16066, 16073, 16074, 16075, 16076, 16078, 16080, 16085, 16087, 16092, 16099, 16103, 16106, 16119, 16123, 16127, 16128, 16134, 16136, 16138, 16141, 16142, 16143, 16144, 16148, 16150, 16152, 16153, 16162, 16171, 16172, 16174, 16176, 16181, 16184, 16185, 16186, 16188, 16194, 16195, 16201, 16209, 16210, 16216, 16219, 16223, 16224, 16227, 16228, 16232, 16237, 16240, 16241, 16242, 16243, 16247, 16258, 16259, 16260, 16261, 16264, 16270, 16273, 16280, 16281, 16283, 16293, 16296, 16300, 16305, 16306, 16310, 16317, 16322, 16332, 16333, 16336, 16341, 16343, 16349, 16352, 16353, 16355, 16357, 16360, 16361, 16367, 16368, 16369, 16370, 16371, 16372, 16375, 16378, 16379, 16381, 16387, 16391, 16392, 16394, 16399, 16400, 16405, 16406, 16409, 16411, 16421, 16423, 16435, 16438, 16439, 16449, 16450, 16451, 16472, 16474, 16475, 16479, 16490, 16498, 16499, 16500, 16504, 16516, 16518, 16519, 16521, 16525, 16526, 16536, 16537, 16543, 16546, 16549, 16550, 16553, 16554, 16555, 16561, 16568, 16570, 16571, 16573, 16575, 16576, 16577, 16579, 16582, 16587, 16590, 16592, 16594, 16601, 16606, 16608, 16610, 16612, 16616, 16617, 16622, 16625, 16626, 16627, 16629, 16635, 16636, 16640, 16642, 16646, 16651, 16655, 16663, 16672, 16676, 16678, 16679, 16680, 16681, 16682, 16683, 16685, 16696, 16699, 16700, 16708, 16709, 16712, 16715, 16717, 16718, 16722, 16724, 16736, 16738, 16740, 16745, 16748, 16752, 16753, 16755, 16757, 16761, 16765, 16770, 16773, 16774, 16779, 16784, 16785, 16788, 16790, 16791, 16794, 16796, 16797, 16808, 16810, 16811, 16813, 16822, 16824, 16827, 16836, 16839, 16843, 16847, 16848, 16850, 16855, 16861, 16866, 16868, 16874, 16888, 16895, 16897, 16899, 16901, 16903, 16907, 16908, 16909, 16911, 16914, 16917, 16918, 16927, 16930, 16932, 16934, 16946, 16949, 16953, 16958, 16961, 16963, 16964, 16970, 16975, 16976, 16979, 16981, 16982, 16985, 16988, 16992, 16994, 16995, 16999, 17000, 17001, 17002, 17005, 17006, 17007, 17009, 17012, 17018, 17021, 17022, 17024, 17027, 17028, 17030, 17034, 17036, 17041, 17047, 17052, 17053, 17054, 17055, 17058, 17060, 17067, 17069, 17072, 17074, 17078, 17079, 17083, 17085, 17086, 17089, 17094, 17098, 17103, 17105, 17106, 17111, 17113, 17121, 17122, 17123, 17125, 17130, 17133, 17134, 17135, 17137, 17138, 17155, 17159, 17162, 17163, 17174, 17175, 17177, 17180, 17184, 17187, 17190, 17192, 17193, 17198, 17203, 17206, 17208, 17210, 17216, 17217, 17218, 17229, 17230, 17232, 17236, 17237, 17238, 17240, 17241, 17245, 17251, 17255, 17260, 17261, 17263, 17272, 17275, 17276, 17279, 17280, 17282, 17287, 17290, 17292, 17294, 17295, 17298, 17302, 17311, 17320, 17321, 17322, 17328, 17331, 17334, 17341, 17344, 17346, 17347, 17350, 17355, 17359, 17362, 17366, 17369, 17370, 17372, 17375, 17378, 17379, 17382, 17384, 17385, 17386, 17388, 17393, 17396, 17399, 17400, 17407, 17410, 17414, 17417, 17421, 17426, 17429, 17432, 17443, 17452, 17457, 17461, 17462, 17464, 17465, 17467, 17469, 17470, 17472, 17478, 17481, 17483, 17484, 17492, 17493, 17496, 17514, 17515, 17519, 17528, 17530, 17535, 17538, 17540, 17548, 17551, 17556, 17557, 17567, 17575, 17581, 17583, 17585, 17588, 17591, 17595, 17602, 17603, 17604, 17605, 17606, 17608, 17610, 17617, 17621, 17630, 17631, 17639, 17640, 17644, 17646, 17647, 17651, 17652, 17658, 17661, 17663, 17664, 17665, 17666, 17667, 17670, 17671, 17674, 17675, 17683, 17684, 17690, 17691, 17694, 17696, 17697, 17702, 17708, 17711, 17714, 17719, 17720, 17721, 17722, 17734, 17735, 17739, 17751, 17758, 17760, 17761, 17764, 17765, 17766, 17773, 17777, 17778, 17780, 17781, 17784, 17785, 17794, 17796, 17798, 17801, 17803, 17807, 17808, 17810, 17811, 17812, 17814, 17815, 17817, 17821, 17827, 17832, 17836, 17838, 17843, 17850, 17857, 17858, 17863, 17864, 17865, 17866, 17868, 17872, 17876, 17884, 17886, 17887, 17889, 17892, 17895, 17896, 17904, 17909, 17919, 17922, 17928, 17931, 17932, 17935, 17943, 17947, 17948, 17949, 17950, 17952, 17954, 17959, 17962, 17966, 17969, 17971, 17973, 17977, 17978, 17988, 17989, 17990, 17996, 17997, 18000, 18004, 18008, 18012, 18014, 18018, 18019, 18021, 18022, 18026, 18033, 18036, 18037, 18045, 18054, 18056, 18057, 18065, 18067, 18071, 18088, 18092, 18094, 18099, 18101, 18104, 18108, 18109, 18113, 18114, 18119, 18120, 18121, 18123, 18124, 18126, 18129, 18130, 18134, 18138, 18139, 18144, 18154, 18159, 18160, 18166, 18167, 18171, 18172, 18176, 18177, 18179, 18182, 18184, 18185, 18190, 18194, 18199, 18203, 18208, 18213, 18214, 18215, 18217, 18221, 18223, 18224, 18229, 18241, 18244, 18247, 18250, 18251, 18253, 18255, 18259, 18261, 18262, 18263, 18264, 18265, 18266, 18267, 18268, 18273, 18274, 18275, 18281, 18288, 18290, 18291, 18300, 18308, 18317, 18320, 18322, 18323, 18333, 18334, 18336, 18340, 18341, 18348, 18353, 18356, 18358, 18361, 18363, 18365, 18366, 18370, 18376, 18379, 18384, 18385, 18390, 18398, 18401, 18403, 18411, 18413, 18418, 18419, 18425, 18430, 18431, 18436, 18442, 18443, 18446, 18449, 18452, 18455, 18456, 18459, 18461, 18465, 18472, 18481, 18487, 18490, 18510, 18511, 18516, 18517, 18520, 18529, 18531, 18532, 18535, 18541, 18547, 18548, 18556, 18559, 18568, 18569, 18570, 18573, 18582, 18584, 18585, 18586, 18595, 18596, 18599, 18605, 18609, 18611, 18612, 18615, 18619, 18623, 18626, 18629, 18635, 18637, 18642, 18644, 18646, 18648, 18650, 18656, 18671, 18672, 18673, 18674, 18677, 18681, 18685, 18686, 18695, 18697, 18698, 18700, 18704, 18712, 18715, 18727, 18728, 18729, 18731, 18734, 18735, 18737, 18741, 18746, 18753, 18757, 18759, 18763, 18764, 18773, 18774, 18775, 18779, 18780, 18786, 18788, 18792, 18794, 18795, 18799, 18801, 18804, 18805, 18812, 18814, 18819, 18825, 18827, 18831, 18837, 18838, 18839, 18844, 18847, 18849, 18852, 18853, 18877, 18885, 18887, 18888, 18889, 18892, 18893, 18897, 18898, 18899, 18901, 18902, 18903, 18906, 18910, 18911, 18912, 18916, 18918, 18920, 18921, 18927, 18930, 18931, 18942, 18943, 18944, 18948, 18950, 18954, 18960, 18966, 18967, 18968, 18969, 18980, 18981, 18982, 18983, 18987, 18988, 18989, 18994, 18995, 18996, 18998, 18999, 19002, 19005, 19016, 19017, 19031, 19033, 19037, 19038, 19039, 19040, 19043, 19056, 19058, 19059, 19062, 19065, 19066, 19069, 19071, 19074, 19075, 19079, 19082, 19083, 19087, 19092, 19093, 19096, 19098, 19099, 19107, 19110, 19116, 19117, 19119, 19120, 19126, 19128, 19129, 19131, 19141, 19142, 19144, 19145, 19149, 19150, 19151, 19152, 19153, 19156, 19157, 19160, 19164, 19167, 19175, 19182, 19195, 19201, 19206, 19209, 19218, 19222, 19223, 19226, 19228, 19229, 19234, 19239, 19240, 19243, 19247, 19256, 19263, 19265, 19269, 19270, 19272, 19279, 19280, 19282, 19294, 19296, 19297, 19312, 19316, 19325, 19326, 19328, 19330, 19341, 19342, 19344, 19350, 19353, 19355, 19358, 19359, 19370, 19371, 19373, 19375, 19383, 19390, 19391, 19395, 19397, 19401, 19402, 19405, 19408, 19411, 19412, 19416, 19418, 19426, 19428, 19432, 19434, 19436, 19440, 19450, 19451, 19452, 19453, 19455, 19457, 19461, 19462, 19463, 19467, 19469, 19470, 19472, 19478, 19484, 19489, 19491, 19492, 19493, 19496, 19497, 19500, 19508, 19510, 19511, 19513, 19523, 19527, 19530, 19536, 19538, 19543, 19547, 19548, 19549, 19550, 19551, 19552, 19558, 19559, 19560, 19568, 19570, 19577, 19585, 19588, 19589, 19590, 19596, 19599, 19602, 19604, 19606, 19608, 19610, 19611, 19612, 19617, 19619, 19620, 19626, 19637, 19644, 19646, 19648, 19649, 19653, 19661, 19662, 19666, 19668, 19677, 19685, 19686, 19687, 19688, 19692, 19693, 19700, 19701, 19706, 19708, 19709, 19710, 19712, 19714, 19717, 19722, 19724, 19729, 19731, 19733, 19741, 19742, 19750, 19755, 19761, 19762, 19763, 19764, 19777, 19780, 19794, 19797, 19798, 19803, 19806, 19807, 19812, 19814, 19817, 19818, 19823, 19826, 19833, 19837, 19839, 19840, 19841, 19842, 19850, 19851, 19855, 19858, 19866, 19868, 19876, 19877, 19885, 19897, 19900, 19901, 19906, 19908, 19912, 19914, 19915, 19916, 19918, 19922, 19928, 19936, 19938, 19940, 19945, 19946, 19950, 19952, 19959, 19965, 19972, 19975, 19977, 19978, 19980, 19986, 19990, 19992, 19994, 19995, 19996, 20004, 20010, 20011, 20012, 20013, 20018, 20023, 20024, 20025, 20037, 20038, 20047, 20049, 20053, 20057, 20061, 20062, 20066, 20068, 20069, 20075, 20083, 20084, 20089, 20091, 20100, 20102, 20103, 20105, 20106, 20111, 20113, 20114, 20115, 20116, 20117, 20121, 20122, 20125, 20128, 20129, 20132, 20133, 20134, 20140, 20145, 20147, 20152, 20154, 20159, 20169, 20173, 20174, 20176, 20183, 20187, 20192, 20194, 20200, 20204, 20207, 20208, 20212, 20214, 20217, 20218, 20220, 20223, 20226, 20230, 20232, 20235, 20241, 20243, 20244, 20245, 20248, 20249, 20250, 20252, 20253, 20256, 20257, 20268, 20273, 20274, 20276, 20279, 20280, 20291, 20292, 20303, 20308, 20309, 20320, 20321, 20322, 20328, 20332, 20334, 20336, 20341, 20351, 20352, 20354, 20355, 20356, 20360, 20361, 20365, 20370, 20372, 20376, 20377, 20378, 20381, 20385, 20389, 20394, 20395, 20396, 20402, 20403, 20407, 20409, 20410, 20411, 20421, 20424, 20425, 20431, 20434, 20435, 20436, 20445, 20446, 20447, 20448, 20450, 20453, 20456, 20457, 20460, 20463, 20466, 20468, 20473, 20476, 20480, 20486, 20496, 20498, 20506, 20508, 20509, 20514, 20515, 20518, 20519, 20528, 20529, 20531, 20535, 20536, 20537, 20547, 20549, 20552, 20553, 20554, 20555, 20558, 20559, 20566, 20567, 20568, 20570, 20574, 20575, 20580, 20583, 20584, 20589, 20593, 20597, 20598, 20600, 20607, 20611, 20612, 20621, 20626, 20632, 20634, 20635, 20637, 20643, 20644, 20648, 20651, 20656, 20657, 20665, 20671, 20677, 20678, 20680, 20683, 20684, 20686, 20688, 20690, 20699, 20703, 20704, 20708, 20711, 20713, 20716, 20720, 20725, 20728, 20734, 20735, 20744, 20745, 20750, 20751, 20752, 20759, 20767, 20770, 20790, 20791, 20792, 20793, 20794, 20795, 20804, 20812, 20813, 20816, 20821, 20822, 20825, 20828, 20829, 20833, 20835, 20836, 20839, 20844, 20848, 20849, 20854, 20860, 20863, 20866, 20867, 20868, 20874, 20878, 20881, 20883, 20884, 20885, 20886, 20891, 20898, 20899, 20901, 20903, 20907, 20908, 20909, 20910, 20914, 20917, 20924, 20931, 20936, 20938, 20940, 20941, 20943, 20944, 20946, 20950, 20951, 20953, 20958, 20964, 20967, 20974, 20976, 20979, 20982, 20986, 20991, 20992, 20994, 20998, 21000, 21003, 21014, 21015, 21022, 21025, 21026, 21029, 21030, 21035, 21037, 21040, 21042, 21043, 21049, 21050, 21057, 21058, 21065, 21068, 21070, 21082, 21085, 21088, 21090, 21092, 21094, 21098, 21103, 21104, 21105, 21107, 21108, 21109, 21111, 21113, 21115, 21118, 21122, 21123, 21129, 21132, 21134, 21137, 21139, 21140, 21144, 21146, 21149, 21150, 21153, 21155, 21156, 21160, 21163, 21165, 21167, 21168, 21175, 21178, 21180, 21183, 21184, 21193, 21202, 21207, 21210, 21215, 21216, 21217, 21227, 21230, 21237, 21241, 21244, 21246, 21252, 21255, 21256, 21258, 21262, 21273, 21275, 21277, 21279, 21282, 21283, 21284, 21286, 21290, 21303, 21304, 21305, 21306, 21307, 21308, 21312, 21318, 21319, 21320, 21322, 21323, 21324, 21328, 21329, 21330, 21339, 21341, 21347, 21348, 21352, 21353, 21359, 21361, 21362, 21364, 21366, 21370, 21371, 21373, 21375, 21376, 21378, 21379, 21384, 21397, 21398, 21399, 21401, 21403, 21410, 21416, 21419, 21427, 21429, 21431, 21432, 21435, 21440, 21447, 21450, 21451, 21452, 21454, 21463, 21465, 21466, 21469, 21471, 21475, 21480, 21481, 21483, 21484, 21485, 21486, 21487, 21488, 21489, 21491, 21493, 21497, 21503, 21504, 21508, 21509, 21510, 21512, 21523, 21525, 21529, 21530, 21531, 21536, 21546, 21551, 21552, 21554, 21555, 21559, 21561, 21564, 21568, 21573, 21577, 21578, 21582, 21589, 21592, 21593, 21594, 21596, 21597, 21598, 21603, 21604, 21607, 21613, 21614, 21615, 21617, 21620, 21622, 21623, 21625, 21626, 21627, 21628, 21629, 21633, 21634, 21635, 21638, 21645, 21647, 21652, 21653, 21654, 21662, 21666, 21667, 21669, 21672, 21673, 21676, 21684, 21686, 21690, 21693, 21705, 21707, 21714, 21719, 21721, 21723, 21726, 21727, 21730, 21731, 21735, 21736, 21738, 21742, 21749, 21752, 21753, 21754, 21757, 21758, 21759, 21764, 21769, 21771, 21773, 21786, 21788, 21791, 21797, 21798, 21805, 21811, 21818, 21821, 21826, 21828, 21829, 21830, 21833, 21834, 21835, 21837, 21846, 21847, 21858, 21862, 21865, 21866, 21869, 21870, 21871, 21873, 21880, 21881, 21891, 21893, 21895, 21897, 21901, 21904, 21908, 21914, 21922, 21926, 21927, 21931, 21945, 21951, 21953, 21954, 21955, 21960, 21963, 21974, 21979, 21986, 21987, 21988, 21990, 21994, 21996, 22003, 22004, 22010, 22017, 22018, 22019, 22020, 22026, 22028, 22035, 22040, 22043, 22045, 22049, 22052, 22055, 22059, 22070, 22071, 22073, 22074, 22076, 22078, 22083, 22087, 22091, 22092, 22095, 22102, 22104, 22105, 22108, 22115, 22118, 22119, 22120, 22122, 22123, 22128, 22131, 22134, 22139, 22144, 22145, 22148, 22150, 22154, 22156, 22157, 22162, 22163, 22165, 22169, 22178, 22180, 22181, 22183, 22184, 22190, 22199, 22201, 22203, 22207, 22210, 22214, 22216, 22219, 22221, 22224, 22227, 22228, 22229, 22232, 22237, 22238, 22239, 22240, 22245, 22249, 22252, 22259, 22264, 22265, 22267, 22269, 22274, 22275, 22276, 22277, 22278, 22281, 22283, 22285, 22289, 22294, 22296, 22297, 22306, 22308, 22310, 22311, 22312, 22315, 22316, 22317, 22320, 22322, 22327, 22332, 22333, 22336, 22338, 22340, 22341, 22343, 22349, 22350, 22354, 22364, 22366, 22368, 22371, 22379, 22380, 22383, 22385, 22387, 22390, 22393, 22394, 22395, 22396, 22401, 22406, 22413, 22416, 22417, 22418, 22420, 22424, 22425, 22426, 22427, 22429, 22431, 22436, 22437, 22439, 22440, 22441, 22442, 22443, 22449, 22450, 22461, 22464, 22480, 22481, 22482, 22485, 22492, 22493, 22497, 22500, 22505, 22508, 22510, 22511, 22515, 22517, 22519, 22520, 22521, 22523, 22525, 22526, 22530, 22532, 22537, 22538, 22540, 22541, 22544, 22545, 22547, 22549, 22553, 22557, 22561, 22563, 22566, 22568, 22570, 22572, 22577, 22578, 22580, 22583, 22584, 22586, 22587, 22588, 22590, 22592, 22599, 22602, 22603, 22606, 22607, 22610, 22612, 22624, 22625, 22626, 22632, 22636, 22637, 22638, 22641, 22648, 22655, 22659, 22662, 22663, 22664, 22669, 22671, 22672, 22673, 22674, 22680, 22684, 22685, 22686, 22692, 22697, 22700, 22707, 22711, 22713, 22714, 22716, 22726, 22728, 22731, 22739, 22743, 22746, 22749, 22751, 22754, 22756, 22761, 22763, 22766, 22770, 22772, 22774, 22777, 22791, 22792, 22793, 22795, 22796, 22802, 22804, 22805, 22809, 22811, 22815, 22816, 22820, 22823, 22841, 22844, 22849, 22851, 22852, 22853, 22855, 22859, 22862, 22866, 22872, 22877, 22879, 22880, 22883, 22888, 22891, 22897, 22899, 22901, 22903, 22904, 22905, 22907, 22910, 22913, 22914, 22915, 22916, 22918, 22920, 22921, 22924, 22927, 22930, 22933, 22938, 22941, 22945, 22949, 22953, 22954, 22957, 22958, 22960, 22962, 22963, 22969, 22970, 22973, 22976, 22980, 22983, 22986, 22988, 22989, 22990, 22991, 22992, 22993, 22996, 23004, 23006, 23008, 23010, 23014, 23018, 23020, 23024, 23028, 23030, 23031, 23034, 23035, 23037, 23038, 23039, 23040, 23042, 23046, 23047, 23051, 23054, 23058, 23075, 23081, 23082, 23083, 23085, 23087, 23090, 23092, 23093, 23098, 23103, 23104, 23107, 23110, 23111, 23112, 23117, 23118, 23121, 23123, 23126, 23128, 23131, 23132, 23133, 23138, 23142, 23147, 23153, 23154, 23161, 23162, 23164, 23167, 23172, 23173, 23176, 23178, 23182, 23186, 23193, 23194, 23195, 23196, 23197, 23201, 23202, 23205, 23210, 23219, 23221, 23224, 23226, 23230, 23232, 23234, 23236, 23237, 23238, 23243, 23244, 23248, 23249, 23251, 23252, 23258, 23262, 23264, 23269, 23271, 23273, 23274, 23275, 23278, 23280, 23281, 23288, 23292, 23306, 23307, 23309, 23318, 23321, 23322, 23324, 23327, 23332, 23334, 23336, 23341, 23348, 23350, 23351, 23353, 23356, 23360, 23365, 23366, 23367, 23368, 23382, 23389, 23394, 23395, 23397, 23399, 23400, 23408, 23411, 23413, 23415, 23419, 23420, 23422, 23427, 23428, 23432, 23434, 23436, 23438, 23442, 23447, 23451, 23455, 23459, 23474, 23476, 23481, 23491, 23492, 23498, 23503, 23506, 23507, 23510, 23511, 23514, 23516, 23519, 23520, 23523, 23526, 23531, 23532, 23534, 23535, 23536, 23537, 23546, 23550, 23552, 23553, 23554, 23556, 23558, 23559, 23571, 23584, 23596, 23613, 23615, 23617, 23618, 23624, 23627, 23632, 23635, 23636, 23642, 23644, 23649, 23654, 23656, 23663, 23671, 23675, 23678, 23680, 23682, 23687, 23688, 23693, 23695, 23699, 23701, 23708, 23709, 23711, 23712, 23714, 23720, 23721, 23726, 23733, 23735, 23737, 23738, 23743, 23744, 23752, 23754, 23757, 23759, 23760, 23761, 23764, 23771, 23777, 23779, 23781, 23782, 23787, 23788, 23790, 23796, 23808, 23809, 23811, 23814, 23821, 23824, 23825, 23826, 23829, 23830, 23833, 23837, 23838, 23843, 23846, 23849, 23858, 23860, 23867, 23868, 23870, 23874, 23877, 23880, 23883, 23885, 23887, 23890, 23892, 23895, 23897, 23899, 23906, 23912, 23920, 23924, 23927, 23928, 23933, 23935, 23938, 23942, 23943, 23944, 23946, 23949, 23953, 23954, 23956, 23965, 23966, 23971, 23974, 23988, 23995, 23999, 24003, 24004, 24007, 24008, 24013, 24015, 24029, 24030, 24032, 24039, 24042, 24043, 24044, 24063, 24064, 24065, 24067, 24069, 24073, 24074, 24078, 24080, 24082, 24083, 24084, 24088, 24089, 24091, 24092, 24095, 24097, 24100, 24102, 24108, 24109, 24110, 24111, 24114, 24117, 24118, 24120, 24121, 24122, 24123, 24129, 24133, 24140, 24141, 24143, 24147, 24152, 24157, 24158, 24161, 24164, 24166, 24167, 24172, 24181, 24182, 24184, 24187, 24188, 24193, 24204, 24208, 24210, 24224, 24227, 24228, 24233, 24235, 24241, 24244, 24247, 24248, 24249, 24252, 24253, 24257, 24266, 24271, 24272, 24276, 24279, 24280, 24281, 24289, 24295, 24298, 24299, 24302, 24308, 24312, 24327, 24330, 24331, 24335, 24338, 24339, 24349, 24355, 24359, 24360, 24367, 24372, 24374, 24377, 24385, 24387, 24391, 24394, 24396, 24397, 24398, 24400, 24412, 24414, 24417, 24418, 24421, 24424, 24428, 24435, 24439, 24444, 24446, 24448, 24450, 24451, 24454, 24455, 24459, 24460, 24461, 24462, 24463, 24469, 24470, 24475, 24477, 24478, 24480, 24492, 24499, 24510, 24515, 24518, 24519, 24532, 24534, 24535, 24536, 24537, 24539, 24547, 24552, 24559, 24562, 24567, 24569, 24573, 24579, 24580, 24582, 24586, 24588, 24590, 24592, 24593, 24594, 24595, 24601, 24608, 24610, 24613, 24615, 24619, 24622, 24623, 24624, 24625, 24626, 24630, 24632, 24636, 24639, 24640, 24643, 24645, 24647, 24649, 24652, 24653, 24655, 24656, 24657, 24659, 24662, 24664, 24666, 24671, 24674, 24678, 24682, 24687, 24688, 24691, 24710, 24713, 24714, 24718, 24725, 24730, 24731, 24737, 24739, 24740, 24750, 24756, 24759, 24761, 24762, 24774, 24779, 24780, 24785, 24793, 24794, 24797, 24798, 24801, 24803, 24804, 24806, 24807, 24808, 24810, 24820, 24822, 24824, 24825, 24827, 24831, 24847, 24851, 24852, 24860, 24862, 24864, 24865, 24869, 24873, 24876, 24882, 24884, 24886, 24888, 24894, 24897, 24899, 24902, 24903, 24907, 24912, 24920, 24924, 24926, 24930, 24933, 24937, 24938, 24942, 24950, 24955, 24957, 24958, 24959, 24961, 24964, 24966, 24967, 24968, 24971, 24975, 24977, 24979, 24981, 24982, 24994, 24995, 25004, 25009, 25012, 25013, 25017, 25026, 25029, 25030, 25031, 25035, 25049, 25054, 25057, 25059, 25060, 25061, 25063, 25064, 25066, 25069, 25078, 25083, 25085, 25087, 25090, 25097, 25100, 25102, 25103, 25104, 25107, 25110, 25115, 25116, 25120, 25121, 25124, 25127, 25135, 25140, 25141, 25151, 25152, 25154, 25157, 25160, 25164, 25168, 25173, 25177, 25198, 25207, 25208, 25209, 25210, 25212, 25213, 25214, 25217, 25218, 25219, 25222, 25223, 25225, 25229, 25230, 25231, 25232, 25234, 25235, 25244, 25255, 25258, 25274, 25280, 25282, 25284, 25289, 25297, 25299, 25300, 25304, 25305, 25307, 25317, 25318, 25321, 25323, 25326, 25329, 25334, 25338, 25339, 25340, 25344, 25349, 25351, 25354, 25356, 25357, 25361, 25364, 25369, 25372, 25381, 25384, 25386, 25390, 25394, 25395, 25412, 25416, 25417, 25419, 25429, 25432, 25433, 25442, 25446, 25452, 25456, 25457, 25458, 25462, 25463, 25466, 25469, 25473, 25475, 25477, 25481, 25487, 25489, 25498, 25504, 25505, 25506, 25507, 25518, 25520, 25521, 25523, 25524, 25532, 25536, 25537, 25538, 25539, 25540, 25542, 25544, 25546, 25547, 25551, 25554, 25568, 25578, 25579, 25581, 25588, 25590, 25591, 25603, 25605, 25606, 25612, 25618, 25621, 25624, 25630, 25634, 25635, 25638, 25640, 25643, 25653, 25656, 25659, 25661, 25664, 25665, 25668, 25673, 25686, 25689, 25696, 25697, 25699, 25700, 25702, 25704, 25705, 25707, 25708, 25710, 25711, 25720, 25723, 25724, 25728, 25729, 25732, 25733, 25734, 25743, 25751, 25754, 25763, 25764, 25766, 25767, 25769, 25772, 25775, 25779, 25780, 25785, 25787, 25790, 25794, 25796, 25800, 25801, 25802, 25808, 25812, 25814, 25817, 25818, 25820, 25823, 25828, 25829, 25833, 25835, 25838, 25840, 25844, 25855, 25856, 25858, 25860, 25861, 25863, 25865, 25866, 25871, 25873, 25874, 25876, 25881, 25882, 25885, 25888, 25889, 25891, 25895, 25901, 25902, 25907, 25916, 25921, 25928, 25929, 25934, 25935, 25937, 25938, 25943, 25946, 25949, 25950, 25952, 25953, 25957, 25961, 25964, 25965, 25966, 25970, 25972, 25975, 25984, 25985, 25986, 25987, 25988, 25996, 25997, 25998, 25999, 26000, 26003, 26004, 26010, 26017, 26023, 26026, 26027, 26031, 26039, 26046, 26048, 26049, 26055, 26061, 26066, 26069, 26077, 26079, 26080, 26083, 26085, 26087, 26090, 26091, 26092, 26095, 26096, 26102, 26103, 26109, 26115, 26119, 26137, 26139, 26140, 26141, 26145, 26149, 26150, 26156, 26158, 26160, 26162, 26165, 26170, 26174, 26175, 26179, 26181, 26183, 26185, 26186, 26190, 26195, 26200, 26205, 26208, 26210, 26211, 26212, 26224, 26228, 26230, 26231, 26233, 26236, 26237, 26239, 26252, 26260, 26261, 26263, 26267, 26268, 26270, 26274, 26285, 26286, 26287, 26292, 26294, 26296, 26297, 26298, 26300, 26311, 26315, 26317, 26318, 26320, 26324, 26328, 26334, 26341, 26342, 26350, 26351, 26353, 26355, 26358, 26359, 26365, 26366, 26369, 26370, 26371, 26375, 26379, 26384, 26392, 26396, 26397, 26398, 26399, 26402, 26406, 26411, 26412, 26421, 26422, 26425, 26432, 26435, 26443, 26445, 26451, 26452, 26455, 26459, 26467, 26469, 26470, 26471, 26472, 26473, 26477, 26485, 26488, 26491, 26498, 26506, 26510, 26513, 26514, 26520, 26521, 26523, 26528, 26534, 26535, 26539, 26543, 26550, 26551, 26562, 26572, 26578, 26586, 26590, 26596, 26597, 26601, 26603, 26604, 26606, 26607, 26608, 26612, 26618, 26619, 26620, 26628, 26630, 26636, 26639, 26642, 26643, 26647, 26648, 26659, 26666, 26668, 26679, 26685, 26686, 26688, 26690, 26698, 26700, 26708, 26710, 26711, 26712, 26714, 26715, 26716, 26717, 26726, 26727, 26728, 26733, 26734, 26736, 26740, 26741, 26744, 26745, 26746, 26748, 26749, 26750, 26751, 26753, 26754, 26757, 26761, 26764, 26767, 26772, 26777, 26780, 26785, 26788, 26790, 26792, 26795, 26797, 26798, 26799, 26803, 26804, 26805, 26809, 26814, 26816, 26817, 26818, 26821, 26826, 26831, 26835, 26838, 26839, 26843, 26844, 26845, 26848, 26853, 26855, 26858, 26861, 26865, 26870, 26879, 26880, 26881, 26883, 26885, 26886, 26887, 26896, 26900, 26901, 26905, 26913, 26915, 26919, 26921, 26922, 26924, 26927, 26940, 26942, 26946, 26948, 26949, 26950, 26954, 26961, 26964, 26965, 26969, 26977, 26983, 26984, 26985, 26986, 26989, 26990, 26991, 26999, 27001, 27013, 27014, 27020, 27022, 27023, 27029, 27030, 27035, 27038, 27040, 27041, 27042, 27043, 27045, 27047, 27052, 27053, 27054, 27056, 27058, 27060, 27062, 27065, 27067, 27068, 27071, 27074, 27075, 27081, 27089, 27090, 27094, 27095, 27100, 27103, 27106, 27109, 27110, 27112, 27118, 27119, 27121, 27123, 27126, 27129, 27130, 27131, 27132, 27136, 27144, 27145, 27147, 27149, 27150, 27151, 27153, 27154, 27155, 27158, 27160, 27161, 27162, 27165, 27170, 27182, 27185, 27189, 27190, 27193, 27195, 27196, 27198, 27209, 27213, 27215, 27217, 27220, 27223, 27224, 27225, 27228, 27235, 27237, 27242, 27245, 27247, 27248, 27251, 27252, 27253, 27254, 27256, 27261, 27269, 27273, 27281, 27289, 27290, 27291, 27292, 27296, 27297, 27299, 27309, 27310, 27311, 27316, 27318, 27322, 27323, 27324, 27329, 27332, 27334, 27335, 27336, 27337, 27343, 27349, 27351, 27352, 27354, 27355, 27356, 27357, 27360, 27361, 27366, 27372, 27378, 27379, 27380, 27391, 27392, 27397, 27398, 27402, 27409, 27411, 27418, 27421, 27427, 27428, 27429, 27434, 27438, 27439, 27443, 27444, 27448, 27456, 27458, 27459, 27462, 27464, 27476, 27478, 27479, 27480, 27481, 27483, 27486, 27494, 27495, 27496, 27504, 27506, 27507, 27513, 27514, 27515, 27517, 27520, 27529, 27530, 27531, 27535, 27544, 27545, 27547, 27548, 27555, 27561, 27563, 27565, 27567, 27569, 27570, 27573, 27574, 27575, 27576, 27577, 27579, 27582, 27586, 27588, 27589, 27591, 27593, 27594, 27597, 27598, 27599, 27601, 27602, 27606, 27607, 27610, 27612, 27613, 27620, 27621, 27622, 27624, 27628, 27631, 27637, 27638, 27640, 27643, 27644, 27645, 27647, 27649, 27652, 27655, 27656, 27659, 27661, 27662, 27663, 27665, 27667, 27668, 27669, 27672, 27673, 27676, 27679, 27680, 27682, 27684, 27687, 27688, 27691, 27694, 27703, 27705, 27708, 27711, 27718, 27721, 27722, 27723, 27726, 27730, 27731, 27732, 27736, 27738, 27739, 27742, 27743, 27745, 27748, 27749, 27751, 27753, 27758, 27766, 27780, 27785, 27791, 27793, 27799, 27801, 27803, 27804, 27805, 27806, 27807, 27808, 27816, 27821, 27825, 27827, 27834, 27840, 27843, 27845, 27851, 27855, 27856, 27863, 27864, 27866, 27875, 27877, 27878, 27879, 27883, 27885, 27886, 27887, 27888, 27889, 27891, 27894, 27901, 27902, 27904, 27911, 27912, 27913, 27916, 27918, 27919, 27921, 27925, 27928, 27929, 27939, 27940, 27942, 27946, 27949, 27952, 27959, 27961, 27964, 27967, 27971, 27975, 27977, 27979, 27981, 27984, 27994, 27995, 27996, 27999, 28000, 28005, 28008, 28012, 28015, 28016, 28019, 28020, 28021, 28026, 28029, 28031, 28037, 28039, 28042, 28045, 28047, 28049, 28050, 28052, 28057, 28060, 28063, 28064, 28065, 28068, 28071, 28072, 28073, 28077, 28079, 28080, 28084, 28085, 28086, 28087, 28089, 28090, 28092, 28093, 28094, 28097, 28104, 28105, 28108, 28109, 28110, 28113, 28117, 28118, 28119, 28123, 28125, 28126, 28127, 28128, 28129, 28130, 28133, 28140, 28143, 28151, 28152, 28155, 28156, 28159, 28160, 28161, 28162, 28163, 28165, 28166, 28167, 28169, 28170, 28172, 28175, 28176, 28182, 28190, 28192, 28193, 28198, 28199, 28202, 28207, 28210, 28211, 28212, 28215, 28218, 28219, 28221, 28223, 28226, 28230, 28231, 28234, 28236, 28238, 28242, 28243, 28244, 28246, 28247, 28254, 28257, 28259, 28262, 28267, 28271, 28272, 28277, 28279, 28280, 28281, 28289, 28292, 28293, 28317, 28318, 28321, 28323, 28324, 28336, 28337, 28338, 28339, 28341, 28351, 28354, 28356, 28358, 28362, 28364, 28371, 28374, 28380, 28382, 28390, 28394, 28395, 28397, 28404, 28407, 28414, 28419, 28422, 28428, 28429, 28432, 28435, 28439, 28441, 28445, 28453, 28456, 28457, 28460, 28462, 28466, 28471, 28477, 28478, 28479, 28482, 28485, 28487, 28488, 28492, 28493, 28496, 28498, 28506, 28510, 28513, 28514, 28516, 28518, 28522, 28523, 28524, 28525, 28529, 28532, 28534, 28539, 28543, 28544, 28549, 28551, 28556, 28560, 28563, 28564, 28565, 28567, 28568, 28571, 28573, 28578, 28584, 28595, 28597, 28601, 28603, 28604, 28605, 28607, 28608, 28609, 28611, 28619, 28620, 28621, 28623, 28625, 28626, 28629, 28632, 28634, 28636, 28643, 28644, 28648, 28651, 28652, 28654, 28665, 28666, 28667, 28672, 28673, 28675, 28676, 28684, 28690, 28692, 28693, 28700, 28701, 28706, 28707, 28716, 28717, 28723, 28724, 28732, 28733, 28734, 28737, 28749, 28753, 28754, 28764, 28768, 28771, 28773, 28774, 28776, 28778, 28780, 28784, 28785, 28790, 28791, 28792, 28802, 28805, 28807, 28808, 28813, 28815, 28820, 28823, 28827, 28829, 28831, 28834, 28835, 28842, 28844, 28845, 28846, 28849, 28855, 28861, 28862, 28864, 28866, 28867, 28874, 28879, 28886, 28892, 28893, 28894, 28896, 28897, 28907, 28910, 28914, 28915, 28916, 28918, 28919, 28922, 28924, 28925, 28929, 28931, 28932, 28933, 28934, 28939, 28941, 28943, 28944, 28946, 28951, 28955, 28959, 28962, 28966, 28968, 28973, 28974, 28975, 28977, 28991, 28996, 28997, 28998, 29000, 29003, 29005, 29010, 29011, 29016, 29026, 29036, 29037, 29041, 29044, 29048, 29050, 29056, 29058, 29070, 29073, 29078, 29080, 29081, 29084, 29086, 29087, 29088, 29092, 29102, 29104, 29115, 29116, 29118, 29120, 29121, 29122, 29123, 29130, 29131, 29134, 29141, 29144, 29145, 29146, 29148, 29151, 29152, 29155, 29156, 29157, 29158, 29161, 29162, 29167, 29168, 29176, 29182, 29184, 29185, 29186, 29190, 29193, 29197, 29199, 29200, 29202, 29203, 29210, 29215, 29216, 29218, 29219, 29222, 29231, 29233, 29236, 29237, 29239, 29240, 29241, 29244, 29260, 29263, 29270, 29273, 29274, 29276, 29283, 29284, 29290, 29294, 29295, 29304, 29311, 29314, 29315, 29319, 29324, 29325, 29326, 29328, 29329, 29330, 29334, 29338, 29341, 29342, 29343, 29346, 29347, 29350, 29353, 29356, 29358, 29359, 29362, 29365, 29375, 29376, 29378, 29379, 29380, 29381, 29383, 29388, 29389, 29392, 29400, 29402, 29403, 29408, 29412, 29413, 29416, 29419, 29420, 29422, 29427, 29431, 29433, 29434, 29438, 29442, 29453, 29455, 29457, 29461, 29465, 29471, 29472, 29480, 29481, 29485, 29492, 29493, 29500, 29502, 29504, 29505, 29506, 29508, 29509, 29514, 29517, 29518, 29520, 29521, 29524, 29526, 29529, 29530, 29531, 29544, 29546, 29552, 29553, 29554, 29556, 29560, 29561, 29566, 29569, 29573, 29574, 29575, 29580, 29581, 29584, 29586, 29589, 29593, 29595, 29597, 29598, 29600, 29602, 29603, 29610, 29614, 29619, 29621, 29622, 29624, 29630, 29632, 29635, 29636, 29638, 29639, 29642, 29644, 29645, 29647, 29649, 29656, 29657, 29660, 29661, 29666, 29668, 29670, 29674, 29679, 29682, 29683, 29686, 29688, 29690, 29691, 29692, 29695, 29700, 29705, 29706, 29712, 29713, 29722, 29725, 29726, 29739, 29741, 29742, 29743, 29744, 29749, 29753, 29755, 29758, 29760, 29762, 29770, 29772, 29773, 29778, 29779, 29786, 29790, 29796, 29804, 29810, 29811, 29815, 29823, 29824, 29828, 29830, 29831, 29834, 29836, 29837, 29838, 29839, 29844, 29851, 29854, 29856, 29857, 29859, 29860, 29861, 29867, 29869, 29872, 29875, 29880, 29881, 29882, 29884, 29888, 29889, 29891, 29892, 29902, 29904, 29911, 29912, 29913, 29918, 29920, 29927, 29929, 29932, 29934, 29939, 29943, 29946, 29948, 29949, 29954, 29955, 29957, 29958, 29961, 29965, 29966, 29967, 29971, 29972, 29981, 29983, 29985, 29987, 29991, 29992, 29994, 29997, 29999, 30004, 30008, 30011, 30016, 30017, 30018, 30019, 30020, 30022, 30023, 30026, 30027, 30029, 30034, 30039, 30045, 30048, 30049, 30053, 30054, 30060, 30062, 30067, 30070, 30071, 30075, 30079, 30080, 30083, 30085, 30087, 30090, 30091, 30093, 30101, 30102, 30106, 30107, 30109, 30111, 30112, 30113, 30114, 30119, 30121, 30122, 30124, 30126, 30130, 30131, 30132, 30135, 30138, 30145, 30146, 30148, 30150, 30153, 30154, 30156, 30157, 30160, 30162, 30164, 30166, 30170, 30171, 30175, 30177, 30178, 30181, 30186, 30188, 30190, 30191, 30192, 30195, 30196, 30197, 30199, 30200, 30204, 30205, 30206, 30207, 30210, 30214, 30218, 30219, 30220, 30223, 30224, 30225, 30227, 30229, 30231, 30232, 30239, 30244, 30247, 30248, 30251, 30253, 30256, 30257, 30258, 30261, 30265, 30268, 30273, 30275, 30277, 30279, 30281, 30282, 30284, 30285, 30286, 30287, 30292, 30298, 30299, 30301, 30303, 30304, 30308, 30312, 30313, 30314, 30316, 30317, 30318, 30321, 30322, 30330, 30332, 30335, 30336, 30338, 30340, 30341, 30342, 30344, 30345, 30349, 30352, 30356, 30357, 30360, 30371, 30374, 30378, 30383, 30384, 30392, 30395, 30398, 30403, 30404, 30408, 30410, 30412, 30413, 30414, 30420, 30428, 30429, 30431, 30436, 30439, 30441, 30442, 30445, 30447, 30448, 30449, 30453, 30456, 30457, 30458, 30460, 30464, 30474, 30476, 30477, 30480, 30483, 30484, 30486, 30488, 30490, 30495, 30501, 30503, 30504, 30505, 30509, 30513, 30522, 30524, 30534, 30542, 30546, 30551, 30558, 30561, 30566, 30570, 30579, 30582, 30588, 30589, 30590, 30591, 30592, 30593, 30596, 30598, 30599, 30601, 30604, 30606, 30607, 30608, 30609, 30610, 30612, 30614, 30617, 30618, 30622, 30625, 30626, 30629, 30630, 30632, 30635, 30637, 30638, 30640, 30641, 30645, 30646, 30648, 30650, 30651, 30659, 30660, 30661, 30663, 30665, 30666, 30667, 30668, 30669, 30670, 30679, 30682, 30685, 30694, 30695, 30696, 30703, 30707, 30713, 30716, 30717, 30719, 30725, 30726, 30732, 30736, 30737, 30739, 30747, 30748, 30750, 30754, 30755, 30756, 30777, 30779, 30780, 30782, 30790, 30792, 30799, 30805, 30811, 30815, 30819, 30820, 30822, 30825, 30826, 30827, 30850, 30854, 30855, 30861, 30864, 30865, 30868, 30876, 30877, 30881, 30882, 30884, 30887, 30902, 30906, 30908, 30909, 30910, 30911, 30913, 30915, 30919, 30945, 30949, 30950, 30962, 30964, 30966, 30968, 30969, 30978, 30981, 30982, 30983, 30992, 30993, 30994, 30995, 30996, 31000, 31002, 31003, 31004, 31005, 31007, 31016, 31019, 31022, 31025, 31026, 31031, 31033, 31038, 31039, 31041, 31044, 31045, 31048, 31050, 31051, 31052, 31054, 31056, 31058, 31070, 31072, 31073, 31074, 31077, 31082, 31083, 31086, 31088, 31093, 31106, 31108, 31110, 31113, 31115, 31116, 31120, 31121, 31127, 31128, 31129, 31130, 31134, 31135, 31136, 31141, 31142, 31144, 31146, 31153, 31160, 31163, 31165, 31169, 31172, 31175, 31182, 31183, 31189, 31195, 31196, 31197, 31199, 31204, 31205, 31207, 31212, 31215, 31217, 31218, 31219, 31222, 31225, 31229, 31232, 31234, 31239, 31241, 31245, 31246, 31252, 31254, 31258, 31260, 31261, 31264, 31272, 31273, 31276, 31279, 31282, 31284, 31285, 31289, 31291, 31292, 31294, 31298, 31301, 31302, 31304, 31309, 31315, 31319, 31320, 31323, 31330, 31331, 31332, 31337, 31341, 31342, 31357, 31360, 31361, 31365, 31369, 31371, 31379, 31380, 31381, 31382, 31389, 31394, 31397, 31402, 31405, 31413, 31420, 31421, 31425, 31430, 31432, 31437, 31438, 31439, 31441, 31442, 31448, 31453, 31454, 31461, 31463, 31470, 31473, 31479, 31482, 31483, 31485, 31489, 31495, 31496, 31497, 31500, 31502, 31504, 31506, 31508, 31509, 31511, 31514, 31516, 31517, 31523, 31527, 31528, 31529, 31533, 31534, 31535, 31539, 31542, 31544, 31545, 31551, 31556, 31561, 31563, 31565, 31567, 31568, 31576, 31580, 31584, 31591, 31603, 31606, 31609, 31615, 31619, 31620, 31621, 31625, 31626, 31628, 31633, 31649, 31650, 31651, 31658, 31670, 31673, 31676, 31680, 31683, 31685, 31688, 31690, 31695, 31696, 31697, 31698, 31707, 31710, 31714, 31720, 31721, 31723, 31724, 31725, 31728, 31729, 31730, 31735, 31746, 31749, 31757, 31765, 31770, 31772, 31779, 31784, 31791, 31792, 31793, 31797, 31798, 31799, 31802, 31803, 31807, 31808, 31815, 31820, 31821, 31822, 31823, 31829, 31830, 31831, 31834, 31836, 31837, 31838, 31851, 31852, 31853, 31854, 31856, 31857, 31858, 31861, 31864, 31865, 31866, 31867, 31869, 31879, 31881, 31883, 31886, 31889, 31890, 31897, 31899, 31901, 31902, 31903, 31904, 31906, 31909, 31910, 31911, 31918, 31922, 31926, 31928, 31929, 31932, 31938, 31941, 31951, 31953, 31958, 31962, 31964, 31966, 31968, 31969, 31970, 31971, 31973, 31981, 31983, 31984, 31986, 31987, 31994, 32000, 32002, 32005, 32010, 32013, 32020, 32026, 32032, 32034, 32037, 32040, 32042, 32045, 32046, 32049, 32050, 32053, 32054, 32058, 32064, 32074, 32075, 32077, 32079, 32080, 32081, 32083, 32085, 32089, 32091, 32094, 32095, 32106, 32108, 32111, 32114, 32116, 32117, 32118, 32120, 32121, 32123, 32125, 32132, 32135, 32136, 32139, 32143, 32145, 32154, 32158, 32161, 32163, 32168, 32173, 32174, 32175, 32184, 32185, 32188, 32193, 32194, 32196, 32197, 32199, 32209, 32212, 32219, 32222, 32234, 32236, 32237, 32240, 32245, 32254, 32260, 32263, 32269, 32272, 32276, 32277, 32278, 32279, 32281, 32282, 32283, 32285, 32287, 32288, 32291, 32293, 32296, 32303, 32304, 32305, 32312, 32315, 32317, 32318, 32319, 32321, 32323, 32325, 32326, 32327, 32329, 32333, 32339, 32340, 32341, 32342, 32350, 32354, 32356, 32357, 32358, 32359, 32360, 32370, 32373, 32375, 32376, 32377, 32380, 32381, 32382, 32383, 32384, 32385, 32387, 32389, 32393, 32395, 32399, 32404, 32406, 32408, 32411, 32412, 32417, 32419, 32420, 32426, 32427, 32429, 32434, 32437, 32440, 32442, 32445, 32447, 32448, 32452, 32455, 32456, 32457, 32463, 32465, 32467, 32474, 32481, 32483, 32486, 32489, 32493, 32497, 32506, 32511, 32516, 32528, 32543, 32546, 32548, 32549, 32551, 32553, 32554, 32559, 32565, 32566, 32568, 32571, 32574, 32575, 32578, 32582, 32589, 32592, 32594, 32598, 32600, 32606, 32607, 32609, 32612, 32615, 32616, 32618, 32624, 32626, 32628, 32631, 32640, 32646, 32649, 32657, 32658, 32661, 32663, 32665, 32667, 32670, 32671, 32672, 32673, 32674, 32675, 32685, 32687, 32688, 32692, 32694, 32697, 32698, 32702, 32703, 32704, 32712, 32713, 32717, 32721, 32723, 32726, 32730, 32733, 32737, 32738, 32742, 32743, 32749, 32750, 32754, 32757, 32762, 32765, 32768, 32773, 32781, 32787, 32793, 32796, 32797, 32798, 32807, 40000, 40004, 40009, 40033, 40063, 40066, 40070, 40071, 40078, 40084, 40094, 40103, 40124, 40133, 40213, 40246, 40273, 40286, 40288, 40308, 40347, 40356, 40370, 40384, 40386, 40387, 40428, 40442, 40456, 40464, 40474, 40496, 40513, 40514, 40546, 40548, 40569, 40576, 40582, 40586, 40595, 40600, 40610, 40612, 40614, 40623, 40632, 40644, 40686, 40693, 40694, 40701, 40708, 40723, 40729, 40738, 40744, 40771, 40772, 40786, 40789, 40811, 40827, 40837, 40850, 40877, 40897, 40912, 40913, 40972, 40983, 40988, 40991, 40998, 40999, 41004, 41016, 41030, 41040, 41050, 41054, 41070, 41077, 41102, 41119, 41121, 41192, 41194, 41205, 41217, 41219, 41254, 41290, 41315, 41318, 41371, 41383, 41393, 41402, 41408, 41409, 41422, 41441, 41452, 41463, 41520, 41526, 41546, 41550, 41567, 41573, 41581, 41589, 41592, 41631, 41638, 41668, 41678, 41687, 41696, 41701, 41724, 41729, 41738, 41768, 41782, 41785, 41788, 41830, 41842, 41861, 41881, 41882, 41890, 41891, 41934, 41976, 41983, 41984, 42020, 42028, 42035, 42060, 42062, 42066, 42067, 42093, 42106, 42124, 42129, 42130, 42148, 42160, 42173, 42174, 42175, 42184, 42189, 42199, 42210, 42211, 42221, 42222, 42243, 42244, 42255, 42288, 42310, 42321, 42326, 42327, 42351, 42364, 42385, 42402, 42405, 42410, 42417, 42437, 42454, 42455, 42460, 42468, 42482, 42486, 42491, 42504, 42509, 42525, 42545, 42572, 42589, 42590, 42604, 42621, 42649, 42677, 42682, 42694, 42747, 42752, 42755, 42771, 42781, 42782, 42790, 42809, 42811, 42826, 42829, 42830, 42851, 42870, 42903, 42904, 42905, 42920, 42929, 42948, 42956, 42962, 42963, 42982, 42995, 43004, 43017, 43056, 43061, 43066, 43080, 43083, 43084, 43086, 43089, 43107, 43112, 43115, 43120, 43122, 43126, 43138, 43151, 43162, 43206, 43215, 43257, 43274, 43284, 43296, 43322, 43325, 43330, 43337, 43392, 43417, 43431, 43437, 43447, 43450, 43456, 43460, 43478, 43501, 43503, 43509, 43543, 43557, 43559, 43564, 43571, 43589, 43599, 43602, 43613, 43632, 43658, 43668, 43671, 43673, 43686, 43690, 43691, 43697, 43708, 43732, 43737, 43746, 43759, 43776, 43787, 43798, 43799, 43803, 43820, 43827, 43837, 43866, 43870, 43875, 43876, 43878, 43911, 43923, 43925, 43942, 43982, 43990, 44036, 44052, 44058, 44061, 44064, 44073, 44083, 44126, 44135, 44139, 44153, 44173, 44181, 44188, 44214, 44228, 44234, 44245, 44248, 44258, 44265, 44298, 44300, 44303, 44323, 44328, 44332, 44339, 44375, 44382, 44383, 44385, 44388, 44408, 44427, 44430, 44434, 44449, 44450, 44451, 44459, 44464, 44468, 44490, 44491, 44506, 44508, 44521, 44532, 44534, 44539, 44546, 44566, 44589, 44592, 44593, 44597, 44600, 44601, 44622, 44632, 44658, 44666, 44677, 44685, 44715, 44721, 44722, 44723, 44724, 44735, 44741, 44768, 44793, 44795, 44806, 44807, 44812, 44817, 44820, 44837, 44844, 44851, 44856, 44876, 44880, 44902, 44908, 44921, 44957, 44958, 44979, 44990, 45012, 45013, 45017, 45040, 45092, 45099, 45102, 45112, 45115, 45123, 45127, 45131, 45138, 45146, 45152, 45157, 45180, 45183, 45186, 45194, 45199, 45207, 45213, 45227, 45236, 45269, 45272, 45273, 45297, 45300, 45304, 45308, 45316, 45317, 45320, 45326, 45329, 45339, 45342, 45346, 45347, 45381, 45407, 45409, 45410, 45415, 45420, 45425, 45434, 45437, 45441, 45457, 45460, 45477, 45489, 45524, 45536, 45557, 45566, 45583, 45589, 45601, 45604, 45612, 45619, 45628, 45632, 45647, 45672, 45698, 45779, 45788, 45791, 45835, 45838, 45848, 45860, 45866, 45885, 45895, 45910, 45914, 45918, 45933, 45959, 45974, 46007, 46022, 46034, 46036, 46038, 46077, 46080, 46081, 46085, 46105, 46116, 46119, 46120, 46125, 46132, 46148, 46150, 46163, 46198, 46217, 46230, 46251, 46263, 46271, 46287, 46315, 46321, 46330, 46335, 46340, 46355, 46366, 46382, 46392, 46398, 46414, 46422, 46423, 46442, 46446, 46455, 46459, 46462, 46487, 46499, 46521, 46536, 46550, 46551, 46566, 46569, 46576, 46586, 46588, 46590, 46603, 46608, 46620, 46624, 46667, 46668, 46672, 46692, 46693, 46694, 46733, 46734, 46744, 46755, 46756, 46764, 46775, 46781, 46793, 46801, 46816, 46817, 46836, 46845, 46849, 46850, 46851, 46853, 46857, 46880, 46910, 46926, 46935, 46938, 46946, 46947, 46955, 46968, 46994, 47014, 47058, 47087, 47093, 47127, 47129, 47132, 47133, 47157, 47164, 47203, 47219, 47224, 47234, 47250, 47257, 47266, 47270, 47286, 47288, 47289, 47305, 47351, 47357, 47362, 47407, 47428, 47430, 47436, 47450, 47466, 47467, 47469, 47477, 47509, 47513, 47520, 47529, 47532, 47563, 47577, 47578, 47591, 47598, 47618, 47620, 47637, 47647, 47668, 47670, 47683, 47693, 47700, 47712, 47715, 47726, 47739, 47748, 47759, 47785, 47798, 47799, 47800, 47807, 47814, 47852, 47875, 47884, 47887, 47892, 47897, 47906, 47912, 47918, 47927, 47940, 47978, 47995, 48006, 48025, 48042, 48056, 48065, 48090, 48115, 48121, 48124, 48165, 48181, 48184, 48217, 48220, 48222, 48232, 48233, 48239, 48253, 48268, 48290, 48292, 48293, 48304, 48308, 48320, 48327, 48344, 48348, 48351, 48373, 48374, 48379, 48380, 48398, 48409, 48456, 48461, 48479, 48480, 48481, 48483, 48503, 48521, 48523, 48524, 48546, 48553, 48554, 48556, 48612, 48617, 48629, 48632, 48637, 48647, 48674, 48687, 48693, 48707, 48730, 48770, 48774, 48804, 48812, 48827, 48833, 48837, 48843, 48865, 48876, 48913, 48915, 48928, 48946, 48950, 48951, 48969, 48999, 49016, 49023, 49037, 49080, 49104, 49105, 49106, 49133, 49141, 49144, 49151, 49171, 49176, 49188, 49190, 49232, 49249, 49261, 49265, 49268, 49290, 49291, 49296, 49311, 49328, 49339, 49350, 49359, 49370, 49375, 49380, 49392, 49407, 49431, 49435, 49453, 49456, 49457, 49469, 49499, 49510, 49534, 49562, 49566, 49580, 49582, 49583, 49586, 49592, 49599, 49603, 49615, 49619, 49622, 49632, 49634, 49635, 49638, 49649, 49650, 49671, 49683, 49705, 49727, 49739, 49742, 49806, 49820, 49828, 49841, 49922, 49930, 49952, 49955, 49964, 49971, 49984, 49998, 49999, 50010, 50020, 50028, 50043, 50050, 50055, 50061, 50087, 50093, 50099, 50104, 50113, 50148, 50164, 50214, 50216, 50217, 50254, 50259, 50268, 50285, 50289, 50300, 50303, 50305, 50359, 50366, 50370, 50384, 50405, 50407, 50409, 50427, 50434, 50440, 50450, 50476, 50485, 50490, 50495, 50498, 50507, 50520, 50528, 50532, 50537, 50544, 50545, 50579, 50594, 50603, 50620, 50623, 50636, 50640, 50651, 50654, 50672, 50691, 50692, 50710, 50725, 50734, 50756, 50762, 50772, 50793, 50815, 50819, 50826, 50827, 50863, 50877, 50882, 50883, 50889, 50895, 50901, 50911, 50913, 50915, 50928, 50939, 50957, 50962, 50968, 50969, 50981, 50987, 50996, 50998, 51000, 51013, 51027, 51053, 51060, 51076, 51086, 51091, 51104, 51136, 51143, 51157, 51177, 51180, 51203, 51216, 51222, 51226, 51231, 51237, 51258, 51259, 51267, 51270, 51288, 51289, 51291, 51297, 51299, 51321, 51327, 51335, 51343, 51348, 51359, 51364, 51390, 51403, 51404, 51418, 51424, 51458, 51465, 51466, 51506, 51522, 51529, 51538, 51542, 51550, 51551, 51557, 51558, 51577, 51597, 51607, 51611, 51670, 51675, 51698, 51750, 51754, 51771, 51776, 51780, 51786, 51790, 51793, 51796, 51797, 51798, 51802, 51820, 51829, 51841, 51849, 51859, 51864, 51872, 51874, 51878, 51883, 51890, 51891, 51922, 51929, 51933, 51936, 51948, 51952, 51976, 51981, 51986, 51992, 52001, 52002, 52010, 52038, 52040, 52046, 52057, 52065, 52077, 52107, 52109, 52121, 52130, 52136, 52139, 52147, 52162, 52164, 52172, 52186, 52193, 52197, 52205, 52219, 52225, 52228, 52231, 52234, 52249, 52263, 52264, 52269, 52271, 52278, 52288, 52296, 52297, 52355, 52359, 52363, 52389, 52420, 52436, 52441, 52456, 52460, 52467, 52478, 52484, 52509, 52518, 52530, 52532, 52535, 52541, 52566, 52582, 52588, 52593, 52594, 52604, 52615, 52619, 52626, 52631, 52653, 52657, 52666, 52693, 52695, 52697, 52726, 52728, 52778, 52802, 52807, 52808, 52834, 52848, 52853, 52855, 52861, 52875, 52876, 52878, 52899, 52952, 52957, 52985, 52990, 53001, 53020, 53081, 53098, 53102, 53105, 53131, 53149, 53176, 53192, 53220, 53238, 53273, 53288, 53297, 53299, 53309, 53314, 53322, 53341, 53347, 53351, 53371, 53392, 53397, 53399, 53404, 53411, 53437, 53459, 53461, 53463, 53464, 53522, 53541, 53545, 53554, 53577, 53591, 53608, 53619, 53632, 53636, 53654, 53663, 53664, 53673, 53679, 53695, 53724, 53725, 53729, 53734, 53735, 53749, 53751, 53754, 53758, 53759, 53763, 53770, 53771, 53772, 53774, 53802, 53804, 53810, 53822, 53833, 53836, 53842, 53847, 53866, 53870, 53921, 53931, 53944, 53953, 53964, 53973, 53974, 54005, 54015, 54054, 54066, 54090, 54120, 54121, 54132, 54147, 54153, 54183, 54187, 54190, 54193, 54229, 54257, 54274, 54276, 54281, 54289, 54325, 54337, 54355, 54368, 54392, 54397, 54401, 54406, 54420, 54444, 54465, 54471, 54473, 54494, 54502, 54509, 54511, 54514, 54523, 54536, 54537, 54559, 54563, 54564, 54577, 54586, 54587, 54595, 54602, 54604, 54609, 54610, 54613, 54620, 54623, 54625, 54642, 54651, 54661, 54662, 54663, 54679, 54682, 54683, 54689, 54739, 54741, 54757, 54762, 54764, 54765, 54782, 54788, 54794, 54797, 54805, 54814, 54825, 54829, 54830, 54861, 54867, 54877, 54891, 54892, 54910, 54928, 54929, 54930, 54935, 54936, 54950, 54958, 54961, 54964, 54966, 54969, 54987, 54999, 55013, 55023, 55055, 55071, 55078, 55083, 55104, 55105, 55111, 55116, 55138, 55147, 55186, 55193, 55201, 55204, 55238, 55266, 55273, 55281, 55287, 55291, 55308, 55334, 55357, 55363, 55365, 55393, 55398, 55400, 55402, 55407, 55409, 55428, 55441, 55473, 55507, 55515, 55523, 55534, 55543, 55545, 55549, 55557, 55559, 55575, 55585, 55601, 55616, 55659, 55670, 55715, 55716, 55719, 55725, 55743, 55750, 55772, 55801, 55844, 55849, 55853, 55871, 55878, 55895, 55896, 55901, 55904, 55910, 55912, 55930, 55935, 55963, 55972, 55992, 56009, 56027, 56032, 56038, 56042, 56045, 56046, 56058, 56060, 56070, 56098, 56108, 56110, 56120, 56168, 56179, 56184, 56191, 56200, 56201, 56204, 56209, 56212, 56224, 56225, 56257, 56269, 56283, 56287, 56289, 56301, 56304, 56316, 56317, 56321, 56356, 56361, 56385, 56391, 56394, 56405, 56407, 56418, 56440, 56443, 56447, 56450, 56456, 56458, 56462, 56468, 56476, 56484, 56492, 56507, 56517, 56549, 56552, 56570, 56572, 56579, 56611, 56613, 56621, 56636, 56652, 56661, 56674, 56678, 56707, 56714, 56724, 56740, 56746, 56816, 56825, 56836, 56848, 56853, 56858, 56867, 56872, 56874, 56890, 56892, 56898, 56910, 56930, 56950, 56963, 56965, 56982, 57056, 57063, 57074, 57081, 57083, 57097, 57103, 57109, 57110, 57126, 57139, 57171, 57183, 57187, 57202, 57231, 57251, 57260, 57264, 57275, 57276, 57283, 57299, 57306, 57310, 57313, 57342, 57369, 57425, 57437, 57443, 57460, 57465, 57470, 57476, 57485, 57511, 57535, 57554, 57562, 57573, 57575, 57585, 57588, 57592, 57593, 57604, 57613, 57615, 57617, 57619, 57663, 57669, 57708, 57711, 57713, 57731, 57735, 57737, 57740, 57745, 57753, 57760, 57765, 57770, 57773, 57774, 57775, 57786, 57788, 57795, 57810, 57832, 57836, 57858, 57872, 57878, 57882, 57886, 57887, 57889, 57899, 57907, 57920, 57953, 57964, 57972, 57981, 57982, 57985, 57989, 57990, 57997, 58004, 58005, 58027, 58049, 58051, 58054, 58071, 58075, 58100, 58105, 58108, 58113, 58128, 58135, 58149, 58153, 58156, 58169, 58184, 58187, 58193, 58203, 58205, 58217, 58218, 58229, 58238, 58249, 58257, 58261, 58264, 58265, 58271, 58278, 58286, 58287, 58296, 58308, 58325, 58351, 58371, 58389, 58416, 58431, 58454, 58456, 58472, 58475, 58483, 58508, 58514, 58518, 58519, 58522, 58524, 58528, 58530, 58540, 58570, 58574, 58576, 58586, 58590, 58593, 58609, 58617, 58618, 58627, 58631, 58633, 58640, 58660, 58672, 58674, 58720, 58723, 58730, 58736, 58752, 58757, 58758, 58771, 58782, 58793, 58796, 58799, 58809, 58811, 58817, 58821, 58825, 58839, 58848, 58851, 58852, 58854, 58855, 58857, 58878, 58889, 58891, 58903, 58930, 58938, 58940, 58947, 58965, 58991, 58993, 59019, 59036, 59069, 59072, 59087, 59090, 59101, 59113, 59120, 59127, 59133, 59136, 59141, 59152, 59156, 59161, 59166, 59169, 59177, 59200, 59210, 59220, 59222, 59226, 59250, 59252, 59255, 59266, 59267, 59278, 59299, 59301, 59317, 59343, 59344, 59347, 59352, 59364, 59367, 59375, 59382, 59387, 59396, 59402, 59411, 59415, 59417, 59420, 59442, 59448, 59458, 59464, 59469, 59472, 59485, 59489, 59496, 59507, 59516, 59537, 59546, 59547, 59563, 59615, 59619, 59628, 59630, 59637, 59657, 59664, 59675, 59677, 59706, 59707, 59712, 59716, 59757, 59761, 59777, 59785, 59788, 59797, 59816, 59819, 59822, 59829, 59839, 59844, 59845, 59870, 59881, 59883, 59889, 59890, 59907, 59911, 59917, 59924, 59941, 59943, 59946, 59948, 59950, 59976, 59979, 59980, 59986, 59988, 59999, 60020, 60046, 60057, 60096, 60104, 60106, 60130, 60139, 60142, 60146, 60169, 60174, 60179, 60180, 60181, 60194, 60204, 60207, 60235, 60325, 60362, 60383, 60423, 60441, 60455, 60463, 60505, 60508, 60531, 60545, 60570, 60572, 60579, 60582, 60597, 60600, 60614, 60624, 60626, 60649, 60653, 60667, 60672, 60679, 60736, 60737, 60739, 60747, 60753, 60763, 60767, 60775, 60782, 60792, 60809, 60851, 60864, 60878, 60881, 60887, 60890, 60893, 60907, 60925, 60949, 60966, 60968, 60969, 60975, 60985, 61003, 61012, 61024, 61034, 61054, 61056, 61076, 61081, 61085, 61106, 61111, 61113, 61118, 61119, 61144, 61181, 61182, 61187, 61188, 61195, 61196, 61198, 61223, 61235, 61236, 61259, 61276, 61282, 61296, 61298, 61316, 61355, 61377, 61393, 61413, 61415, 61417, 61421, 61434, 61441, 61452, 61458, 61462, 61472, 61473, 61474, 61475, 61502, 61517, 61519, 61521, 61539, 61546, 61562, 61569, 61574, 61581, 61585, 61587, 61597, 61620, 61622, 61630, 61638, 61663, 61673, 61676, 61688, 61711, 61733, 61735, 61740, 61749, 61764, 61771, 61783, 61802, 61871, 61886, 61904, 61932, 61948, 61959, 61991, 61998, 62004, 62007, 62028, 62042, 62087, 62157, 62169, 62181, 62190, 62210, 62212, 62227, 62242, 62261, 62271, 62296, 62308, 62316, 62343, 62345, 62346, 62383, 62389, 62393, 62402, 62412, 62415, 62421, 62456, 62466, 62476, 62479, 62480, 62488, 62497, 62498, 62505, 62506, 62518, 62522, 62539, 62543, 62547, 62559, 62561, 62576, 62586, 62603, 62606, 62639, 62647, 62650, 62654, 62669, 62684, 62698, 62734, 62735, 62761, 62764, 62795, 62813, 62837, 62841, 62854, 62858, 62884, 62909, 62914, 62927, 62930, 62933, 62937, 62946, 62950, 62958, 62974, 62980, 62983, 62995, 63000, 63007, 63009, 63058, 63059, 63063, 63074, 63088, 63098, 63109, 63139, 63163, 63173, 63185, 63200, 63201, 63218, 63220, 63234, 63238, 63249, 63253, 63280, 63290, 63298, 63320, 63327, 63329, 63360, 63390, 63402, 63405, 63409, 63415, 63430, 63456, 63459, 63461, 63481, 63489, 63492, 63494, 63496, 63509, 63559, 63560, 63563, 63567, 63579, 63581, 63582, 63616, 63628, 63646, 63652, 63676, 63692, 63693, 63695, 63697, 63701, 63710, 63740, 63745, 63746, 63761, 63771, 63778, 63785, 63833, 63845, 63852, 63866, 63875, 63898, 63922, 63943, 63958, 63961, 63992, 63993, 64004, 64008, 64014, 64021, 64026, 64027, 64037, 64041, 64093, 64099, 64100, 64104, 64123, 64145, 64154, 64168, 64182, 64188, 64191, 64195, 64197, 64227, 64232, 64248, 64260, 64262, 64270, 64282, 64292, 64295, 64298, 64311, 64357, 64370, 64432, 64465, 64472, 64485, 64495, 64514, 64519, 64550, 64558, 64570, 64579, 64581, 64597, 64601, 64619, 64652, 64655, 64671, 64689, 64694, 64701, 64714, 64719, 64721, 64740, 64752, 64753, 64766, 64780, 64785, 64796, 64825, 64839, 64846, 64874, 64881, 64891, 64893, 64904, 64906, 64921, 64925, 64927, 64944, 64952, 64969, 64970, 64980, 64983, 64994, 64997, 64999, 65003, 65007, 65025, 65049, 65052, 65056, 65082, 65090, 65113, 65124, 65130, 65147, 65161, 65176, 65219, 65226, 65247, 65268, 65310, 65341, 65342, 65359, 65360, 65370, 65374, 65375, 65393, 65421, 65426, 65427, 65431, 65435, 65442, 65463, 65465, 65476, 65490, 65502, 65516, 65523, 65527, 65535, 65538, 65541, 65544, 65547, 65556, 65559, 65565, 65611, 65620, 65623, 65630, 65636, 65646, 65678, 65703, 65706, 65723, 65733, 65741, 65760, 65779, 65793, 65810, 65815, 65824, 65827, 65833, 65835, 65855, 65887, 65893, 65894, 65895, 65946, 65950, 65959, 65962, 65970, 65975, 65988, 65994, 65999, 66015, 66037, 66054, 66061, 66069, 66084, 66093, 66094, 66130, 66133, 66144, 66152, 66157, 66172, 66190, 66193, 66217, 66218, 66221, 66228, 66246, 66256, 66264, 66283, 66288, 66296, 66322, 66373, 66383, 66395, 66405, 66411, 66437, 66463, 66496, 66505, 66507, 66532, 66555, 66560, 66572, 66574, 66580, 66595, 66596, 66629, 66641, 66657, 66683, 66690, 66696, 66730, 66736, 66745, 66753, 66761, 66768, 66772, 66807, 66825, 66827, 66831, 66850, 66851, 66878, 66880, 66892, 66900, 66903, 66919, 66936, 66953, 66955, 66959, 66974, 66988, 67017, 67041, 67042, 67067, 67070, 67072, 67085, 67089, 67134, 67139, 67140, 67144, 67158, 67175, 67188, 67213, 67227, 67239, 67241, 67281, 67301, 67321, 67343, 67365, 67390, 67408, 67409, 67421, 67423, 67461, 67477, 67480, 67481, 67505, 67512, 67520, 67527, 67533, 67548, 67559, 67568, 67576, 67603, 67617, 67619, 67620, 67623, 67629, 67635, 67636, 67639, 67652, 67653, 67716, 67722, 67739, 67740, 67744, 67771, 67772, 67774, 67788, 67792, 67800, 67814, 67831, 67835, 67843, 67853, 67887, 67902, 67920, 67922, 67926, 67937, 67976, 67985, 67987, 68006, 68007, 68024, 68028, 68075, 68082, 68089, 68094, 68095, 68109, 68116, 68126, 68128, 68132, 68145, 68162, 68173, 68176, 68186, 68192, 68201, 68208, 68209, 68218, 68221, 68223, 68231, 68234, 68251, 68272, 68300, 68319, 68350, 68355, 68356, 68381, 68391, 68392, 68396, 68416, 68420, 68437, 68457, 68460, 68464, 68491, 68505, 68513, 68530, 68543, 68546, 68579, 68591, 68623, 68624, 68635, 68645, 68673, 68687, 68702, 68738, 68759, 68764, 68765, 68780, 68784, 68791, 68797, 68827, 68832, 68849, 68850, 68857, 68860, 68863, 68865, 68870, 68878, 68884, 68887, 68900, 68902, 68907, 68909, 68915, 68926, 68939, 68941, 68956, 68989, 68991, 69000, 69006, 69015, 69022, 69047, 69079, 69110, 69131, 69158, 69162, 69167, 69168, 69182, 69194, 69248, 69265, 69272, 69274, 69280, 69282, 69289, 69295, 69296, 69299, 69322, 69341, 69342, 69344, 69359, 69370, 69388, 69398, 69399, 69402, 69406, 69411, 69416, 69438, 69480, 69483, 69487, 69507, 69518, 69528, 69546, 69548, 69567, 69574, 69578, 69585, 69596, 69604, 69620, 69626, 69639, 69650, 69672, 69678, 69679, 69682, 69684, 69690, 69698, 69711, 69713, 69719, 69739, 69763, 69778, 69781, 69797, 69799, 69805, 69811, 69822, 69825, 69890, 69896, 69898, 69904, 69917, 69934, 69976, 69981, 69992, 70003, 70023, 70026, 70080, 70083, 70099, 70123, 70146, 70156, 70164, 70169, 70182, 70184, 70206, 70217, 70221, 70222, 70226, 70234, 70239, 70252, 70259, 70267, 70278, 70323, 70329, 70337, 70368, 70373, 70380, 70389, 70396, 70420, 70441, 70458, 70463, 70469, 70473, 70478, 70485, 70491, 70492, 70511, 70517, 70520, 70543, 70550, 70561, 70571, 70572, 70576, 70592, 70609, 70612, 70623, 70628, 70645, 70651, 70655, 70675, 70682, 70698, 70699, 70709, 70714, 70717, 70722, 70723, 70728, 70736, 70740, 70772, 70784, 70807, 70822, 70832, 70851, 70854, 70864, 70874, 70890, 70899, 70911, 70937, 70954, 70957, 70971, 70974, 70989, 71005, 71006, 71070, 71072, 71089, 71091, 71093, 71103, 71119, 71127, 71129, 71143, 71175, 71184, 71192, 71194, 71200, 71233, 71243, 71262, 71280, 71296, 71309, 71327, 71328, 71336, 71339, 71341, 71353, 71364, 71365, 71377, 71382, 71386, 71397, 71412, 71413, 71414, 71433, 71445, 71477, 71479, 71507, 71513, 71525, 71532, 71536, 71564, 71571, 71576, 71577, 71580, 71596, 71617, 71645, 71652, 71660, 71665, 71689, 71704, 71722, 71728, 71739, 71743, 71760, 71788, 71797, 71800, 71812, 71821, 71837, 71857, 71862, 71869, 71871, 71878, 71889, 71921, 71923, 71927, 71929, 71945, 71961, 71998, 72031, 72036, 72043, 72048, 72054, 72073, 72079, 72091, 72102, 72107, 72116, 72119, 72120, 72126, 72143, 72146, 72164, 72173, 72197, 72202, 72207, 72225, 72229, 72231, 72233, 72236, 72279, 72287, 72290, 72300, 72309, 72317, 72324, 72329, 72353, 72355, 72358, 72378, 72387, 72408, 72412, 72417, 72421, 72447, 72454, 72461, 72511, 72520, 72536, 72540, 72554, 72557, 72562, 72582, 72608, 72633, 72634, 72647, 72667, 72671, 72673, 72690, 72725, 72727, 72763, 72775, 72778, 72783, 72790, 72827, 72844, 72849, 72851, 72860, 72885, 72897, 72907, 72930, 72932, 72942, 72943, 72969, 72978, 72988, 72999, 73015, 73020, 73038, 73055, 73061, 73063, 73074, 73077, 73090, 73110, 73113, 73126, 73129, 73155, 73175, 73186, 73198, 73223, 73231, 73234, 73243, 73265, 73280, 73289, 73313, 73316, 73363, 73375, 73376, 73384, 73401, 73429, 73433, 73440, 73444, 73460, 73463, 73465, 73473, 73477, 73498, 73499, 73509, 73512, 73521, 73526, 73530, 73537, 73540, 73557, 73559, 73565, 73575, 73578, 73583, 73612, 73639, 73648, 73661, 73682, 73693, 73755, 73756, 73765, 73776, 73790, 73808, 73811, 73819, 73822, 73832, 73834, 73859, 73863, 73868, 73886, 73887, 73888, 73896, 73953, 73961, 73970, 74002, 74010, 74022, 74034, 74038, 74039, 74046, 74081, 74092, 74116, 74195, 74201, 74211, 74215, 74217, 74229, 74230, 74232, 74237, 74269, 74282, 74291, 74332, 74346, 74348, 74353, 74369, 74373, 74376, 74388, 74394, 74404, 74406, 74409, 74444, 74456, 74486, 74493, 74502, 74503, 74505, 74546, 74548, 74556, 74562, 74575, 74585, 74608, 74610, 74632, 74639, 74640, 74673, 74674, 74679, 74680, 74702, 74716, 74725, 74737, 74755, 74763, 74764, 74771, 74786, 74794, 74798, 74805, 74823, 74830, 74851, 74857, 74860, 74880, 74888, 74889, 74891, 74899, 74907, 74926, 74932, 74945, 74950, 74975, 74976, 74993, 74999, 75001, 75026, 75029, 75034, 75054, 75061, 75100, 75101, 75107, 75114, 75117, 75155, 75159, 75170, 75181, 75230, 75240, 75249, 75281, 75305, 75314, 75320, 75335, 75347, 75353, 75360, 75395, 75396, 75401, 75424, 75430, 75431, 75444, 75446, 75454, 75493, 75509, 75510, 75514, 75517, 75526, 75536, 75607, 75640, 75650, 75658, 75688, 75696, 75715, 75722, 75737, 75759, 75772, 75775, 75800, 75819, 75824, 75828, 75829, 75843, 75856, 75865, 75867, 75882, 75889, 75890, 75891, 75898, 75899, 75941, 75950, 75960, 75990, 75994, 76005, 76010, 76030, 76034, 76074, 76078, 76097, 76115, 76118, 76126, 76143, 76180, 76186, 76253, 76261, 76262, 76265, 76275, 76300, 76318, 76319, 76328, 76329, 76333, 76392, 76410, 76414, 76418, 76437, 76439, 76457, 76476, 76479, 76480, 76508, 76529, 76541, 76544, 76558, 76562, 76571, 76575, 76597, 76600, 76602, 76612, 76633, 76637, 76641, 76658, 76659, 76665, 76667, 76670, 76674, 76707, 76716, 76717, 76741, 76761, 76812, 76826, 76843, 76873, 76880, 76886, 76890, 76912, 76920, 76921, 76923, 76930, 76945, 76952, 76955, 76974, 76987, 76988, 76994, 77010, 77013, 77014, 77026, 77028, 77034, 77037, 77041, 77045, 77053, 77078, 77081, 77083, 77084, 77115, 77135, 77149, 77163, 77189, 77192, 77206, 77218, 77223, 77241, 77265, 77301, 77303, 77331, 77341, 77349, 77416, 77443, 77450, 77471, 77472, 77503, 77511, 77520, 77537, 77552, 77575, 77576, 77579, 77586, 77599, 77600, 77606, 77609, 77618, 77630, 77665, 77689, 77690, 77691, 77696, 77697, 77707, 77731, 77732, 77746, 77764, 77781, 77787, 77801, 77804, 77826, 77828, 77829, 77850, 77875, 77882, 77900, 77901, 77912, 77913, 77922, 77947, 77948, 77949, 78000, 78009, 78023, 78086, 78097, 78100, 78108, 78117, 78149, 78152, 78155, 78158, 78171, 78204, 78215, 78219, 78222, 78226, 78230, 78231, 78234, 78248, 78288, 78298, 78303, 78318, 78319, 78322, 78325, 78336, 78337, 78354, 78365, 78373, 78380, 78388, 78410, 78447, 78454, 78474, 78494, 78495, 78505, 78506, 78532, 78536, 78551, 78560, 78568, 78570, 78597, 78600, 78615, 78647, 78658, 78666, 78678, 78685, 78693, 78701, 78708, 78716, 78722, 78729, 78739, 78745, 78756, 78797, 78804, 78814, 78828, 78840, 78859, 78864, 78867, 78873, 78879, 78891, 78895, 78898, 78902, 78904, 78911, 78914, 78946, 78959, 78983, 78996, 79051, 79053, 79060, 79089, 79103, 79126, 79132, 79154, 79166, 79168, 79177, 79188, 79224, 79229, 79252, 79283, 79288, 79294, 79308, 79337, 79348, 79349, 79355, 79404, 79418, 79422, 79426, 79427, 79441, 79466, 79501, 79511, 79532, 79538, 79548, 79565, 79578, 79584, 79585, 79589, 79596, 79602, 79603, 79655, 79667, 79673, 79677, 79690, 79737, 79751, 79754, 79804, 79813, 79826, 79831, 79836, 79862, 79875, 79877, 79909, 79919, 79921, 79923, 79945, 79962, 79987, 79991, 79998, 80018, 80020, 80030, 80042, 80053, 80081, 80106, 80108, 80110, 80111, 80117, 80132, 80136, 80158, 80160, 80190, 80204, 80220, 80228, 80237, 80239, 80245, 80254, 80260, 80262, 80267, 80274, 80299, 80305, 80308, 80317, 80319, 80320, 80326, 80335, 80344, 80345, 80347, 80360, 80375, 80400, 80410, 80423, 80429, 80436, 80442, 80447, 80470, 80472, 80475, 80490, 80494, 80504, 80511, 80519, 80521, 80523, 80534, 80539, 80547, 80559, 80580, 80586, 80587, 80606, 80625, 80629, 80632, 80649, 80653, 80656, 80678, 80696, 80729, 80742, 80745, 80751, 80757, 80767, 80790, 80791, 80810, 80826, 80844, 80849, 80856, 80861, 80862, 80880, 80885, 80891, 80914, 80935, 80942, 80956, 80985, 80987, 80989, 80993, 80999, 81004, 81006, 81007, 81012, 81025, 81032, 81037, 81045, 81049, 81053, 81057, 81069, 81087, 81099, 81107, 81195, 81209, 81212, 81223, 81235, 81236, 81237, 81242, 81247, 81267, 81274, 81280, 81285, 81295, 81335, 81342, 81349, 81359, 81363, 81376, 81385, 81416, 81425, 81427, 81431, 81438, 81441, 81449, 81464, 81478, 81480, 81491, 81534, 81548, 81556, 81561, 81583, 81601, 81614, 81633, 81651, 81675, 81679, 81686, 81691, 81692, 81715, 81721, 81728, 81755, 81783, 81797, 81807, 81810, 81818, 81827, 81844, 81849, 81850, 81864, 81865, 81876, 81881, 81891, 81904, 81918, 81926, 81927, 81958, 81978, 81983, 81992, 81997, 81998, 82003, 82021, 82030, 82036, 82042, 82055, 82057, 82072, 82075, 82091, 82100, 82111, 82122, 82130, 82132, 82138, 82148, 82150, 82157, 82177, 82178, 82195, 82217, 82235, 82238, 82245, 82258, 82279, 82290, 82293, 82309, 82326, 82328, 82360, 82362, 82363, 82393, 82433, 82459, 82466, 82482, 82490, 82505, 82514, 82518, 82563, 82565, 82594, 82602, 82605, 82619, 82627, 82637, 82641, 82642, 82646, 82655, 82713, 82736, 82746, 82769, 82799, 82802, 82826, 82828, 82836, 82843, 82845, 82847, 82864, 82881, 82915, 82929, 82939, 82950, 82970, 82973, 83010, 83013, 83065, 83079, 83085, 83091, 83099, 83122, 83128, 83143, 83169, 83180, 83202, 83203, 83210, 83212, 83228, 83235, 83248, 83258, 83263, 83278, 83292, 83317, 83335, 83342, 83345, 83361, 83372, 83375, 83382, 83389, 83422, 83427, 83435, 83464, 83466, 83469, 83496, 83500, 83509, 83514, 83524, 83532, 83545, 83561, 83564, 83592, 83594, 83596, 83607, 83617, 83629, 83662, 83668, 83681, 83690, 83692, 83697, 83699, 83700, 83702, 83709, 83723, 83751, 83766, 83768, 83773, 83776, 83792, 83802, 83818, 83826, 83865, 83869, 83879, 83892, 83899, 83901, 83908, 83914, 83915, 83922, 83924, 83932, 83947, 83957, 83960, 83975, 83976, 83981, 83988, 83991, 84001, 84020, 84021, 84033, 84036, 84039, 84061, 84081, 84095, 84105, 84117, 84120, 84128, 84129, 84142, 84172, 84198, 84208, 84223, 84247, 84266, 84286, 84302, 84303, 84306, 84318, 84319, 84332, 84346, 84347, 84359, 84362, 84382, 84388, 84392, 84394, 84402, 84420, 84427, 84450, 84454, 84457, 84458, 84461, 84463, 84473, 84476, 84478, 84479, 84488, 84489, 84495, 84516, 84519, 84523, 84534, 84598, 84615, 84624, 84632, 84638, 84640, 84647, 84669, 84681, 84694, 84711, 84737, 84742, 84745, 84758, 84764, 84766, 84798, 84802, 84822, 84838, 84875, 84881, 84886, 84891, 84896, 84904, 84909, 84934, 84938, 84952, 84958, 84964, 84979, 84983, 85017, 85019, 85023, 85033, 85039, 85042, 85056, 85062, 85076, 85079, 85085, 85089, 85095, 85134, 85138, 85143, 85147, 85152, 85163, 85171, 85205, 85212, 85257, 85258, 85264, 85272, 85361, 85370, 85375, 85417, 85441, 85455, 85459, 85490, 85492, 85493, 85501, 85508, 85516, 85533, 85535, 85552, 85560, 85562, 85566, 85575, 85583, 85607, 85608, 85638, 85645, 85647, 85665, 85669, 85685, 85689, 85698, 85700, 85720, 85723, 85730, 85747, 85748, 85757, 85769, 85775, 85780, 85781, 85784, 85828, 85842, 85851, 85853, 85860, 85866, 85870, 85885, 85889, 85899, 85907, 85932, 85938, 85962, 85974, 85983, 86007, 86024, 86026, 86036, 86041, 86042, 86078, 86086, 86087, 86123, 86129, 86137, 86144, 86176, 86193, 86199, 86220, 86233, 86238, 86239, 86251, 86270, 86295, 86300, 86306, 86323, 86325, 86326, 86355, 86356, 86368, 86379, 86382, 86387, 86411, 86413, 86429, 86450, 86485, 86502, 86505, 86516, 86539, 86546, 86550, 86561, 86570, 86577, 86580, 86583, 86585, 86586, 86587, 86590, 86603, 86611, 86616, 86618, 86621, 86678, 86692, 86702, 86712, 86717, 86745, 86749, 86752, 86754, 86762, 86766, 86773, 86807, 86810, 86821, 86824, 86832, 86841, 86846, 86848, 86880, 86883, 86888, 86894, 86897, 86907, 86915, 86918, 86919, 86921, 86976, 87007, 87029, 87035, 87056, 87082, 87085, 87108, 87118, 87135, 87157, 87160, 87185, 87187, 87188, 87197, 87206, 87209, 87216, 87232, 87236, 87245, 87252, 87253, 87254, 87259, 87263, 87275, 87279, 87283, 87284, 87291, 87320, 87325, 87336, 87344, 87347, 87361, 87375, 87376, 87400, 87421, 87424, 87450, 87461, 87474, 87484, 87497, 87506, 87532, 87533, 87565, 87572, 87585, 87595, 87605, 87608, 87628, 87631, 87642, 87646, 87651, 87659, 87660, 87704, 87706, 87708, 87711, 87721, 87733, 87734, 87754, 87758, 87769, 87783, 87818, 87846, 87858, 87880, 87887, 87926, 87932, 87936, 87948, 87949, 87957, 87963, 87969, 87986, 87992, 87994, 87999, 88000, 88006, 88008, 88009, 88012, 88021, 88040, 88079, 88089, 88112, 88154, 88163, 88165, 88175, 88180, 88206, 88211, 88214, 88220, 88224, 88243, 88254, 88258, 88269, 88291, 88296, 88303, 88312, 88315, 88316, 88345, 88350, 88360, 88407, 88409, 88422, 88429, 88432, 88433, 88434, 88438, 88449, 88471, 88476, 88486, 88514, 88529, 88531, 88539, 88560, 88565, 88571, 88584, 88594, 88602, 88624, 88626, 88632, 88635, 88649, 88663, 88665, 88695, 88696, 88734, 88738, 88747, 88764, 88773, 88778, 88790, 88795, 88824, 88844, 88851, 88882, 88883, 88888, 88912, 88921, 88951, 88961, 88991, 89021, 89050, 89104, 89119, 89124, 89126, 89132, 89157, 89176, 89193, 89205, 89252, 89265, 89268, 89285, 89290, 89292, 89294, 89324, 89334, 89336, 89355, 89363, 89368, 89369, 89379, 89398, 89400, 89416, 89419, 89446, 89448, 89451, 89481, 89485, 89493, 89502, 89520, 89536, 89544, 89563, 89579, 89584, 89597, 89600, 89607, 89616, 89617, 89634, 89643, 89654, 89672, 89686, 89719, 89735, 89736, 89742, 89755, 89760, 89797, 89798, 89817, 89849, 89895, 89901, 89906, 89914, 89917, 89927, 89929, 89934, 89950, 89952, 89956, 89957, 89984, 89996, 89997, 90008, 90021, 90028, 90036, 90051, 90057, 90070, 90078, 90106, 90120, 90150, 90151, 90158, 90165, 90176, 90195, 90208, 90233, 90238, 90259, 90261, 90270, 90279, 90302, 90310, 90327, 90369, 90389, 90392, 90418, 90427, 90441, 90452, 90455, 90478, 90488, 90508, 90521, 90533, 90538, 90541, 90544, 90549, 90559, 90560, 90609, 90639, 90657, 90663, 90672, 90686, 90699, 90704, 90708, 90720, 90722, 90727, 90739, 90755, 90764, 90768, 90776, 90780, 90789, 90795, 90798, 90802, 90805, 90814, 90830, 90834, 90846, 90863, 90874, 90884, 90887, 90889, 90890, 90898, 90901, 90903, 90905, 90910, 90917, 90937, 90954, 90990, 91000, 91001, 91008, 91046, 91051, 91053, 91074, 91076, 91086, 91098, 91101, 91150, 91167, 91197, 91200, 91219, 91223, 91245, 91249, 91285, 91296, 91333, 91342, 91350, 91351, 91419, 91428, 91437, 91458, 91460, 91461, 91463, 91469, 91472, 91493, 91519, 91525, 91531, 91534, 91548, 91557, 91568, 91572, 91583, 91586, 91588, 91601, 91603, 91614, 91635, 91682, 91705, 91712, 91733, 91744, 91763, 91782, 91783, 91788, 91790, 91798, 91802, 91814, 91832, 91842, 91853, 91855, 91883, 91913, 91915, 91925, 91926, 91934, 91948, 91954, 91965, 91989, 91992, 91996, 92025, 92036, 92041, 92063, 92066, 92094, 92105, 92129, 92136, 92170, 92201, 92212, 92235, 92239, 92253, 92278, 92287, 92288, 92291, 92314, 92323, 92331, 92346, 92347, 92353, 92382, 92410, 92438, 92452, 92454, 92458, 92464, 92487, 92516, 92554, 92580, 92585, 92606, 92613, 92615, 92636, 92668, 92676, 92684, 92686, 92695, 92704, 92718, 92725, 92767, 92775, 92777, 92787, 92793, 92796, 92799, 92800, 92861, 92865, 92919, 92933, 92934, 92941, 92954, 92955, 92969, 92974, 92999, 93011, 93033, 93056, 93062, 93084, 93088, 93104, 93113, 93117, 93119, 93122, 93139, 93142, 93155, 93159, 93191, 93194, 93207, 93209, 93219, 93233, 93280, 93284, 93299, 93301, 93315, 93320, 93321, 93325, 93357, 93365, 93378, 93379, 93388, 93390, 93392, 93394, 93398, 93401, 93411, 93422, 93462, 93472, 93479, 93494, 93496, 93507, 93541, 93556, 93560, 93566, 93567, 93588, 93593, 93601, 93602, 93648, 93659, 93662, 93701, 93710, 93721, 93739, 93784, 93788, 93807, 93814, 93829, 93833, 93834, 93838, 93840, 93847, 93887, 93894, 93905, 93920, 93923, 93924, 93932, 93945, 93950, 93952, 93956, 93966, 93970, 93974, 93983, 93987, 94005, 94007, 94008, 94016, 94025, 94029, 94030, 94032, 94072, 94081, 94084, 94085, 94104, 94105, 94113, 94120, 94126, 94152, 94162, 94166, 94167, 94184, 94192, 94195, 94196, 94197, 94213, 94232, 94240, 94260, 94264, 94297, 94319, 94335, 94351, 94404, 94422, 94437, 94447, 94484, 94538, 94539, 94546, 94550, 94560, 94564, 94581, 94592, 94597, 94609, 94611, 94642, 94661, 94663, 94665, 94669, 94685, 94689, 94696, 94733, 94734, 94753, 94763, 94765, 94767, 94768, 94777, 94786, 94799, 94805, 94817, 94821, 94828, 94837, 94863, 94864, 94872, 94906, 94912, 94933, 94944, 94954, 94956, 94959, 94962, 94966, 94979, 94982, 94987, 94997, 95000, 95004, 95011, 95030, 95039, 95045, 95052, 95057, 95058, 95076, 95077, 95084, 95090, 95105, 95111, 95115, 95126, 95143, 95155, 95157, 95170, 95177, 95227, 95241, 95251, 95271, 95316, 95324, 95335, 95340, 95351, 95372, 95380, 95408, 95410, 95424, 95427, 95435, 95440, 95465, 95469, 95508, 95512, 95517, 95540, 95555, 95561, 95574, 95609, 95611, 95621, 95628, 95631, 95632, 95637, 95641, 95659, 95672, 95674, 95707, 95721, 95725, 95736, 95760, 95763, 95765, 95782, 95787, 95798, 95803, 95810, 95818, 95821, 95830, 95834, 95841, 95863, 95864, 95868, 95878, 95887, 95889, 95892, 95893, 95895, 95907, 95909, 95951, 95968, 95977, 96004, 96017, 96023, 96025, 96040, 96046, 96071, 96098, 96100, 96104, 96115, 96120, 96125, 96126, 96137, 96140, 96171, 96176, 96187, 96218, 96234, 96247, 96260, 96261, 96284, 96323, 96324, 96333, 96336, 96346, 96347, 96350, 96365, 96370, 96377, 96378, 96389, 96391, 96402, 96404, 96410, 96430, 96435, 96442, 96445, 96446, 96456, 96458, 96468, 96477, 96479, 96482, 96519, 96540, 96553, 96574, 96575, 96579, 96581, 96582, 96588, 96610, 96621, 96624, 96637, 96645, 96659, 96666, 96670, 96677, 96683, 96695, 96699, 96709, 96719, 96731, 96737, 96759, 96761, 96772, 96785, 96808, 96813, 96815, 96825, 96843, 96848, 96864, 96869, 96872, 96877, 96888, 96920, 96922, 96923, 96930, 96947, 96960, 96962, 96974, 96984, 96997, 97007, 97008, 97022, 97023, 97028, 97037, 97048, 97061, 97069, 97079, 97102, 97121, 97132, 97143, 97151, 97156, 97172, 97178, 97206, 97229, 97232, 97263, 97314, 97333, 97339, 97351, 97395, 97399, 97429, 97437, 97441, 97443, 97449, 97460, 97484, 97497, 97524, 97529, 97531, 97532, 97537, 97539, 97560, 97564, 97566, 97589, 97593, 97598, 97603, 97625, 97637, 97646, 97679, 97680, 97681, 97683, 97706, 97714, 97716, 97735, 97762, 97768, 97771, 97781, 97783, 97795, 97799, 97833, 97834, 97841, 97848, 97864, 97876, 97898, 97916, 97933, 97937, 97942, 97947, 97971, 97974, 97976, 97978, 97981, 97984, 98001, 98040, 98046, 98048, 98050, 98057, 98130, 98162, 98176, 98182, 98185, 98188, 98195, 98196, 98198, 98204, 98220, 98228, 98229, 98235, 98256, 98258, 98268, 98279, 98283, 98284, 98303, 98309, 98310, 98318, 98335, 98342, 98347, 98359, 98383, 98390, 98400, 98402, 98413, 98417, 98420, 98448, 98484, 98511, 98517, 98525, 98530, 98554, 98558, 98564, 98584, 98586, 98588, 98589, 98601, 98622, 98642, 98647, 98649, 98664, 98665, 98669, 98674, 98685, 98698, 98710, 98734, 98751, 98759, 98794, 98805, 98813, 98814, 98831, 98833, 98834, 98879, 98883, 98899, 98905, 98919, 98931, 98939, 98969, 98973, 98991, 98999, 99023, 99078, 99085, 99098, 99111, 99135, 99141, 99166, 99178, 99205, 99208, 99213, 99218, 99231, 99247, 99255, 99256, 99258, 99282, 99283, 99286, 99293, 99312, 99319, 99322, 99333, 99339, 99344, 99356, 99411, 99412, 99430, 99439, 99440, 99458, 99464, 99467, 99469, 99474, 99485, 99492, 99495, 99509, 99528, 99538, 99539, 99545, 99556, 99560, 99561, 99562, 99572, 99598, 99599, 99611, 99629, 99637, 99645, 99647, 99650, 99666, 99674, 99691, 99693, 99721, 99726, 99740, 99759, 99762, 99768, 99781, 99791, 99797, 99806, 99810, 99817, 99819, 99823, 99836, 99837, 99847, 99864, 99868, 99873, 99880, 99881, 99883, 99901, 99913, 99934, 99936, 99939, 99944, 99955, 99957, 99965, 99991, 99992, 99995]\n","new_dev_ids =  [11, 32, 33, 43, 98, 150, 151, 161, 172, 188, 243, 266, 276, 317, 325, 328, 359, 368, 372, 376, 388, 426, 456, 493, 495, 501, 509, 527, 548, 551, 585, 588, 589, 591, 592, 595, 606, 622, 625, 627, 629, 636, 670, 709, 734, 746, 751, 798, 803, 810, 827, 852, 856, 905, 915, 932, 936, 947, 956, 1029, 1034, 1069, 1103, 1104, 1147, 1184, 1191, 1206, 1213, 1290, 1323, 1369, 1371, 1398, 1467, 1494, 1539, 1590, 1593, 1597, 1641, 1693, 1696, 1708, 1725, 1753, 1758, 1792, 1853, 1862, 1928, 1964, 1974, 1986, 1997, 2036, 2067, 2071, 2080, 2102, 2105, 2156, 2223, 2245, 2257, 2266, 2270, 2314, 2317, 2334, 2379, 2385, 2436, 2460, 2466, 2467, 2480, 2502, 2506, 2537, 2550, 2569, 2636, 2689, 2711, 2736, 2760, 2783, 2813, 2831, 2849, 2862, 2881, 2961, 2962, 2967, 3023, 3039, 3080, 3134, 3142, 3145, 3183, 3231, 3241, 3242, 3297, 3313, 3324, 3325, 3342, 3351, 3354, 3404, 3432, 3439, 3465, 3470, 3565, 3595, 3597, 3598, 3628, 3631, 3646, 3663, 3675, 3688, 3767, 3778, 3787, 3862, 3892, 3899, 3903, 3947, 3979, 3983, 3985, 4007, 4059, 4074, 4084, 4092, 4100, 4130, 4169, 4202, 4209, 4224, 4225, 4243, 4276, 4326, 4338, 4371, 4414, 4431, 4433, 4462, 4485, 4527, 4562, 4565, 4576, 4626, 4649, 4673, 4674, 4708, 4722, 4732, 4743, 4777, 4813, 4826, 4838, 4839, 4860, 4863, 4868, 4871, 4877, 4903, 4926, 4939, 4969, 4974, 4988, 5003, 5009, 5027, 5057, 5060, 5099, 5199, 5270, 5272, 5304, 5312, 5331, 5349, 5355, 5362, 5381, 5415, 5453, 5455, 5506, 5521, 5536, 5551, 5555, 5557, 5573, 5579, 5585, 5586, 5622, 5626, 5650, 5683, 5686, 5715, 5731, 5762, 5817, 5830, 5833, 5834, 5864, 5905, 5913, 5987, 6000, 6010, 6022, 6077, 6078, 6083, 6085, 6118, 6126, 6127, 6134, 6137, 6202, 6213, 6229, 6234, 6246, 6322, 6353, 6391, 6478, 6495, 6536, 6540, 6545, 6555, 6600, 6621, 6623, 6632, 6643, 6647, 6669, 6694, 6707, 6723, 6736, 6821, 6838, 6865, 6878, 6957, 7009, 7052, 7073, 7163, 7180, 7204, 7230, 7242, 7263, 7315, 7320, 7321, 7328, 7385, 7392, 7410, 7461, 7496, 7505, 7512, 7538, 7569, 7597, 7604, 7648, 7651, 7663, 7715, 7757, 7761, 7766, 7778, 7792, 7797, 7821, 7822, 7861, 7866, 7881, 7886, 7946, 7955, 7969, 7971, 7999, 8043, 8073, 8117, 8151, 8224, 8225, 8264, 8281, 8389, 8408, 8464, 8471, 8493, 8535, 8537, 8546, 8559, 8618, 8635, 8691, 8710, 8741, 8764, 8770, 8803, 8816, 8845, 8852, 8875, 8904, 8927, 8936, 8956, 9101, 9124, 9137, 9138, 9155, 9186, 9191, 9192, 9283, 9284, 9336, 9348, 9408, 9416, 9427, 9443, 9501, 9514, 9545, 9646, 9663, 9678, 9693, 9749, 9767, 9776, 9838, 9841, 9866, 9897, 9949, 9977, 9992, 9998, 10003, 10059, 10119, 10145, 10147, 10193, 10199, 10238, 10315, 10316, 10317, 10331, 10363, 10401, 10402, 10412, 10439, 10445, 10461, 10490, 10497, 10515, 10523, 10538, 10585, 10589, 10608, 10707, 10712, 10725, 10729, 10744, 10746, 10760, 10763, 10768, 10769, 10792, 10805, 10820, 10831, 10850, 10873, 10908, 10945, 10958, 11008, 11058, 11098, 11184, 11191, 11193, 11200, 11217, 11228, 11229, 11238, 11244, 11256, 11307, 11321, 11325, 11359, 11378, 11383, 11386, 11388, 11395, 11416, 11421, 11453, 11471, 11492, 11549, 11564, 11578, 11595, 11605, 11671, 11673, 11694, 11702, 11773, 11806, 11807, 11866, 11922, 11928, 11934, 11943, 11956, 12009, 12018, 12055, 12097, 12098, 12099, 12109, 12114, 12117, 12171, 12198, 12238, 12270, 12326, 12360, 12375, 12399, 12406, 12487, 12510, 12528, 12542, 12550, 12560, 12571, 12579, 12688, 12693, 12694, 12702, 12733, 12783, 12811, 12832, 12860, 12978, 13010, 13013, 13026, 13032, 13039, 13188, 13191, 13193, 13201, 13210, 13227, 13253, 13263, 13264, 13271, 13282, 13293, 13318, 13366, 13374, 13375, 13414, 13416, 13458, 13519, 13538, 13546, 13555, 13561, 13596, 13602, 13614, 13646, 13649, 13660, 13672, 13680, 13702, 13705, 13783, 13790, 13791, 13810, 13818, 13843, 13870, 13890, 13909, 13929, 13967, 13989, 14030, 14036, 14072, 14114, 14117, 14122, 14138, 14144, 14146, 14167, 14169, 14244, 14264, 14272, 14291, 14297, 14312, 14328, 14373, 14390, 14394, 14398, 14408, 14439, 14453, 14474, 14489, 14493, 14513, 14551, 14553, 14566, 14603, 14616, 14645, 14685, 14696, 14761, 14798, 14802, 14822, 14845, 14857, 14950, 14962, 15009, 15013, 15017, 15025, 15061, 15079, 15117, 15122, 15125, 15128, 15159, 15178, 15203, 15209, 15254, 15295, 15305, 15330, 15351, 15416, 15470, 15477, 15496, 15551, 15595, 15626, 15627, 15654, 15691, 15713, 15725, 15773, 15883, 15898, 15907, 15992, 16000, 16003, 16011, 16037, 16040, 16081, 16177, 16179, 16205, 16284, 16295, 16334, 16344, 16458, 16459, 16469, 16478, 16483, 16565, 16607, 16637, 16714, 16721, 16728, 16750, 16793, 16818, 16840, 16853, 16867, 16881, 16884, 16902, 16921, 16960, 16968, 16973, 16997, 17026, 17031, 17043, 17061, 17127, 17239, 17268, 17274, 17308, 17339, 17411, 17413, 17442, 17451, 17468, 17505, 17513, 17521, 17533, 17547, 17586, 17589, 17609, 17614, 17623, 17686, 17693, 17698, 17709, 17745, 17759, 17800, 17813, 17841, 17902, 17929, 17957, 17998, 18013, 18023, 18060, 18061, 18082, 18089, 18111, 18122, 18153, 18180, 18216, 18257, 18299, 18331, 18371, 18408, 18426, 18457, 18473, 18483, 18489, 18509, 18513, 18525, 18543, 18572, 18589, 18590, 18597, 18600, 18630, 18655, 18678, 18679, 18690, 18719, 18745, 18766, 18781, 18857, 18860, 18900, 18908, 18934, 18962, 18975, 19013, 19044, 19057, 19070, 19076, 19081, 19105, 19125, 19147, 19172, 19205, 19230, 19267, 19305, 19306, 19338, 19367, 19392, 19421, 19487, 19504, 19520, 19553, 19613, 19614, 19618, 19673, 19675, 19723, 19728, 19746, 19792, 19793, 19828, 19845, 19892, 19899, 19927, 19941, 19981, 20001, 20064, 20130, 20210, 20236, 20251, 20255, 20259, 20262, 20284, 20286, 20315, 20316, 20368, 20383, 20400, 20414, 20428, 20459, 20471, 20511, 20513, 20540, 20543, 20545, 20561, 20564, 20578, 20579, 20616, 20633, 20664, 20718, 20760, 20763, 20796, 20801, 20805, 20877, 20920, 20929, 20957, 20981, 20984, 21004, 21017, 21021, 21052, 21074, 21084, 21091, 21095, 21173, 21181, 21206, 21242, 21263, 21298, 21334, 21349, 21357, 21372, 21380, 21382, 21428, 21434, 21453, 21462, 21479, 21501, 21507, 21548, 21601, 21618, 21658, 21687, 21750, 21765, 21781, 21789, 21799, 21820, 21827, 21853, 21864, 21867, 21888, 21905, 21920, 21946, 22025, 22047, 22051, 22085, 22143, 22191, 22200, 22226, 22235, 22339, 22414, 22462, 22469, 22528, 22552, 22631, 22715, 22757, 22771, 22890, 22896, 22979, 23022, 23023, 23055, 23063, 23088, 23105, 23115, 23120, 23152, 23159, 23171, 23211, 23229, 23250, 23303, 23352, 23359, 23370, 23401, 23478, 23497, 23524, 23592, 23600, 23604, 23607, 23659, 23674, 23703, 23719, 23741, 23755, 23789, 23795, 23803, 23804, 23827, 23861, 23905, 23940, 23948, 23977, 23997, 24052, 24066, 24071, 24119, 24134, 24135, 24160, 24212, 24263, 24268, 24283, 24307, 24314, 24317, 24322, 24324, 24356, 24388, 24466, 24493, 24546, 24561, 24600, 24650, 24663, 24675, 24711, 24763, 24777, 24800, 24823, 24840, 24856, 24858, 24881, 24892, 24986, 24991, 24992, 25008, 25027, 25034, 25043, 25084, 25118, 25171, 25185, 25201, 25206, 25224, 25228, 25245, 25251, 25306, 25325, 25350, 25378, 25411, 25413, 25482, 25549, 25550, 25556, 25639, 25644, 25678, 25731, 25741, 25770, 25776, 25869, 25969, 25993, 26006, 26013, 26018, 26054, 26063, 26072, 26125, 26169, 26176, 26184, 26192, 26214, 26220, 26241, 26244, 26255, 26339, 26374, 26419, 26458, 26475, 26479, 26483, 26502, 26516, 26549, 26560, 26569, 26595, 26610, 26673, 26695, 26696, 26732, 26776, 26787, 26796, 26800, 26820, 26833, 26841, 26851, 26911, 26914, 26968, 27000, 27046, 27049, 27083, 27084, 27092, 27096, 27108, 27125, 27138, 27146, 27163, 27171, 27243, 27263, 27272, 27275, 27279, 27300, 27301, 27345, 27359, 27370, 27387, 27389, 27403, 27440, 27473, 27493, 27511, 27523, 27566, 27584, 27596, 27600, 27605, 27634, 27689, 27692, 27710, 27754, 27767, 27795, 27800, 27826, 27836, 27926, 27953, 27955, 27969, 27978, 27993, 28028, 28032, 28035, 28043, 28091, 28098, 28103, 28137, 28195, 28213, 28224, 28241, 28258, 28268, 28283, 28360, 28377, 28385, 28409, 28423, 28517, 28531, 28540, 28550, 28617, 28633, 28641, 28655, 28669, 28670, 28677, 28686, 28726, 28729, 28750, 28799, 28828, 28856, 28883, 28911, 28936, 28985, 29028, 29061, 29069, 29075, 29089, 29127, 29139, 29170, 29220, 29221, 29224, 29251, 29254, 29261, 29333, 29344, 29418, 29429, 29447, 29473, 29477, 29490, 29495, 29527, 29536, 29565, 29571, 29592, 29617, 29618, 29620, 29651, 29664, 29676, 29685, 29702, 29711, 29752, 29774, 29825, 29866, 29894, 29941, 29959, 29969, 29986, 29995, 30015, 30038, 30040, 30105, 30184, 30241, 30393, 30409, 30437, 30467, 30475, 30481, 30493, 30497, 30528, 30538, 30539, 30642, 30662, 30674, 30677, 30692, 30740, 30800, 30833, 30846, 30924, 30940, 30954, 30971, 30975, 30979, 30984, 30998, 31036, 31140, 31161, 31187, 31191, 31216, 31221, 31238, 31293, 31305, 31310, 31312, 31359, 31367, 31376, 31377, 31445, 31452, 31507, 31510, 31513, 31519, 31541, 31570, 31589, 31664, 31671, 31716, 31717, 31727, 31756, 31777, 31817, 31872, 31877, 31916, 31930, 31937, 31942, 31950, 31956, 31977, 32001, 32015, 32024, 32025, 32036, 32059, 32060, 32065, 32070, 32073, 32078, 32088, 32092, 32093, 32115, 32126, 32128, 32153, 32157, 32166, 32183, 32195, 32231, 32267, 32368, 32372, 32396, 32403, 32422, 32439, 32444, 32450, 32460, 32480, 32525, 32526, 32531, 32532, 32542, 32550, 32584, 32605, 32608, 32629, 32639, 32645, 32664, 32695, 32696, 32701, 32718, 32779, 32800, 40008, 40138, 40236, 40251, 40269, 40299, 40385, 40406, 40433, 40707, 40775, 40797, 40826, 40851, 40995, 41031, 41098, 41132, 41266, 41286, 41385, 41398, 41430, 41439, 41457, 41468, 41565, 41603, 41634, 41661, 41693, 41841, 42071, 42073, 42268, 42307, 42336, 42411, 42591, 42606, 42696, 42728, 42748, 42763, 42800, 43099, 43149, 43305, 43452, 43461, 43584, 43637, 43729, 43741, 43969, 44002, 44023, 44190, 44318, 44342, 44494, 44630, 44672, 44787, 44910, 44963, 44969, 44971, 45141, 45184, 45281, 45391, 45395, 45458, 45598, 45646, 45736, 45812, 45893, 45996, 46058, 46060, 46088, 46243, 46246, 46257, 46278, 46303, 46338, 46429, 46441, 46449, 46474, 46504, 46528, 46676, 46911, 47013, 47022, 47071, 47091, 47216, 47263, 47312, 47324, 47342, 47429, 47492, 47543, 47582, 47787, 47788, 47862, 47919, 47956, 47963, 48044, 48100, 48145, 48164, 48215, 48267, 48372, 48425, 48536, 48580, 48606, 48636, 48666, 48677, 48734, 48743, 48771, 48780, 48806, 48863, 48944, 49031, 49036, 49079, 49093, 49225, 49367, 49369, 49520, 49527, 49881, 49995, 50178, 50189, 50190, 50415, 50432, 50514, 50575, 50751, 50767, 50822, 50832, 50857, 50906, 50991, 51017, 51020, 51038, 51082, 51094, 51174, 51275, 51307, 51482, 51507, 51833, 51909, 51914, 51971, 52011, 52026, 52261, 52343, 52482, 52515, 52553, 52568, 52602, 52743, 52846, 52910, 53093, 53156, 53194, 53249, 53280, 53377, 53456, 53556, 53707, 53738, 53806, 53897, 53979, 54047, 54083, 54084, 54348, 54438, 54527, 54638, 54641, 54655, 54811, 55001, 55260, 55369, 55423, 55514, 55629, 55703, 55793, 55817, 55873, 56025, 56089, 56134, 56195, 56229, 56340, 56446, 56470, 56545, 56565, 56772, 56790, 56796, 56850, 57023, 57138, 57449, 57468, 57645, 57767, 58008, 58016, 58031, 58155, 58270, 58312, 58319, 58356, 58357, 58422, 58433, 58588, 58668, 58679, 58706, 58794, 58888, 58899, 58948, 58984, 59208, 59260, 59276, 59281, 59403, 59490, 59585, 59701, 59710, 59731, 59903, 60063, 60234, 60341, 60355, 60399, 60408, 60534, 60548, 60735, 60783, 60825, 60826, 61030, 61157, 61339, 61465, 61500, 61586, 61590, 61856, 61939, 62034, 62150, 62239, 62380, 62395, 62430, 62450, 62458, 62534, 62600, 62613, 62681, 62707, 62736, 62776, 62798, 62954, 63024, 63028, 63043, 63177, 63206, 63467, 63515, 63621, 63687, 63878, 64025, 64055, 64150, 64160, 64208, 64280, 64354, 64516, 64687, 64808, 64819, 64911, 64974, 65091, 65175, 65190, 65237, 65240, 65255, 65368, 65382, 65394, 65395, 65399, 65504, 65558, 65589, 65610, 65627, 65753, 65786, 65871, 65951, 65973, 66001, 66068, 66143, 66307, 66362, 66483, 66557, 66694, 66724, 66859, 66891, 67061, 67101, 67150, 67375, 67404, 67543, 67621, 67651, 67673, 67684, 67939, 68065, 68068, 68102, 68175, 68299, 68307, 68308, 68368, 68425, 68611, 68621, 68674, 68807, 68816, 68964, 69027, 69118, 69176, 69186, 69207, 69208, 69243, 69251, 69395, 69630, 69764, 69791, 69941, 70084, 70359, 70462, 70518, 70620, 70842, 70926, 71146, 71215, 71286, 71330, 71555, 71647, 71676, 71698, 71735, 71872, 71880, 71962, 72033, 72154, 72470, 72488, 72555, 72656, 72658, 72739, 72807, 72891, 72909, 72924, 72956, 73058, 73059, 73089, 73270, 73389, 73582, 73833, 73867, 73946, 73955, 74016, 74147, 74174, 74252, 74289, 74354, 74433, 74448, 74468, 74515, 74525, 74642, 74686, 74954, 75023, 75031, 75200, 75251, 75414, 75576, 75626, 75632, 75663, 75782, 75844, 75883, 75938, 75970, 76009, 76021, 76071, 76134, 76219, 76231, 76546, 76644, 76709, 76726, 76780, 76801, 76803, 76975, 76990, 77094, 77101, 77261, 77347, 77360, 77436, 77500, 77578, 77625, 77676, 77807, 78168, 78214, 78237, 78366, 78375, 78557, 78759, 78892, 79049, 79222, 79240, 79275, 79280, 79285, 79407, 79450, 79477, 79553, 79557, 79564, 79648, 79697, 79808, 79838, 79878, 80161, 80411, 80555, 80602, 80616, 80628, 80789, 80895, 80920, 81083, 81130, 81558, 81567, 81579, 81660, 81701, 81729, 81745, 81883, 81888, 81902, 81980, 82000, 82004, 82006, 82051, 82068, 82147, 82154, 82231, 82291, 82299, 82381, 82520, 82541, 82559, 82649, 82669, 82820, 82904, 82910, 82919, 82921, 82960, 83016, 83038, 83093, 83171, 83224, 83381, 83419, 83421, 83468, 83547, 83550, 83609, 83641, 83653, 83714, 83808, 83815, 83817, 83823, 83845, 83979, 84042, 84257, 84329, 84629, 84722, 84816, 84826, 85002, 85025, 85083, 85144, 85235, 85489, 85531, 85536, 85594, 85637, 85711, 85862, 85929, 85958, 86191, 86291, 86622, 86780, 86939, 86993, 87144, 87172, 87184, 87225, 87514, 87630, 87779, 87782, 87794, 87868, 87897, 87905, 88003, 88146, 88202, 88242, 88484, 88499, 88521, 88573, 88612, 88655, 88674, 88726, 88733, 88816, 88971, 88977, 89171, 89180, 89309, 89332, 89405, 89483, 89565, 89717, 89766, 89894, 89900, 90044, 90061, 90115, 90121, 90143, 90237, 90325, 90330, 90333, 90363, 90388, 90393, 90398, 90514, 90540, 90605, 90746, 90800, 90926, 91044, 91067, 91313, 91520, 91536, 91613, 91631, 91841, 91907, 92002, 92007, 92244, 92247, 92272, 92283, 92393, 92551, 92590, 92625, 92645, 92685, 92700, 92721, 92779, 92903, 93052, 93208, 93211, 93235, 93245, 93290, 93366, 93391, 93505, 93517, 93550, 93799, 94075, 94293, 94387, 94503, 94737, 94974, 94977, 95136, 95147, 95323, 95377, 95413, 95503, 95515, 95806, 95925, 95948, 96142, 96151, 96310, 96356, 96388, 96427, 96643, 96669, 96701, 96791, 96821, 96903, 96932, 96951, 97004, 97418, 97663, 97666, 97769, 97852, 97907, 98006, 98038, 98103, 98118, 98174, 98266, 98434, 98488, 98552, 98636, 98720, 98724, 98952, 99184, 99230, 99354, 99384, 99389, 99441, 99444, 99894, 99973]\n","new_test_ids =  [4, 6, 26, 42, 44, 53, 59, 73, 79, 117, 142, 145, 157, 162, 179, 199, 236, 240, 241, 294, 322, 333, 342, 384, 386, 408, 414, 425, 433, 443, 454, 461, 463, 480, 494, 511, 521, 546, 570, 571, 584, 587, 596, 618, 619, 628, 631, 651, 668, 682, 697, 702, 735, 738, 754, 768, 780, 782, 785, 792, 824, 863, 887, 890, 908, 919, 927, 929, 937, 940, 952, 954, 981, 985, 1005, 1008, 1018, 1020, 1044, 1045, 1047, 1056, 1085, 1094, 1098, 1105, 1107, 1112, 1114, 1115, 1118, 1123, 1133, 1139, 1152, 1169, 1171, 1174, 1194, 1233, 1234, 1236, 1260, 1269, 1271, 1279, 1281, 1284, 1287, 1316, 1322, 1327, 1363, 1368, 1380, 1386, 1390, 1409, 1422, 1428, 1440, 1441, 1453, 1458, 1477, 1487, 1488, 1526, 1551, 1552, 1556, 1561, 1570, 1578, 1579, 1580, 1598, 1620, 1621, 1631, 1636, 1637, 1647, 1675, 1684, 1721, 1732, 1742, 1744, 1757, 1767, 1783, 1787, 1790, 1793, 1811, 1818, 1854, 1881, 1893, 1912, 1923, 1924, 1956, 1967, 1987, 1992, 2005, 2014, 2021, 2042, 2052, 2064, 2066, 2167, 2184, 2188, 2201, 2211, 2216, 2219, 2230, 2231, 2243, 2246, 2251, 2258, 2272, 2279, 2297, 2332, 2336, 2361, 2363, 2381, 2386, 2388, 2414, 2429, 2433, 2444, 2473, 2487, 2508, 2515, 2539, 2547, 2570, 2574, 2589, 2602, 2630, 2647, 2664, 2667, 2668, 2701, 2732, 2739, 2747, 2755, 2762, 2765, 2766, 2771, 2777, 2788, 2794, 2799, 2814, 2830, 2834, 2835, 2836, 2856, 2876, 2885, 2892, 2899, 2900, 2907, 2927, 2949, 2964, 2966, 2981, 2984, 2997, 3008, 3011, 3016, 3019, 3022, 3046, 3056, 3057, 3105, 3115, 3120, 3122, 3125, 3127, 3140, 3169, 3175, 3179, 3184, 3204, 3210, 3222, 3240, 3258, 3261, 3267, 3290, 3295, 3300, 3310, 3319, 3334, 3340, 3365, 3384, 3389, 3394, 3396, 3397, 3421, 3424, 3425, 3431, 3433, 3441, 3442, 3462, 3481, 3488, 3512, 3516, 3550, 3554, 3561, 3563, 3567, 3594, 3616, 3617, 3639, 3647, 3685, 3703, 3708, 3710, 3725, 3742, 3748, 3775, 3788, 3807, 3808, 3813, 3823, 3834, 3863, 3880, 3906, 3911, 3914, 3920, 3921, 3937, 3956, 3970, 4012, 4015, 4020, 4034, 4044, 4045, 4076, 4078, 4085, 4090, 4103, 4105, 4123, 4126, 4161, 4168, 4180, 4216, 4217, 4260, 4267, 4269, 4270, 4300, 4303, 4308, 4334, 4377, 4378, 4390, 4393, 4410, 4413, 4419, 4454, 4482, 4494, 4507, 4512, 4516, 4533, 4534, 4539, 4543, 4547, 4556, 4564, 4583, 4603, 4625, 4628, 4641, 4655, 4665, 4676, 4678, 4696, 4718, 4727, 4749, 4758, 4770, 4778, 4781, 4787, 4800, 4816, 4817, 4837, 4841, 4844, 4846, 4855, 4859, 4864, 4870, 4884, 4885, 4895, 4905, 4911, 4917, 4931, 4964, 4970, 4972, 4978, 4992, 4995, 5002, 5014, 5047, 5049, 5069, 5076, 5078, 5095, 5104, 5113, 5118, 5134, 5142, 5158, 5166, 5180, 5194, 5197, 5198, 5213, 5224, 5226, 5231, 5262, 5271, 5278, 5281, 5330, 5342, 5354, 5365, 5370, 5384, 5392, 5394, 5398, 5419, 5458, 5496, 5498, 5508, 5513, 5525, 5535, 5543, 5545, 5549, 5556, 5558, 5559, 5569, 5589, 5593, 5596, 5619, 5628, 5638, 5642, 5646, 5647, 5662, 5673, 5680, 5693, 5695, 5709, 5725, 5734, 5749, 5752, 5764, 5766, 5768, 5771, 5786, 5790, 5797, 5815, 5829, 5838, 5852, 5860, 5882, 5942, 5948, 5954, 5957, 5989, 5991, 6003, 6006, 6016, 6017, 6026, 6028, 6029, 6055, 6060, 6094, 6105, 6110, 6112, 6116, 6128, 6143, 6147, 6153, 6156, 6173, 6174, 6186, 6187, 6194, 6203, 6206, 6222, 6244, 6253, 6262, 6280, 6285, 6288, 6303, 6310, 6327, 6335, 6344, 6359, 6374, 6378, 6400, 6423, 6428, 6450, 6452, 6453, 6454, 6457, 6474, 6480, 6491, 6518, 6528, 6533, 6548, 6550, 6554, 6561, 6590, 6615, 6626, 6627, 6633, 6645, 6651, 6655, 6658, 6662, 6676, 6689, 6696, 6699, 6702, 6705, 6725, 6734, 6756, 6776, 6811, 6846, 6849, 6855, 6870, 6892, 6895, 6911, 6921, 6923, 6933, 6939, 6946, 6955, 6978, 6981, 6990, 7012, 7022, 7053, 7067, 7069, 7149, 7158, 7181, 7219, 7241, 7253, 7266, 7270, 7301, 7303, 7308, 7347, 7350, 7373, 7374, 7386, 7423, 7447, 7456, 7460, 7466, 7468, 7504, 7508, 7517, 7535, 7544, 7561, 7577, 7616, 7638, 7647, 7666, 7691, 7695, 7702, 7704, 7717, 7729, 7737, 7752, 7769, 7773, 7786, 7798, 7799, 7815, 7839, 7841, 7849, 7860, 7864, 7880, 7918, 7945, 7949, 7960, 7965, 8006, 8046, 8049, 8055, 8057, 8060, 8068, 8086, 8098, 8115, 8124, 8125, 8131, 8132, 8167, 8169, 8184, 8186, 8202, 8208, 8234, 8238, 8245, 8250, 8258, 8275, 8280, 8287, 8292, 8306, 8309, 8314, 8316, 8331, 8358, 8362, 8373, 8374, 8384, 8388, 8412, 8423, 8431, 8450, 8451, 8457, 8465, 8485, 8508, 8515, 8517, 8522, 8524, 8551, 8586, 8594, 8608, 8612, 8634, 8641, 8652, 8685, 8708, 8717, 8722, 8731, 8772, 8775, 8779, 8814, 8829, 8844, 8850, 8876, 8880, 8895, 8905, 8924, 8934, 8946, 8947, 8966, 8973, 8988, 8994, 9004, 9005, 9008, 9030, 9031, 9043, 9051, 9052, 9059, 9069, 9081, 9087, 9090, 9096, 9110, 9112, 9115, 9118, 9126, 9130, 9132, 9148, 9172, 9176, 9178, 9182, 9183, 9185, 9188, 9210, 9237, 9238, 9241, 9247, 9248, 9256, 9261, 9278, 9296, 9298, 9304, 9311, 9330, 9333, 9357, 9368, 9369, 9378, 9392, 9395, 9418, 9426, 9438, 9440, 9445, 9477, 9481, 9488, 9497, 9504, 9509, 9534, 9537, 9564, 9568, 9589, 9609, 9613, 9637, 9642, 9643, 9666, 9671, 9686, 9688, 9702, 9704, 9706, 9719, 9734, 9742, 9770, 9779, 9782, 9783, 9791, 9794, 9795, 9797, 9811, 9813, 9822, 9823, 9834, 9839, 9855, 9867, 9893, 9899, 9906, 9907, 9908, 9912, 9929, 9932, 9963, 9967, 9978, 9988, 10000, 10013, 10076, 10083, 10094, 10112, 10114, 10124, 10134, 10163, 10190, 10197, 10198, 10211, 10214, 10217, 10230, 10233, 10237, 10274, 10307, 10310, 10311, 10314, 10323, 10328, 10335, 10338, 10381, 10384, 10386, 10387, 10399, 10413, 10415, 10430, 10444, 10456, 10463, 10471, 10475, 10514, 10525, 10539, 10542, 10550, 10569, 10580, 10595, 10604, 10611, 10612, 10628, 10630, 10631, 10642, 10644, 10660, 10661, 10701, 10702, 10715, 10743, 10757, 10759, 10761, 10767, 10782, 10813, 10857, 10864, 10875, 10891, 10904, 10917, 10947, 10950, 10959, 10973, 10979, 10993, 11006, 11026, 11035, 11041, 11060, 11064, 11065, 11071, 11072, 11073, 11079, 11085, 11100, 11101, 11107, 11137, 11140, 11144, 11148, 11163, 11165, 11168, 11176, 11215, 11216, 11218, 11261, 11283, 11289, 11317, 11331, 11341, 11364, 11389, 11401, 11417, 11424, 11441, 11444, 11445, 11449, 11473, 11493, 11506, 11509, 11512, 11519, 11537, 11538, 11543, 11555, 11558, 11582, 11611, 11618, 11619, 11631, 11638, 11657, 11666, 11668, 11681, 11691, 11704, 11721, 11722, 11724, 11767, 11788, 11789, 11796, 11804, 11809, 11812, 11824, 11827, 11838, 11846, 11861, 11864, 11865, 11872, 11906, 11919, 11931, 11965, 11993, 12000, 12002, 12051, 12074, 12076, 12081, 12088, 12104, 12123, 12135, 12139, 12144, 12147, 12166, 12191, 12231, 12240, 12260, 12263, 12272, 12279, 12293, 12303, 12312, 12319, 12323, 12339, 12356, 12357, 12362, 12365, 12396, 12403, 12404, 12414, 12426, 12429, 12433, 12439, 12459, 12543, 12568, 12577, 12607, 12620, 12629, 12651, 12653, 12655, 12666, 12676, 12686, 12690, 12707, 12709, 12711, 12720, 12738, 12739, 12743, 12763, 12771, 12781, 12791, 12800, 12804, 12806, 12815, 12829, 12831, 12849, 12866, 12873, 12894, 12912, 12920, 12925, 12937, 12940, 12943, 12951, 12952, 12959, 12961, 12987, 13033, 13054, 13068, 13070, 13096, 13108, 13114, 13131, 13139, 13143, 13149, 13152, 13175, 13176, 13180, 13181, 13187, 13213, 13219, 13262, 13265, 13294, 13302, 13335, 13347, 13351, 13355, 13377, 13386, 13394, 13408, 13409, 13427, 13435, 13437, 13440, 13457, 13479, 13480, 13485, 13501, 13506, 13526, 13529, 13540, 13557, 13593, 13594, 13608, 13619, 13629, 13631, 13638, 13641, 13681, 13687, 13700, 13703, 13715, 13719, 13723, 13725, 13734, 13739, 13743, 13747, 13778, 13802, 13816, 13887, 13891, 13899, 13901, 13913, 13933, 13942, 13944, 13957, 13970, 13977, 13981, 13984, 13991, 13994, 14021, 14037, 14039, 14048, 14070, 14076, 14077, 14101, 14106, 14107, 14108, 14109, 14123, 14127, 14158, 14161, 14180, 14210, 14248, 14253, 14255, 14266, 14276, 14299, 14302, 14330, 14332, 14372, 14376, 14397, 14411, 14412, 14419, 14423, 14425, 14429, 14438, 14452, 14458, 14460, 14461, 14463, 14475, 14500, 14502, 14503, 14516, 14521, 14542, 14577, 14602, 14618, 14622, 14628, 14646, 14648, 14654, 14655, 14687, 14699, 14703, 14720, 14725, 14728, 14730, 14747, 14762, 14764, 14773, 14785, 14790, 14808, 14813, 14821, 14849, 14854, 14859, 14865, 14911, 14918, 14929, 14930, 14934, 14946, 14960, 14995, 15012, 15021, 15068, 15071, 15082, 15144, 15194, 15198, 15205, 15221, 15223, 15226, 15253, 15270, 15272, 15274, 15284, 15292, 15303, 15311, 15355, 15374, 15377, 15394, 15396, 15417, 15428, 15430, 15450, 15466, 15480, 15481, 15486, 15487, 15501, 15503, 15546, 15567, 15596, 15610, 15640, 15641, 15645, 15646, 15648, 15655, 15662, 15674, 15693, 15701, 15717, 15718, 15719, 15723, 15724, 15726, 15728, 15734, 15751, 15753, 15758, 15770, 15782, 15817, 15819, 15829, 15832, 15854, 15861, 15869, 15871, 15873, 15893, 15895, 15904, 15929, 15932, 15964, 15967, 15969, 16024, 16031, 16071, 16084, 16088, 16093, 16096, 16105, 16109, 16115, 16117, 16122, 16133, 16139, 16145, 16149, 16165, 16166, 16178, 16180, 16196, 16199, 16236, 16248, 16249, 16269, 16271, 16275, 16288, 16298, 16307, 16308, 16329, 16337, 16373, 16396, 16404, 16407, 16412, 16436, 16443, 16455, 16457, 16460, 16463, 16476, 16492, 16493, 16523, 16534, 16540, 16560, 16567, 16619, 16630, 16633, 16657, 16660, 16669, 16670, 16690, 16695, 16697, 16720, 16727, 16732, 16744, 16751, 16792, 16800, 16802, 16809, 16815, 16817, 16821, 16823, 16832, 16856, 16858, 16860, 16876, 16877, 16879, 16892, 16905, 16916, 16920, 16943, 16948, 16954, 16965, 16978, 17003, 17010, 17025, 17065, 17102, 17114, 17116, 17124, 17128, 17131, 17132, 17139, 17145, 17150, 17164, 17171, 17172, 17191, 17212, 17234, 17259, 17267, 17278, 17310, 17318, 17324, 17368, 17374, 17389, 17412, 17419, 17439, 17440, 17456, 17510, 17512, 17518, 17526, 17531, 17532, 17543, 17546, 17559, 17582, 17596, 17598, 17600, 17622, 17628, 17634, 17669, 17677, 17681, 17703, 17712, 17717, 17736, 17738, 17743, 17748, 17756, 17774, 17775, 17786, 17788, 17791, 17793, 17804, 17819, 17820, 17831, 17835, 17839, 17844, 17860, 17873, 17882, 17891, 17894, 17905, 17907, 17912, 17920, 17923, 17926, 17955, 17956, 18032, 18035, 18044, 18055, 18059, 18068, 18076, 18078, 18103, 18140, 18169, 18181, 18183, 18186, 18193, 18200, 18205, 18211, 18227, 18233, 18234, 18239, 18254, 18280, 18287, 18293, 18307, 18309, 18321, 18342, 18351, 18357, 18367, 18377, 18393, 18416, 18434, 18435, 18447, 18469, 18471, 18475, 18484, 18498, 18524, 18527, 18564, 18565, 18594, 18598, 18614, 18618, 18620, 18621, 18625, 18659, 18665, 18684, 18688, 18699, 18708, 18721, 18724, 18733, 18736, 18738, 18752, 18754, 18762, 18768, 18778, 18782, 18802, 18807, 18818, 18836, 18846, 18922, 18936, 18961, 18971, 18972, 18978, 18985, 19000, 19012, 19018, 19050, 19067, 19101, 19102, 19106, 19127, 19159, 19168, 19173, 19183, 19184, 19212, 19213, 19278, 19286, 19292, 19293, 19314, 19317, 19333, 19345, 19347, 19372, 19382, 19444, 19446, 19473, 19475, 19483, 19506, 19518, 19529, 19541, 19546, 19561, 19580, 19593, 19594, 19607, 19623, 19627, 19631, 19641, 19642, 19667, 19678, 19690, 19705, 19726, 19734, 19736, 19738, 19749, 19754, 19765, 19781, 19785, 19786, 19810, 19819, 19827, 19830, 19836, 19847, 19848, 19859, 19870, 19926, 19932, 19942, 19947, 19983, 19998, 20000, 20009, 20040, 20072, 20077, 20107, 20126, 20135, 20138, 20149, 20171, 20182, 20197, 20199, 20215, 20219, 20225, 20237, 20242, 20246, 20247, 20263, 20288, 20304, 20310, 20312, 20333, 20335, 20338, 20340, 20359, 20386, 20388, 20412, 20441, 20452, 20462, 20465, 20499, 20503, 20521, 20538, 20550, 20603, 20605, 20620, 20646, 20654, 20691, 20693, 20714, 20724, 20731, 20742, 20746, 20748, 20754, 20758, 20766, 20777, 20779, 20789, 20824, 20827, 20830, 20856, 20879, 20895, 20900, 20906, 20918, 20922, 20942, 20952, 20955, 20968, 20970, 20978, 20988, 20990, 20996, 21005, 21055, 21056, 21071, 21079, 21124, 21130, 21141, 21148, 21159, 21176, 21192, 21195, 21197, 21209, 21218, 21220, 21223, 21229, 21233, 21238, 21247, 21259, 21267, 21274, 21301, 21302, 21325, 21346, 21350, 21354, 21358, 21374, 21377, 21385, 21391, 21395, 21402, 21414, 21420, 21426, 21442, 21444, 21456, 21458, 21467, 21477, 21515, 21533, 21538, 21547, 21569, 21572, 21575, 21581, 21586, 21599, 21606, 21611, 21640, 21656, 21663, 21668, 21677, 21681, 21683, 21700, 21709, 21710, 21737, 21740, 21745, 21766, 21784, 21793, 21809, 21859, 21882, 21886, 21896, 21910, 21925, 21929, 21930, 21934, 21935, 21937, 21957, 21965, 21970, 21977, 22014, 22029, 22039, 22050, 22056, 22064, 22065, 22068, 22079, 22094, 22096, 22107, 22132, 22137, 22138, 22140, 22152, 22160, 22177, 22186, 22187, 22213, 22217, 22230, 22243, 22266, 22280, 22298, 22304, 22335, 22337, 22359, 22361, 22391, 22400, 22403, 22434, 22446, 22453, 22460, 22465, 22476, 22490, 22499, 22507, 22512, 22518, 22548, 22550, 22560, 22562, 22564, 22567, 22571, 22585, 22596, 22616, 22618, 22623, 22642, 22658, 22660, 22668, 22677, 22681, 22682, 22704, 22706, 22722, 22723, 22727, 22732, 22735, 22784, 22794, 22846, 22847, 22856, 22865, 22871, 22878, 22885, 22912, 22944, 22951, 22972, 22997, 23001, 23005, 23017, 23021, 23041, 23048, 23053, 23057, 23059, 23061, 23078, 23089, 23108, 23122, 23141, 23150, 23175, 23231, 23290, 23294, 23299, 23308, 23312, 23320, 23325, 23329, 23339, 23363, 23418, 23450, 23458, 23470, 23475, 23483, 23486, 23517, 23530, 23538, 23539, 23548, 23568, 23575, 23581, 23583, 23591, 23603, 23619, 23620, 23637, 23647, 23669, 23681, 23691, 23697, 23698, 23715, 23725, 23748, 23749, 23756, 23770, 23785, 23786, 23799, 23840, 23847, 23869, 23873, 23884, 23896, 23904, 23930, 23936, 23983, 23984, 23985, 23986, 24001, 24019, 24021, 24022, 24031, 24049, 24053, 24056, 24076, 24077, 24086, 24126, 24128, 24136, 24149, 24150, 24168, 24170, 24185, 24190, 24221, 24223, 24225, 24234, 24255, 24262, 24264, 24296, 24325, 24326, 24336, 24373, 24376, 24383, 24401, 24407, 24409, 24431, 24432, 24447, 24457, 24476, 24489, 24496, 24504, 24523, 24524, 24528, 24553, 24556, 24571, 24575, 24603, 24614, 24631, 24638, 24641, 24644, 24660, 24689, 24690, 24692, 24694, 24716, 24747, 24749, 24754, 24766, 24771, 24773, 24776, 24783, 24788, 24790, 24792, 24845, 24863, 24866, 24880, 24893, 24895, 24896, 24909, 24921, 24951, 24963, 24988, 25001, 25056, 25065, 25070, 25073, 25093, 25095, 25105, 25106, 25111, 25112, 25131, 25133, 25142, 25147, 25150, 25166, 25172, 25174, 25178, 25179, 25180, 25184, 25188, 25196, 25216, 25256, 25257, 25264, 25285, 25286, 25290, 25302, 25303, 25313, 25328, 25332, 25367, 25374, 25391, 25405, 25410, 25436, 25454, 25484, 25492, 25497, 25509, 25515, 25516, 25517, 25522, 25533, 25534, 25553, 25555, 25557, 25558, 25562, 25582, 25585, 25596, 25599, 25622, 25647, 25658, 25662, 25683, 25690, 25692, 25727, 25739, 25740, 25757, 25782, 25788, 25811, 25836, 25837, 25859, 25884, 25886, 25894, 25905, 25915, 25917, 25919, 25941, 25955, 25963, 25967, 25989, 26001, 26002, 26012, 26014, 26019, 26029, 26041, 26045, 26051, 26052, 26062, 26067, 26073, 26074, 26086, 26089, 26101, 26108, 26117, 26130, 26134, 26136, 26142, 26143, 26151, 26155, 26167, 26191, 26201, 26203, 26206, 26227, 26234, 26262, 26265, 26310, 26316, 26322, 26325, 26337, 26344, 26356, 26368, 26372, 26376, 26380, 26388, 26390, 26401, 26413, 26418, 26441, 26450, 26462, 26484, 26489, 26490, 26497, 26499, 26501, 26515, 26527, 26532, 26538, 26568, 26583, 26593, 26594, 26602, 26626, 26655, 26658, 26661, 26676, 26693, 26704, 26719, 26720, 26747, 26759, 26763, 26770, 26783, 26836, 26856, 26864, 26866, 26869, 26884, 26897, 26917, 26925, 26936, 26937, 26945, 27003, 27021, 27024, 27036, 27039, 27044, 27059, 27063, 27066, 27069, 27072, 27097, 27098, 27117, 27122, 27133, 27135, 27137, 27142, 27183, 27202, 27204, 27229, 27230, 27239, 27266, 27267, 27271, 27278, 27282, 27286, 27293, 27294, 27295, 27327, 27328, 27331, 27350, 27369, 27375, 27388, 27400, 27404, 27419, 27423, 27424, 27431, 27436, 27442, 27451, 27455, 27463, 27466, 27468, 27477, 27490, 27491, 27498, 27499, 27500, 27505, 27532, 27537, 27541, 27562, 27572, 27587, 27603, 27617, 27619, 27650, 27658, 27664, 27675, 27677, 27696, 27701, 27712, 27725, 27744, 27759, 27773, 27777, 27783, 27797, 27823, 27829, 27831, 27842, 27853, 27860, 27861, 27865, 27890, 27933, 27934, 27935, 27937, 27944, 27945, 27970, 28004, 28011, 28017, 28018, 28036, 28044, 28048, 28054, 28082, 28100, 28112, 28115, 28135, 28154, 28168, 28171, 28197, 28200, 28206, 28217, 28256, 28264, 28270, 28278, 28302, 28304, 28305, 28326, 28335, 28343, 28348, 28357, 28361, 28376, 28386, 28401, 28410, 28412, 28413, 28436, 28440, 28450, 28455, 28458, 28464, 28473, 28474, 28476, 28484, 28503, 28526, 28527, 28559, 28576, 28588, 28591, 28592, 28613, 28631, 28639, 28646, 28650, 28660, 28679, 28698, 28712, 28713, 28720, 28735, 28741, 28742, 28745, 28756, 28765, 28775, 28793, 28797, 28801, 28811, 28832, 28836, 28847, 28857, 28859, 28869, 28875, 28902, 28913, 28937, 28949, 28960, 28992, 29006, 29007, 29012, 29018, 29031, 29034, 29045, 29051, 29057, 29077, 29079, 29083, 29090, 29091, 29110, 29125, 29129, 29147, 29153, 29154, 29178, 29191, 29194, 29198, 29208, 29232, 29247, 29248, 29252, 29256, 29264, 29278, 29292, 29300, 29307, 29309, 29313, 29317, 29320, 29336, 29345, 29368, 29372, 29382, 29390, 29398, 29415, 29425, 29444, 29445, 29450, 29456, 29464, 29488, 29498, 29501, 29503, 29507, 29515, 29539, 29540, 29541, 29543, 29549, 29572, 29576, 29585, 29590, 29612, 29615, 29616, 29625, 29626, 29637, 29641, 29652, 29669, 29680, 29699, 29703, 29721, 29723, 29724, 29731, 29736, 29751, 29775, 29795, 29805, 29817, 29829, 29840, 29841, 29862, 29863, 29868, 29905, 29906, 29919, 29951, 29988, 29989, 29998, 30041, 30044, 30046, 30055, 30068, 30078, 30095, 30116, 30143, 30144, 30165, 30193, 30211, 30217, 30236, 30238, 30240, 30246, 30290, 30293, 30296, 30307, 30331, 30365, 30376, 30388, 30419, 30434, 30461, 30469, 30482, 30491, 30494, 30496, 30536, 30547, 30548, 30552, 30576, 30581, 30615, 30616, 30633, 30644, 30678, 30689, 30690, 30693, 30699, 30730, 30733, 30738, 30746, 30770, 30816, 30836, 30839, 30870, 30875, 30912, 30930, 30934, 30943, 30955, 30956, 30977, 30985, 30988, 30999, 31013, 31024, 31037, 31061, 31067, 31075, 31080, 31117, 31156, 31171, 31200, 31202, 31208, 31211, 31226, 31236, 31237, 31243, 31247, 31263, 31275, 31307, 31317, 31336, 31391, 31393, 31404, 31407, 31434, 31455, 31462, 31464, 31472, 31486, 31488, 31499, 31501, 31505, 31515, 31532, 31552, 31559, 31560, 31572, 31581, 31583, 31592, 31610, 31611, 31636, 31638, 31645, 31648, 31657, 31661, 31700, 31709, 31712, 31732, 31751, 31758, 31760, 31766, 31775, 31780, 31783, 31801, 31839, 31841, 31845, 31868, 31882, 31894, 31895, 31934, 31935, 31972, 31982, 31985, 31993, 32012, 32023, 32029, 32072, 32082, 32084, 32096, 32098, 32113, 32122, 32129, 32172, 32191, 32214, 32221, 32239, 32241, 32249, 32250, 32286, 32302, 32332, 32347, 32371, 32413, 32435, 32459, 32471, 32484, 32498, 32502, 32504, 32513, 32527, 32573, 32622, 32623, 32625, 32637, 32643, 32678, 32716, 32720, 32734, 32736, 32740, 32746, 32756, 32759, 32766, 32774, 32790, 40123, 40187, 40227, 40287, 40337, 40435, 40485, 40510, 40519, 40560, 40597, 40601, 40622, 40684, 40687, 40695, 40761, 40788, 40819, 40832, 40861, 40871, 40878, 40882, 40923, 40940, 40976, 41014, 41022, 41024, 41061, 41144, 41150, 41156, 41180, 41195, 41234, 41251, 41289, 41308, 41312, 41350, 41384, 41405, 41432, 41448, 41517, 41525, 41621, 41639, 41640, 41704, 41714, 41733, 41875, 41897, 41929, 41956, 42009, 42031, 42054, 42055, 42180, 42188, 42274, 42281, 42390, 42397, 42400, 42412, 42430, 42444, 42479, 42501, 42510, 42515, 42519, 42668, 42697, 42725, 42795, 42843, 42857, 42872, 42885, 42892, 42924, 42971, 43043, 43067, 43074, 43116, 43147, 43175, 43209, 43220, 43258, 43426, 43439, 43481, 43540, 43610, 43656, 43725, 43748, 43881, 43893, 43907, 43917, 43927, 43932, 43943, 43961, 44082, 44128, 44158, 44222, 44225, 44305, 44319, 44337, 44416, 44514, 44527, 44545, 44553, 44644, 44656, 44784, 44788, 44823, 44894, 44929, 44956, 44973, 44996, 45050, 45071, 45132, 45160, 45210, 45233, 45307, 45309, 45380, 45455, 45492, 45505, 45580, 45585, 45650, 45684, 45688, 45753, 45761, 45767, 45775, 45851, 45936, 45979, 45981, 45994, 46011, 46039, 46093, 46108, 46114, 46127, 46178, 46223, 46297, 46399, 46473, 46480, 46519, 46615, 46641, 46642, 46695, 46740, 46763, 46796, 46815, 46820, 46904, 46943, 46959, 47003, 47100, 47183, 47194, 47247, 47287, 47306, 47326, 47385, 47409, 47420, 47448, 47483, 47522, 47634, 47660, 47673, 47684, 47721, 47816, 47821, 47853, 47858, 47874, 47909, 47921, 47953, 47974, 47989, 47993, 48010, 48064, 48153, 48159, 48231, 48237, 48314, 48346, 48358, 48370, 48383, 48418, 48422, 48439, 48542, 48657, 48667, 48672, 48701, 48706, 48718, 48732, 48777, 48996, 49015, 49019, 49024, 49028, 49038, 49191, 49202, 49276, 49281, 49322, 49327, 49341, 49443, 49446, 49480, 49514, 49525, 49565, 49594, 49604, 49623, 49687, 49873, 49879, 49925, 50039, 50079, 50137, 50182, 50191, 50231, 50257, 50312, 50379, 50424, 50447, 50618, 50629, 50643, 50664, 50738, 50757, 50761, 50816, 50828, 50855, 50879, 50890, 50899, 50923, 51004, 51025, 51078, 51178, 51200, 51210, 51230, 51238, 51301, 51320, 51384, 51387, 51413, 51515, 51545, 51582, 51591, 51613, 51615, 51635, 51658, 51682, 51684, 51735, 51746, 51795, 51823, 51836, 51856, 51951, 51964, 51998, 52021, 52094, 52183, 52232, 52252, 52254, 52307, 52344, 52370, 52409, 52494, 52533, 52537, 52620, 52641, 52647, 52663, 52676, 52732, 52766, 52826, 52831, 52897, 52904, 53019, 53024, 53070, 53247, 53269, 53271, 53294, 53310, 53342, 53425, 53474, 53479, 53492, 53534, 53549, 53578, 53600, 53609, 53676, 53683, 53714, 53782, 53832, 53861, 53864, 53884, 53939, 53991, 54031, 54082, 54088, 54096, 54130, 54203, 54221, 54254, 54340, 54364, 54381, 54393, 54398, 54506, 54535, 54540, 54600, 54636, 54639, 54681, 54724, 54750, 54817, 54826, 54832, 54878, 54883, 54934, 54945, 54993, 54994, 55005, 55012, 55036, 55049, 55115, 55141, 55163, 55180, 55232, 55245, 55276, 55360, 55370, 55413, 55440, 55642, 55646, 55713, 55753, 55778, 55810, 55880, 55925, 55934, 56001, 56052, 56097, 56112, 56115, 56149, 56175, 56353, 56378, 56409, 56427, 56472, 56478, 56490, 56529, 56562, 56582, 56593, 56646, 56651, 56689, 56703, 56757, 56784, 56849, 56855, 56873, 57158, 57168, 57169, 57180, 57190, 57233, 57293, 57490, 57579, 57594, 57632, 57697, 57738, 57763, 57806, 57817, 57848, 57905, 57908, 57923, 57955, 58010, 58019, 58055, 58097, 58102, 58116, 58134, 58154, 58157, 58167, 58199, 58305, 58324, 58337, 58414, 58441, 58451, 58466, 58510, 58562, 58597, 58643, 58649, 58700, 58763, 58836, 58868, 58945, 58957, 59004, 59049, 59102, 59124, 59135, 59186, 59188, 59273, 59374, 59462, 59736, 59789, 59825, 59833, 59918, 59922, 59964, 60014, 60032, 60037, 60054, 60115, 60170, 60176, 60219, 60262, 60294, 60303, 60306, 60403, 60552, 60641, 60660, 60666, 60720, 60776, 60952, 61005, 61007, 61041, 61068, 61073, 61121, 61132, 61155, 61179, 61180, 61183, 61194, 61207, 61215, 61406, 61526, 61606, 61665, 61670, 61729, 61739, 61748, 61791, 61794, 61823, 61825, 61833, 61852, 61855, 61857, 61913, 61940, 62018, 62072, 62135, 62141, 62183, 62186, 62237, 62238, 62323, 62413, 62416, 62447, 62487, 62495, 62528, 62536, 62562, 62608, 62626, 62637, 62653, 62710, 62733, 62762, 62782, 62828, 62829, 62845, 62855, 62860, 62921, 62994, 63003, 63039, 63041, 63066, 63079, 63131, 63145, 63240, 63245, 63292, 63307, 63311, 63351, 63407, 63431, 63439, 63473, 63477, 63499, 63512, 63536, 63541, 63601, 63659, 63685, 63733, 63741, 63749, 63755, 63769, 63834, 63849, 63867, 63921, 63926, 63932, 63936, 63955, 64063, 64067, 64153, 64255, 64286, 64296, 64315, 64332, 64336, 64407, 64416, 64421, 64450, 64695, 64709, 64739, 64772, 64845, 64884, 64916, 64935, 64992, 64995, 65006, 65013, 65015, 65030, 65055, 65089, 65129, 65144, 65152, 65196, 65217, 65250, 65263, 65328, 65353, 65412, 65454, 65469, 65515, 65555, 65570, 65594, 65609, 65654, 65675, 65710, 65843, 65878, 65906, 65915, 65924, 65949, 65991, 66016, 66086, 66115, 66165, 66184, 66189, 66206, 66337, 66389, 66399, 66542, 66549, 66598, 66631, 66643, 66654, 66656, 66664, 66671, 66688, 66712, 66714, 66727, 66770, 66786, 66787, 66822, 67037, 67050, 67069, 67104, 67112, 67154, 67177, 67195, 67209, 67216, 67248, 67260, 67272, 67306, 67388, 67395, 67397, 67446, 67449, 67468, 67473, 67511, 67583, 67648, 67657, 67710, 67760, 67823, 67877, 67938, 67996, 68036, 68104, 68177, 68184, 68204, 68217, 68310, 68344, 68377, 68402, 68409, 68426, 68433, 68450, 68454, 68542, 68607, 68642, 68664, 68676, 68691, 68709, 68753, 68796, 68812, 68813, 68875, 68890, 68923, 68958, 68965, 68998, 69141, 69225, 69267, 69281, 69339, 69464, 69569, 69611, 69676, 69685, 69702, 69786, 69851, 69858, 69891, 69900, 69903, 69932, 70004, 70016, 70017, 70025, 70048, 70072, 70104, 70108, 70115, 70119, 70178, 70188, 70223, 70301, 70303, 70313, 70340, 70363, 70427, 70476, 70516, 70521, 70622, 70694, 70753, 70755, 70794, 70916, 70944, 70990, 71011, 71021, 71054, 71083, 71130, 71211, 71219, 71293, 71297, 71320, 71322, 71332, 71369, 71461, 71464, 71467, 71483, 71487, 71545, 71559, 71615, 71618, 71702, 71713, 71808, 71851, 71884, 71924, 71955, 71974, 71988, 72067, 72095, 72209, 72219, 72260, 72284, 72323, 72335, 72354, 72357, 72439, 72483, 72522, 72592, 72688, 72819, 72854, 72867, 72886, 72931, 72940, 72960, 72993, 73008, 73037, 73039, 73124, 73143, 73150, 73156, 73242, 73292, 73295, 73303, 73322, 73339, 73417, 73425, 73488, 73504, 73536, 73539, 73553, 73594, 73608, 73645, 73652, 73758, 73837, 73843, 73902, 73917, 73971, 74098, 74129, 74159, 74164, 74188, 74258, 74286, 74410, 74422, 74443, 74463, 74509, 74529, 74586, 74599, 74683, 74793, 74852, 74869, 74883, 74913, 74924, 74967, 75071, 75099, 75134, 75193, 75194, 75201, 75244, 75248, 75369, 75432, 75488, 75503, 75586, 75631, 75677, 75685, 75752, 75762, 75808, 75869, 75886, 75894, 75972, 75993, 76096, 76116, 76154, 76173, 76178, 76257, 76315, 76327, 76386, 76397, 76630, 76683, 76698, 76736, 76739, 76751, 76782, 76867, 76915, 76957, 77031, 77046, 77131, 77132, 77148, 77203, 77217, 77259, 77282, 77370, 77439, 77469, 77478, 77484, 77524, 77526, 77543, 77565, 77589, 77617, 77654, 77703, 77711, 77716, 77734, 77738, 77852, 77924, 77926, 77980, 77992, 78012, 78038, 78102, 78127, 78182, 78290, 78309, 78342, 78357, 78415, 78473, 78515, 78672, 78778, 78801, 78824, 78839, 78855, 78910, 78956, 79017, 79069, 79072, 79084, 79090, 79115, 79117, 79118, 79172, 79262, 79272, 79344, 79352, 79365, 79372, 79594, 79709, 79876, 79900, 79976, 80024, 80041, 80046, 80059, 80119, 80121, 80134, 80142, 80154, 80259, 80439, 80482, 80536, 80551, 80594, 80596, 80759, 80779, 80803, 80813, 80860, 80878, 80883, 80905, 80963, 81020, 81041, 81085, 81111, 81157, 81179, 81187, 81268, 81317, 81354, 81371, 81378, 81456, 81461, 81532, 81543, 81546, 81560, 81597, 81671, 81700, 81770, 81773, 81817, 81846, 81866, 81938, 81975, 82015, 82048, 82118, 82139, 82160, 82202, 82207, 82229, 82257, 82329, 82380, 82408, 82451, 82454, 82465, 82529, 82534, 82544, 82554, 82562, 82564, 82625, 82633, 82656, 82702, 82765, 82784, 82794, 82851, 82986, 83022, 83034, 83222, 83288, 83321, 83326, 83332, 83358, 83365, 83395, 83406, 83430, 83537, 83603, 83682, 83691, 83789, 83857, 83894, 83963, 83968, 84082, 84089, 84116, 84130, 84153, 84165, 84232, 84297, 84433, 84437, 84445, 84474, 84501, 84606, 84687, 84708, 84749, 84849, 84941, 84966, 85050, 85169, 85175, 85255, 85278, 85281, 85350, 85444, 85464, 85572, 85644, 85699, 85714, 85749, 85753, 85776, 85802, 85901, 85987, 85994, 86018, 86068, 86102, 86108, 86122, 86143, 86146, 86148, 86165, 86254, 86259, 86276, 86318, 86340, 86377, 86383, 86392, 86402, 86410, 86487, 86498, 86531, 86544, 86556, 86596, 86628, 86666, 86711, 86764, 86765, 86866, 86879, 86913, 86965, 86969, 86984, 86997, 87043, 87049, 87055, 87081, 87111, 87257, 87308, 87331, 87350, 87352, 87365, 87425, 87459, 87468, 87470, 87481, 87526, 87530, 87547, 87601, 87640, 87657, 87803, 87867, 87869, 87896, 87959, 88025, 88078, 88106, 88157, 88265, 88266, 88277, 88294, 88308, 88309, 88526, 88532, 88591, 88595, 88702, 88731, 88801, 88804, 88805, 88826, 88857, 88866, 88907, 88978, 88985, 89026, 89045, 89064, 89091, 89187, 89297, 89328, 89374, 89394, 89459, 89503, 89553, 89575, 89633, 89711, 89734, 89752, 89765, 89816, 89840, 89867, 89971, 89973, 90040, 90046, 90096, 90135, 90213, 90242, 90296, 90303, 90365, 90373, 90479, 90484, 90585, 90618, 90688, 90756, 90788, 90794, 90854, 90891, 90899, 90959, 90972, 90992, 91062, 91092, 91151, 91152, 91258, 91336, 91361, 91438, 91453, 91462, 91470, 91549, 91554, 91580, 91581, 91665, 91673, 91720, 91752, 91819, 91867, 91881, 91904, 91975, 91983, 92064, 92092, 92135, 92145, 92200, 92281, 92292, 92387, 92415, 92420, 92425, 92446, 92473, 92475, 92640, 92644, 92651, 92652, 92752, 92764, 92772, 92839, 92841, 92864, 92984, 93018, 93021, 93026, 93031, 93077, 93093, 93134, 93196, 93227, 93229, 93272, 93318, 93437, 93458, 93518, 93521, 93610, 93631, 93632, 93633, 93653, 93671, 93679, 93704, 93810, 93831, 94049, 94050, 94064, 94079, 94135, 94147, 94221, 94229, 94312, 94327, 94407, 94491, 94537, 94677, 94759, 94782, 94889, 94903, 94932, 94950, 94993, 95013, 95071, 95088, 95107, 95200, 95210, 95238, 95273, 95312, 95390, 95495, 95523, 95530, 95634, 95726, 95750, 95771, 95772, 95924, 95933, 95952, 96057, 96108, 96111, 96174, 96199, 96225, 96238, 96244, 96252, 96305, 96369, 96429, 96491, 96591, 96686, 96793, 96838, 96924, 97013, 97091, 97149, 97316, 97407, 97421, 97442, 97525, 97543, 97545, 97578, 97581, 97585, 97649, 97733, 97786, 97849, 97890, 97924, 97927, 98009, 98028, 98043, 98070, 98139, 98177, 98242, 98254, 98297, 98362, 98582, 98604, 98713, 98744, 98753, 98829, 98887, 98948, 99008, 99018, 99038, 99063, 99096, 99100, 99102, 99287, 99346, 99366, 99374, 99423, 99454, 99461, 99573, 99613, 99862, 99865, 99938]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f50ZKV2NEGdb"},"outputs":[],"source":["data_ids = [(new_train_ids, new_dev_ids, new_test_ids)]\n","data_names = [\"new\"]\n","\n","for i, (tr, de, te) in zip(data_names, data_ids):\n","    \n","    y_train = Ys_train.loc[tr]\n","    y_dev = Ys_dev.loc[de]\n","    y_test = Ys_test.loc[te]\n","\n","    sub_train = lvl2_train.loc[tr]\n","    sub_train = sub_train.loc[:, pd.IndexSlice[:, 'mean']]\n","\n","    sub_dev = lvl2_dev.loc[de]\n","    sub_dev = sub_dev.loc[:, pd.IndexSlice[:, 'mean']]\n","\n","    sub_test = lvl2_test.loc[te]\n","    sub_test = sub_test.loc[:, pd.IndexSlice[:, 'mean']]\n","    #print(sub_train)\n","    #sub_train = sub_train.as_matrix()\n","    #sub_dev = sub_dev.as_matrix()\n","    #sub_test = sub_test.as_matrix()\n","\n","    sub_train = sub_train.to_numpy()\n","    sub_dev = sub_dev.to_numpy()\n","    sub_test = sub_test.to_numpy()\n","\n","    # reshape the data for timeseries prediction\n","    x_train_lstm = sub_train.reshape(int(sub_train.shape[0] / 24), 24, 104)\n","    x_dev_lstm = sub_dev.reshape(int(sub_dev.shape[0] / 24), 24, 104)\n","    x_test_lstm = sub_test.reshape(int(sub_test.shape[0] / 24), 24, 104)\n","\n","    \n","    pd.to_pickle(x_train_lstm, \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+i+\"_x_train.pkl\")\n","    pd.to_pickle(x_dev_lstm, \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+i+\"_x_dev.pkl\")\n","    pd.to_pickle(x_test_lstm, \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+i+\"_x_test.pkl\")\n","    \n","    pd.to_pickle(y_train, \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+i+\"_y_train.pkl\")\n","    pd.to_pickle(y_dev, \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+i+\"_y_dev.pkl\")\n","    pd.to_pickle(y_test, \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+i+\"_y_test.pkl\")"]},{"cell_type":"code","source":[""],"metadata":{"id":"XdqljLM6ue0v"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jimBrnTCdtTx"},"outputs":[],"source":["#import statistics\n","#from statistics import mean\n","def create_dataset(dict_of_ner):\n","    temp_data = []\n","    for k, v in sorted(dict_of_ner.items()):\n","        temp = []\n","        for embed in v:\n","            temp.append(embed)\n","        temp_data.append(np.mean(temp, axis = 0)) \n","    return np.asarray(temp_data)\n","#def create_dataset(dict_of_ner):\n","#    temp_data = []\n","#    for k, v in sorted(dict_of_ner.items()):\n","#        temp = []\n","#        for embed in v:\n","            #print(type(v))\n","            #ndArray= np.array(embed, dtype=float)\n","#            temp.append(ndArray)\n","            #temp.extend(embed)\n","            #print(\"ndArray[0] \", type(ndArray[0]))\n","            ##rint(\"ndArray \", type(ndArray))\n","        #print(temp.shape)\n","        #print(len(temp))\n","        #temp1 = np.mean(temp, axis=0)\n","        #temp = np.asarray(temp)\n","        #temp1 = list(map(mean, zip(*temp)))\n","        #temp = np.asarray(temp)\n","        #print(temp)\n","        #print(temp.shape)\n","        #temp1 = np.mean(temp)\n","        #temp1 = np.mean(temp, axis=0)\n","        #temp_data.append(temp1)\n","        #temp1 = np.array(list(map(float, temp)))\n","        #temp1 = np.asarray(list(map(float, temp)))\n","        #temp1 = np.sum(temp)\n","        #temp1 = sum(float(t) for t in temp)\n","        #temp1 = map(mean(temp))\n","        #temp = np.asarray(temp)\n","        #length = max(map(len, temp))\n","        #temp = np.array([xi+[None]*(length-len(xi)) for xi in temp])\n","        #temp = np.array([np.array(xi) for xi in temp])\n","        #print(temp.shape)\n","        #print(temp)\n","        #temp1 = np.asarray(list(map(mean, zip(*temp))))\n","        #temp1 = np.stack(temp, axis=0)\n","        #temp2 = np.mean(temp1)\n","        #print(temp2)\n","        #lists = temp\n","        #flat_list = [item for sublist in lists for item in sublist]\n","        #print(np.mean(flat_list))\n","#        print(type(temp[0]))\n","#        temp1 = np.mean(temp)\n","        #print(temp1)\n","#        temp_data.append(temp1)\n"," \n","#    return np.asarray(temp_data)\n","\n","def make_prediction_multi_avg(model, test_data):\n","    probs = model.predict(test_data)\n","    y_pred = [1 if i>=0.5 else 0 for i in probs]\n","    return probs, y_pred\n","\n","def save_scores_multi_avg(predictions, probs, ground_truth, \n","                          \n","                          embed_name, problem_type, iteration, hidden_unit_size,\n","                          \n","                          sequence_name, type_of_ner):\n","    \n","    auc = roc_auc_score(ground_truth, probs)\n","    auprc = average_precision_score(ground_truth, probs)\n","    acc   = accuracy_score(ground_truth, predictions)\n","    F1    = f1_score(ground_truth, predictions)\n","    \n","    result_dict = {}    \n","    result_dict['auc'] = auc\n","    result_dict['auprc'] = auprc\n","    result_dict['acc'] = acc\n","    result_dict['F1'] = F1\n","    \n","    result_path = \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/results/\"\n","    file_name = str(sequence_name)+\"-\"+str(hidden_unit_size)+\"-\"+embed_name\n","    file_name = file_name +\"-\"+problem_type+\"-\"+str(iteration)+\"-\"+type_of_ner+\"-avg-.p\"\n","    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n","\n","    print(auc, auprc, acc, F1)\n","    \n","def avg_ner_model(layer_name, number_of_unit, embedding_name):\n","\n","    if embedding_name == \"concat\":\n","        input_dimension = 200\n","    else:\n","        input_dimension = 100\n","\n","    sequence_input = Input(shape=(24,104))\n","\n","    input_avg = Input(shape=(input_dimension, ), name = \"avg\")        \n","#     x_1 = Dense(256, activation='relu')(input_avg)\n","#     x_1 = Dropout(0.3)(x_1)\n","    \n","    if layer_name == \"GRU\":\n","        x = GRU(number_of_unit)(sequence_input)\n","    elif layer_name == \"LSTM\":\n","        x = LSTM(number_of_unit)(sequence_input)\n","\n","    x = keras.layers.Concatenate()([x, input_avg])\n","\n","    x = Dense(256, activation='relu')(x)\n","    x = Dropout(0.2)(x)\n","    \n","    \n","    #logits_regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n","    logits_regularizer = tf.keras.regularizers.L2(0.01)\n","    \n","    preds = Dense(1, activation='sigmoid',use_bias=False,\n","                         kernel_initializer=tf.keras.initializers.glorot_normal(), \n","                  kernel_regularizer=logits_regularizer)(x)\n","    \n","    \n","    #opt = Adam(lr=0.001, decay = 0.01)\n","    model = Model(inputs=[sequence_input, input_avg], outputs=preds)\n","    model.compile(loss='binary_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['acc'])\n","    \n","    return model"]},{"cell_type":"code","source":[""],"metadata":{"id":"_P87snXgujL4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UWuADlkdtUA"},"outputs":[],"source":["type_of_ner = \"new\"\n","\n","x_train_lstm = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_x_train.pkl\")\n","x_dev_lstm = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_x_dev.pkl\")\n","x_test_lstm = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_x_test.pkl\")\n","\n","y_train = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_y_train.pkl\")\n","y_dev = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_y_dev.pkl\")\n","y_test = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_y_test.pkl\")\n","\n","#ner_word2vec = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_ner_word2vec_limited_dict.pkl\")\n","#ner_fasttext = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_ner_fasttext_limited_dict.pkl\")\n","#ner_concat = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_ner_combined_limited_dict.pkl\")\n","ner_word2vec = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner4_word2vec_limited_dict.pkl\")\n","ner_fasttext = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner4_fasttext_limited_dict.pkl\")\n","ner_concat = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner4_combined_limited_dict.pkl\")\n","\n","new_keys = set(new_word2vec.keys())\n","train_ids = sorted(all_train_ids.intersection(new_keys))\n","dev_ids = sorted(all_dev_ids.intersection(new_keys))\n","test_ids = sorted(all_test_ids.intersection(new_keys))\n","#train_ids = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_train_ids.pkl\")\n","#dev_ids = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_dev_ids.pkl\")\n","#test_ids = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"+type_of_ner+\"_test_ids.pkl\")"]},{"cell_type":"code","source":["# Reset Keras Session\n","def reset_keras(model):\n","    sess = get_session()\n","    clear_session()\n","    sess.close()\n","    sess = get_session()\n","\n","    try:\n","        del model # this is from global space - change this as you need\n","    except:\n","        pass\n","\n","    gc.collect() # if it's done something you should see a number being outputted"],"metadata":{"id":"ur8j43g5ybWk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ihm-zPxBdtUF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"110f9d4b-e2ef-48ce-beb0-712624fbb15b","executionInfo":{"status":"ok","timestamp":1650005030724,"user_tz":300,"elapsed":3443764,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Layer:  GRU\n","Hidden unit:  256\n","Embedding:  word2vec\n","=============================\n","Iteration number:  1\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2885 - acc: 0.9030\n","Epoch 1: val_loss improved from inf to 0.24204, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 26s 101ms/step - loss: 0.2885 - acc: 0.9030 - val_loss: 0.2420 - val_acc: 0.9168\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2407 - acc: 0.9141\n","Epoch 2: val_loss improved from 0.24204 to 0.23059, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 26s 108ms/step - loss: 0.2407 - acc: 0.9141 - val_loss: 0.2306 - val_acc: 0.9168\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2212 - acc: 0.9207\n","Epoch 3: val_loss did not improve from 0.23059\n","238/238 [==============================] - 24s 102ms/step - loss: 0.2212 - acc: 0.9207 - val_loss: 0.2406 - val_acc: 0.9150\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2071 - acc: 0.9248\n","Epoch 4: val_loss did not improve from 0.23059\n","238/238 [==============================] - 21s 90ms/step - loss: 0.2071 - acc: 0.9248 - val_loss: 0.2327 - val_acc: 0.9168\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1919 - acc: 0.9284\n","Epoch 5: val_loss did not improve from 0.23059\n","238/238 [==============================] - 23s 98ms/step - loss: 0.1919 - acc: 0.9284 - val_loss: 0.2441 - val_acc: 0.9122\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1720 - acc: 0.9370\n","Epoch 6: val_loss did not improve from 0.23059\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1720 - acc: 0.9370 - val_loss: 0.2563 - val_acc: 0.9164\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1491 - acc: 0.9453\n","Epoch 7: val_loss did not improve from 0.23059\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1491 - acc: 0.9453 - val_loss: 0.2588 - val_acc: 0.9113\n","0.8750941712926249 0.5628213706870306 0.9112235510579577 0.404320987654321\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2210 - acc: 0.9323\n","Epoch 1: val_loss improved from inf to 0.18392, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 19s 71ms/step - loss: 0.2210 - acc: 0.9323 - val_loss: 0.1839 - val_acc: 0.9432\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1757 - acc: 0.9437\n","Epoch 2: val_loss improved from 0.18392 to 0.18334, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1757 - acc: 0.9437 - val_loss: 0.1833 - val_acc: 0.9436\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1621 - acc: 0.9464\n","Epoch 3: val_loss improved from 0.18334 to 0.17300, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1621 - acc: 0.9464 - val_loss: 0.1730 - val_acc: 0.9427\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1490 - acc: 0.9493\n","Epoch 4: val_loss did not improve from 0.17300\n","238/238 [==============================] - 17s 73ms/step - loss: 0.1490 - acc: 0.9493 - val_loss: 0.1764 - val_acc: 0.9450\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1357 - acc: 0.9534\n","Epoch 5: val_loss did not improve from 0.17300\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1357 - acc: 0.9534 - val_loss: 0.1790 - val_acc: 0.9399\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1213 - acc: 0.9579\n","Epoch 6: val_loss did not improve from 0.17300\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1213 - acc: 0.9579 - val_loss: 0.1801 - val_acc: 0.9358\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1021 - acc: 0.9639\n","Epoch 7: val_loss did not improve from 0.17300\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1021 - acc: 0.9639 - val_loss: 0.1969 - val_acc: 0.9316\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0803 - acc: 0.9721\n","Epoch 8: val_loss did not improve from 0.17300\n","238/238 [==============================] - 17s 70ms/step - loss: 0.0803 - acc: 0.9721 - val_loss: 0.2114 - val_acc: 0.9362\n","0.8839880144201508 0.5200123863342351 0.9408923643054278 0.449678800856531\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6744 - acc: 0.6240\n","Epoch 1: val_loss improved from inf to 0.62675, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 19s 68ms/step - loss: 0.6744 - acc: 0.6240 - val_loss: 0.6268 - val_acc: 0.6691\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6260 - acc: 0.6663\n","Epoch 2: val_loss improved from 0.62675 to 0.62151, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.6260 - acc: 0.6663 - val_loss: 0.6215 - val_acc: 0.6627\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6090 - acc: 0.6810\n","Epoch 3: val_loss improved from 0.62151 to 0.61627, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.6090 - acc: 0.6810 - val_loss: 0.6163 - val_acc: 0.6714\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5951 - acc: 0.6928\n","Epoch 4: val_loss improved from 0.61627 to 0.61525, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 69ms/step - loss: 0.5951 - acc: 0.6928 - val_loss: 0.6153 - val_acc: 0.6733\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5759 - acc: 0.7099\n","Epoch 5: val_loss did not improve from 0.61525\n","238/238 [==============================] - 16s 69ms/step - loss: 0.5759 - acc: 0.7099 - val_loss: 0.6222 - val_acc: 0.6747\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5553 - acc: 0.7225\n","Epoch 6: val_loss did not improve from 0.61525\n","238/238 [==============================] - 16s 69ms/step - loss: 0.5553 - acc: 0.7225 - val_loss: 0.6390 - val_acc: 0.6664\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5244 - acc: 0.7437\n","Epoch 7: val_loss did not improve from 0.61525\n","238/238 [==============================] - 16s 68ms/step - loss: 0.5244 - acc: 0.7437 - val_loss: 0.6561 - val_acc: 0.6516\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4799 - acc: 0.7750\n","Epoch 8: val_loss did not improve from 0.61525\n","238/238 [==============================] - 17s 71ms/step - loss: 0.4799 - acc: 0.7750 - val_loss: 0.6981 - val_acc: 0.6562\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4277 - acc: 0.8045\n","Epoch 9: val_loss did not improve from 0.61525\n","238/238 [==============================] - 17s 71ms/step - loss: 0.4277 - acc: 0.8045 - val_loss: 0.7491 - val_acc: 0.6465\n","0.6996294665786191 0.6483173582381545 0.6570837166513339 0.5798816568047338\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2848 - acc: 0.9185\n","Epoch 1: val_loss improved from inf to 0.26543, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 19s 71ms/step - loss: 0.2848 - acc: 0.9185 - val_loss: 0.2654 - val_acc: 0.9238\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2547 - acc: 0.9219\n","Epoch 2: val_loss improved from 0.26543 to 0.25779, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 69ms/step - loss: 0.2547 - acc: 0.9219 - val_loss: 0.2578 - val_acc: 0.9251\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2432 - acc: 0.9222\n","Epoch 3: val_loss improved from 0.25779 to 0.25441, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2432 - acc: 0.9222 - val_loss: 0.2544 - val_acc: 0.9251\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2323 - acc: 0.9227\n","Epoch 4: val_loss did not improve from 0.25441\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2323 - acc: 0.9227 - val_loss: 0.2612 - val_acc: 0.9261\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2213 - acc: 0.9250\n","Epoch 5: val_loss did not improve from 0.25441\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2213 - acc: 0.9250 - val_loss: 0.2644 - val_acc: 0.9233\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2055 - acc: 0.9290\n","Epoch 6: val_loss did not improve from 0.25441\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2055 - acc: 0.9290 - val_loss: 0.2682 - val_acc: 0.9205\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1825 - acc: 0.9373\n","Epoch 7: val_loss did not improve from 0.25441\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1825 - acc: 0.9373 - val_loss: 0.2872 - val_acc: 0.9104\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1524 - acc: 0.9474\n","Epoch 8: val_loss did not improve from 0.25441\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1524 - acc: 0.9474 - val_loss: 0.3110 - val_acc: 0.9136\n","0.7218066751480489 0.20826015445800816 0.9181232750689973 0.01111111111111111\n","Iteration number:  2\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2833 - acc: 0.9038\n","Epoch 1: val_loss improved from inf to 0.24297, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.2833 - acc: 0.9038 - val_loss: 0.2430 - val_acc: 0.9099\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2389 - acc: 0.9146\n","Epoch 2: val_loss improved from 0.24297 to 0.23301, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 17s 73ms/step - loss: 0.2389 - acc: 0.9146 - val_loss: 0.2330 - val_acc: 0.9182\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2209 - acc: 0.9197\n","Epoch 3: val_loss did not improve from 0.23301\n","238/238 [==============================] - 17s 72ms/step - loss: 0.2209 - acc: 0.9197 - val_loss: 0.2346 - val_acc: 0.9164\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2069 - acc: 0.9229\n","Epoch 4: val_loss improved from 0.23301 to 0.23002, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 18s 76ms/step - loss: 0.2069 - acc: 0.9229 - val_loss: 0.2300 - val_acc: 0.9154\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1918 - acc: 0.9276\n","Epoch 5: val_loss did not improve from 0.23002\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1918 - acc: 0.9276 - val_loss: 0.2411 - val_acc: 0.9090\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1698 - acc: 0.9378\n","Epoch 6: val_loss did not improve from 0.23002\n","238/238 [==============================] - 18s 75ms/step - loss: 0.1698 - acc: 0.9378 - val_loss: 0.2611 - val_acc: 0.9025\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1501 - acc: 0.9466\n","Epoch 7: val_loss did not improve from 0.23002\n","238/238 [==============================] - 18s 74ms/step - loss: 0.1501 - acc: 0.9466 - val_loss: 0.2613 - val_acc: 0.9025\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1195 - acc: 0.9575\n","Epoch 8: val_loss did not improve from 0.23002\n","238/238 [==============================] - 18s 74ms/step - loss: 0.1195 - acc: 0.9575 - val_loss: 0.3147 - val_acc: 0.8919\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0959 - acc: 0.9664\n","Epoch 9: val_loss did not improve from 0.23002\n","238/238 [==============================] - 17s 70ms/step - loss: 0.0959 - acc: 0.9664 - val_loss: 0.3212 - val_acc: 0.8970\n","0.8728450744558992 0.5745036883833349 0.9132934682612696 0.43647234678624813\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2201 - acc: 0.9325\n","Epoch 1: val_loss improved from inf to 0.18149, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 20s 73ms/step - loss: 0.2201 - acc: 0.9325 - val_loss: 0.1815 - val_acc: 0.9427\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1765 - acc: 0.9428\n","Epoch 2: val_loss improved from 0.18149 to 0.17207, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1765 - acc: 0.9428 - val_loss: 0.1721 - val_acc: 0.9436\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1593 - acc: 0.9457\n","Epoch 3: val_loss did not improve from 0.17207\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1593 - acc: 0.9457 - val_loss: 0.1746 - val_acc: 0.9427\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1477 - acc: 0.9493\n","Epoch 4: val_loss did not improve from 0.17207\n","238/238 [==============================] - 18s 75ms/step - loss: 0.1477 - acc: 0.9493 - val_loss: 0.1770 - val_acc: 0.9372\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1350 - acc: 0.9529\n","Epoch 5: val_loss improved from 0.17207 to 0.16881, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.1350 - acc: 0.9529 - val_loss: 0.1688 - val_acc: 0.9432\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1190 - acc: 0.9593\n","Epoch 6: val_loss did not improve from 0.16881\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1190 - acc: 0.9593 - val_loss: 0.1861 - val_acc: 0.9339\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0978 - acc: 0.9651\n","Epoch 7: val_loss did not improve from 0.16881\n","238/238 [==============================] - 16s 69ms/step - loss: 0.0978 - acc: 0.9651 - val_loss: 0.1959 - val_acc: 0.9404\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0763 - acc: 0.9747\n","Epoch 8: val_loss did not improve from 0.16881\n","238/238 [==============================] - 17s 70ms/step - loss: 0.0763 - acc: 0.9747 - val_loss: 0.2094 - val_acc: 0.9325\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0572 - acc: 0.9806\n","Epoch 9: val_loss did not improve from 0.16881\n","238/238 [==============================] - 16s 69ms/step - loss: 0.0572 - acc: 0.9806 - val_loss: 0.2293 - val_acc: 0.9367\n","Epoch 10/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0406 - acc: 0.9879\n","Epoch 10: val_loss did not improve from 0.16881\n","238/238 [==============================] - 17s 70ms/step - loss: 0.0406 - acc: 0.9879 - val_loss: 0.2664 - val_acc: 0.9325\n","0.8702030369711441 0.5158136890022036 0.9434222631094756 0.4557522123893805\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6924 - acc: 0.6174\n","Epoch 1: val_loss improved from inf to 0.63297, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 19s 68ms/step - loss: 0.6924 - acc: 0.6174 - val_loss: 0.6330 - val_acc: 0.6613\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6311 - acc: 0.6671\n","Epoch 2: val_loss improved from 0.63297 to 0.62735, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.6311 - acc: 0.6671 - val_loss: 0.6274 - val_acc: 0.6604\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6109 - acc: 0.6791\n","Epoch 3: val_loss improved from 0.62735 to 0.62681, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 17s 73ms/step - loss: 0.6109 - acc: 0.6791 - val_loss: 0.6268 - val_acc: 0.6604\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5950 - acc: 0.6937\n","Epoch 4: val_loss improved from 0.62681 to 0.61742, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 17s 70ms/step - loss: 0.5950 - acc: 0.6937 - val_loss: 0.6174 - val_acc: 0.6714\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5772 - acc: 0.7069\n","Epoch 5: val_loss did not improve from 0.61742\n","238/238 [==============================] - 17s 70ms/step - loss: 0.5772 - acc: 0.7069 - val_loss: 0.6174 - val_acc: 0.6756\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5541 - acc: 0.7271\n","Epoch 6: val_loss did not improve from 0.61742\n","238/238 [==============================] - 16s 68ms/step - loss: 0.5541 - acc: 0.7271 - val_loss: 0.6307 - val_acc: 0.6682\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5273 - acc: 0.7438\n","Epoch 7: val_loss did not improve from 0.61742\n","238/238 [==============================] - 17s 71ms/step - loss: 0.5273 - acc: 0.7438 - val_loss: 0.6515 - val_acc: 0.6585\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4846 - acc: 0.7715\n","Epoch 8: val_loss did not improve from 0.61742\n","238/238 [==============================] - 17s 70ms/step - loss: 0.4846 - acc: 0.7715 - val_loss: 0.6750 - val_acc: 0.6604\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4309 - acc: 0.8016\n","Epoch 9: val_loss did not improve from 0.61742\n","238/238 [==============================] - 16s 69ms/step - loss: 0.4309 - acc: 0.8016 - val_loss: 0.7367 - val_acc: 0.6303\n","0.7023921741269697 0.6406244147067383 0.6690432382704692 0.5606106870229007\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.3004 - acc: 0.9144\n","Epoch 1: val_loss improved from inf to 0.27232, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 20s 76ms/step - loss: 0.3004 - acc: 0.9144 - val_loss: 0.2723 - val_acc: 0.9238\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2601 - acc: 0.9216\n","Epoch 2: val_loss improved from 0.27232 to 0.26660, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 17s 72ms/step - loss: 0.2601 - acc: 0.9216 - val_loss: 0.2666 - val_acc: 0.9256\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2442 - acc: 0.9214\n","Epoch 3: val_loss did not improve from 0.26660\n","238/238 [==============================] - 17s 70ms/step - loss: 0.2442 - acc: 0.9214 - val_loss: 0.2707 - val_acc: 0.9214\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2343 - acc: 0.9225\n","Epoch 4: val_loss improved from 0.26660 to 0.26570, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.2343 - acc: 0.9225 - val_loss: 0.2657 - val_acc: 0.9242\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2202 - acc: 0.9254\n","Epoch 5: val_loss did not improve from 0.26570\n","238/238 [==============================] - 17s 73ms/step - loss: 0.2202 - acc: 0.9254 - val_loss: 0.2702 - val_acc: 0.9233\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2055 - acc: 0.9292\n","Epoch 6: val_loss did not improve from 0.26570\n","238/238 [==============================] - 17s 72ms/step - loss: 0.2055 - acc: 0.9292 - val_loss: 0.2831 - val_acc: 0.9219\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1853 - acc: 0.9351\n","Epoch 7: val_loss did not improve from 0.26570\n","238/238 [==============================] - 17s 73ms/step - loss: 0.1853 - acc: 0.9351 - val_loss: 0.2880 - val_acc: 0.9182\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1570 - acc: 0.9452\n","Epoch 8: val_loss did not improve from 0.26570\n","238/238 [==============================] - 18s 74ms/step - loss: 0.1570 - acc: 0.9452 - val_loss: 0.3162 - val_acc: 0.9168\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1250 - acc: 0.9576\n","Epoch 9: val_loss did not improve from 0.26570\n","238/238 [==============================] - 18s 76ms/step - loss: 0.1250 - acc: 0.9576 - val_loss: 0.3601 - val_acc: 0.9154\n","0.7162386570894599 0.18825592643560712 0.9160533578656854 0.04699738903394256\n","Iteration number:  3\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2857 - acc: 0.9022\n","Epoch 1: val_loss improved from inf to 0.24395, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.2857 - acc: 0.9022 - val_loss: 0.2439 - val_acc: 0.9168\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2385 - acc: 0.9148\n","Epoch 2: val_loss improved from 0.24395 to 0.23519, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2385 - acc: 0.9148 - val_loss: 0.2352 - val_acc: 0.9196\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2194 - acc: 0.9213\n","Epoch 3: val_loss did not improve from 0.23519\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2194 - acc: 0.9213 - val_loss: 0.2355 - val_acc: 0.9145\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2068 - acc: 0.9221\n","Epoch 4: val_loss improved from 0.23519 to 0.23396, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.2068 - acc: 0.9221 - val_loss: 0.2340 - val_acc: 0.9168\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1917 - acc: 0.9294\n","Epoch 5: val_loss did not improve from 0.23396\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1917 - acc: 0.9294 - val_loss: 0.2372 - val_acc: 0.9108\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1737 - acc: 0.9357\n","Epoch 6: val_loss did not improve from 0.23396\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1737 - acc: 0.9357 - val_loss: 0.2487 - val_acc: 0.9131\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1471 - acc: 0.9454\n","Epoch 7: val_loss did not improve from 0.23396\n","238/238 [==============================] - 17s 69ms/step - loss: 0.1471 - acc: 0.9454 - val_loss: 0.2671 - val_acc: 0.9094\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1250 - acc: 0.9532\n","Epoch 8: val_loss did not improve from 0.23396\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1250 - acc: 0.9532 - val_loss: 0.2893 - val_acc: 0.9099\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0923 - acc: 0.9667\n","Epoch 9: val_loss did not improve from 0.23396\n","238/238 [==============================] - 16s 69ms/step - loss: 0.0923 - acc: 0.9667 - val_loss: 0.3439 - val_acc: 0.9048\n","0.8640651158692396 0.5539353107857704 0.9128334866605335 0.4178187403993856\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2155 - acc: 0.9353\n","Epoch 1: val_loss improved from inf to 0.18085, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 20s 72ms/step - loss: 0.2155 - acc: 0.9353 - val_loss: 0.1809 - val_acc: 0.9432\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1778 - acc: 0.9418\n","Epoch 2: val_loss improved from 0.18085 to 0.17021, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.1778 - acc: 0.9418 - val_loss: 0.1702 - val_acc: 0.9413\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1595 - acc: 0.9468\n","Epoch 3: val_loss improved from 0.17021 to 0.16965, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1595 - acc: 0.9468 - val_loss: 0.1697 - val_acc: 0.9422\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1475 - acc: 0.9493\n","Epoch 4: val_loss did not improve from 0.16965\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1475 - acc: 0.9493 - val_loss: 0.1730 - val_acc: 0.9422\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1342 - acc: 0.9530\n","Epoch 5: val_loss did not improve from 0.16965\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1342 - acc: 0.9530 - val_loss: 0.1796 - val_acc: 0.9409\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1191 - acc: 0.9562\n","Epoch 6: val_loss did not improve from 0.16965\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1191 - acc: 0.9562 - val_loss: 0.1871 - val_acc: 0.9395\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1033 - acc: 0.9641\n","Epoch 7: val_loss did not improve from 0.16965\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1033 - acc: 0.9641 - val_loss: 0.1907 - val_acc: 0.9376\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0797 - acc: 0.9728\n","Epoch 8: val_loss did not improve from 0.16965\n","238/238 [==============================] - 17s 70ms/step - loss: 0.0797 - acc: 0.9728 - val_loss: 0.2273 - val_acc: 0.9362\n","0.8800092076720196 0.5077511267423016 0.9411223551057958 0.4576271186440678\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6819 - acc: 0.6226\n","Epoch 1: val_loss improved from inf to 0.63680, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 20s 71ms/step - loss: 0.6819 - acc: 0.6226 - val_loss: 0.6368 - val_acc: 0.6617\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6278 - acc: 0.6646\n","Epoch 2: val_loss improved from 0.63680 to 0.63075, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 18s 74ms/step - loss: 0.6278 - acc: 0.6646 - val_loss: 0.6307 - val_acc: 0.6659\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6080 - acc: 0.6765\n","Epoch 3: val_loss improved from 0.63075 to 0.62290, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.6080 - acc: 0.6765 - val_loss: 0.6229 - val_acc: 0.6682\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5953 - acc: 0.6939\n","Epoch 4: val_loss improved from 0.62290 to 0.61919, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 17s 72ms/step - loss: 0.5953 - acc: 0.6939 - val_loss: 0.6192 - val_acc: 0.6617\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5769 - acc: 0.7051\n","Epoch 5: val_loss did not improve from 0.61919\n","238/238 [==============================] - 16s 68ms/step - loss: 0.5769 - acc: 0.7051 - val_loss: 0.6223 - val_acc: 0.6798\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5541 - acc: 0.7253\n","Epoch 6: val_loss did not improve from 0.61919\n","238/238 [==============================] - 16s 69ms/step - loss: 0.5541 - acc: 0.7253 - val_loss: 0.6359 - val_acc: 0.6654\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5236 - acc: 0.7497\n","Epoch 7: val_loss did not improve from 0.61919\n","238/238 [==============================] - 17s 71ms/step - loss: 0.5236 - acc: 0.7497 - val_loss: 0.6490 - val_acc: 0.6645\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4820 - acc: 0.7751\n","Epoch 8: val_loss did not improve from 0.61919\n","238/238 [==============================] - 16s 68ms/step - loss: 0.4820 - acc: 0.7751 - val_loss: 0.7028 - val_acc: 0.6506\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4280 - acc: 0.8077\n","Epoch 9: val_loss did not improve from 0.61919\n","238/238 [==============================] - 17s 70ms/step - loss: 0.4280 - acc: 0.8077 - val_loss: 0.7173 - val_acc: 0.6617\n","0.6998984647788236 0.6429044138806429 0.656163753449862 0.563248612328367\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2895 - acc: 0.9177\n","Epoch 1: val_loss improved from inf to 0.26249, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.2895 - acc: 0.9177 - val_loss: 0.2625 - val_acc: 0.9247\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2571 - acc: 0.9214\n","Epoch 2: val_loss did not improve from 0.26249\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2571 - acc: 0.9214 - val_loss: 0.2695 - val_acc: 0.9256\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2449 - acc: 0.9223\n","Epoch 3: val_loss improved from 0.26249 to 0.25591, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 15s 65ms/step - loss: 0.2449 - acc: 0.9223 - val_loss: 0.2559 - val_acc: 0.9256\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2342 - acc: 0.9230\n","Epoch 4: val_loss did not improve from 0.25591\n","238/238 [==============================] - 16s 69ms/step - loss: 0.2342 - acc: 0.9230 - val_loss: 0.2595 - val_acc: 0.9247\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2212 - acc: 0.9240\n","Epoch 5: val_loss did not improve from 0.25591\n","238/238 [==============================] - 17s 70ms/step - loss: 0.2212 - acc: 0.9240 - val_loss: 0.2631 - val_acc: 0.9247\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2042 - acc: 0.9282\n","Epoch 6: val_loss did not improve from 0.25591\n","238/238 [==============================] - 17s 70ms/step - loss: 0.2042 - acc: 0.9282 - val_loss: 0.2717 - val_acc: 0.9205\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1814 - acc: 0.9363\n","Epoch 7: val_loss did not improve from 0.25591\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1814 - acc: 0.9363 - val_loss: 0.3038 - val_acc: 0.9043\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1545 - acc: 0.9470\n","Epoch 8: val_loss did not improve from 0.25591\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1545 - acc: 0.9470 - val_loss: 0.3068 - val_acc: 0.9034\n","0.727251905496386 0.21581865578437004 0.9188132474701012 0.027548209366391185\n","Iteration number:  4\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2913 - acc: 0.9028\n","Epoch 1: val_loss improved from inf to 0.24572, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.2913 - acc: 0.9028 - val_loss: 0.2457 - val_acc: 0.9154\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2422 - acc: 0.9139\n","Epoch 2: val_loss improved from 0.24572 to 0.23970, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 17s 70ms/step - loss: 0.2422 - acc: 0.9139 - val_loss: 0.2397 - val_acc: 0.9187\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2231 - acc: 0.9205\n","Epoch 3: val_loss improved from 0.23970 to 0.23096, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2231 - acc: 0.9205 - val_loss: 0.2310 - val_acc: 0.9177\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2108 - acc: 0.9220\n","Epoch 4: val_loss did not improve from 0.23096\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2108 - acc: 0.9220 - val_loss: 0.2324 - val_acc: 0.9145\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1942 - acc: 0.9288\n","Epoch 5: val_loss did not improve from 0.23096\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1942 - acc: 0.9288 - val_loss: 0.2427 - val_acc: 0.9117\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1731 - acc: 0.9361\n","Epoch 6: val_loss did not improve from 0.23096\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1731 - acc: 0.9361 - val_loss: 0.2555 - val_acc: 0.9140\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1537 - acc: 0.9441\n","Epoch 7: val_loss did not improve from 0.23096\n","238/238 [==============================] - 17s 71ms/step - loss: 0.1537 - acc: 0.9441 - val_loss: 0.2687 - val_acc: 0.9034\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1258 - acc: 0.9554\n","Epoch 8: val_loss did not improve from 0.23096\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1258 - acc: 0.9554 - val_loss: 0.2830 - val_acc: 0.9090\n","0.8787417944312275 0.5794382127651745 0.9146734130634775 0.40449438202247184\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2121 - acc: 0.9336\n","Epoch 1: val_loss improved from inf to 0.18639, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 20s 72ms/step - loss: 0.2121 - acc: 0.9336 - val_loss: 0.1864 - val_acc: 0.9418\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1750 - acc: 0.9416\n","Epoch 2: val_loss improved from 0.18639 to 0.17364, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1750 - acc: 0.9416 - val_loss: 0.1736 - val_acc: 0.9455\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1604 - acc: 0.9458\n","Epoch 3: val_loss improved from 0.17364 to 0.16898, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.1604 - acc: 0.9458 - val_loss: 0.1690 - val_acc: 0.9409\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1502 - acc: 0.9487\n","Epoch 4: val_loss improved from 0.16898 to 0.16769, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.1502 - acc: 0.9487 - val_loss: 0.1677 - val_acc: 0.9409\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1377 - acc: 0.9522\n","Epoch 5: val_loss did not improve from 0.16769\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1377 - acc: 0.9522 - val_loss: 0.1701 - val_acc: 0.9399\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1201 - acc: 0.9565\n","Epoch 6: val_loss did not improve from 0.16769\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1201 - acc: 0.9565 - val_loss: 0.1793 - val_acc: 0.9372\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1028 - acc: 0.9628\n","Epoch 7: val_loss did not improve from 0.16769\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1028 - acc: 0.9628 - val_loss: 0.1895 - val_acc: 0.9358\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0796 - acc: 0.9733\n","Epoch 8: val_loss did not improve from 0.16769\n","238/238 [==============================] - 16s 67ms/step - loss: 0.0796 - acc: 0.9733 - val_loss: 0.2019 - val_acc: 0.9399\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0588 - acc: 0.9810\n","Epoch 9: val_loss did not improve from 0.16769\n","238/238 [==============================] - 15s 64ms/step - loss: 0.0588 - acc: 0.9810 - val_loss: 0.2440 - val_acc: 0.9261\n","0.8817820746913868 0.5212287281180977 0.9402023919043239 0.47368421052631576\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6794 - acc: 0.6202\n","Epoch 1: val_loss improved from inf to 0.63310, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 20s 72ms/step - loss: 0.6794 - acc: 0.6202 - val_loss: 0.6331 - val_acc: 0.6567\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6301 - acc: 0.6592\n","Epoch 2: val_loss improved from 0.63310 to 0.61645, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.6301 - acc: 0.6592 - val_loss: 0.6164 - val_acc: 0.6802\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6107 - acc: 0.6751\n","Epoch 3: val_loss improved from 0.61645 to 0.60977, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.6107 - acc: 0.6751 - val_loss: 0.6098 - val_acc: 0.6779\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5934 - acc: 0.6910\n","Epoch 4: val_loss did not improve from 0.60977\n","238/238 [==============================] - 16s 69ms/step - loss: 0.5934 - acc: 0.6910 - val_loss: 0.6132 - val_acc: 0.6677\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5775 - acc: 0.7077\n","Epoch 5: val_loss did not improve from 0.60977\n","238/238 [==============================] - 17s 70ms/step - loss: 0.5775 - acc: 0.7077 - val_loss: 0.6237 - val_acc: 0.6645\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5563 - acc: 0.7210\n","Epoch 6: val_loss did not improve from 0.60977\n","238/238 [==============================] - 16s 68ms/step - loss: 0.5563 - acc: 0.7210 - val_loss: 0.6310 - val_acc: 0.6585\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5286 - acc: 0.7424\n","Epoch 7: val_loss did not improve from 0.60977\n","238/238 [==============================] - 16s 69ms/step - loss: 0.5286 - acc: 0.7424 - val_loss: 0.6454 - val_acc: 0.6571\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4837 - acc: 0.7723\n","Epoch 8: val_loss did not improve from 0.60977\n","238/238 [==============================] - 16s 69ms/step - loss: 0.4837 - acc: 0.7723 - val_loss: 0.6969 - val_acc: 0.6414\n","0.7052134721376995 0.6467965949910232 0.670883164673413 0.5595567867036011\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2942 - acc: 0.9171\n","Epoch 1: val_loss improved from inf to 0.26942, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 20s 72ms/step - loss: 0.2942 - acc: 0.9171 - val_loss: 0.2694 - val_acc: 0.9247\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2568 - acc: 0.9221\n","Epoch 2: val_loss improved from 0.26942 to 0.25958, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2568 - acc: 0.9221 - val_loss: 0.2596 - val_acc: 0.9251\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2439 - acc: 0.9221\n","Epoch 3: val_loss improved from 0.25958 to 0.25717, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2439 - acc: 0.9221 - val_loss: 0.2572 - val_acc: 0.9256\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2333 - acc: 0.9240\n","Epoch 4: val_loss did not improve from 0.25717\n","238/238 [==============================] - 16s 69ms/step - loss: 0.2333 - acc: 0.9240 - val_loss: 0.2627 - val_acc: 0.9219\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2223 - acc: 0.9242\n","Epoch 5: val_loss did not improve from 0.25717\n","238/238 [==============================] - 17s 70ms/step - loss: 0.2223 - acc: 0.9242 - val_loss: 0.2632 - val_acc: 0.9238\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2078 - acc: 0.9289\n","Epoch 6: val_loss did not improve from 0.25717\n","238/238 [==============================] - 17s 71ms/step - loss: 0.2078 - acc: 0.9289 - val_loss: 0.2725 - val_acc: 0.9214\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1827 - acc: 0.9363\n","Epoch 7: val_loss did not improve from 0.25717\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1827 - acc: 0.9363 - val_loss: 0.2910 - val_acc: 0.9150\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1573 - acc: 0.9457\n","Epoch 8: val_loss did not improve from 0.25717\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1573 - acc: 0.9457 - val_loss: 0.3233 - val_acc: 0.9002\n","0.7291289038751662 0.21384428383260068 0.9185832566697332 0.027472527472527476\n","Iteration number:  5\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2891 - acc: 0.9030\n","Epoch 1: val_loss improved from inf to 0.25267, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 19s 69ms/step - loss: 0.2891 - acc: 0.9030 - val_loss: 0.2527 - val_acc: 0.9150\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2433 - acc: 0.9146\n","Epoch 2: val_loss improved from 0.25267 to 0.23308, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2433 - acc: 0.9146 - val_loss: 0.2331 - val_acc: 0.9177\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2266 - acc: 0.9177\n","Epoch 3: val_loss did not improve from 0.23308\n","238/238 [==============================] - 17s 72ms/step - loss: 0.2266 - acc: 0.9177 - val_loss: 0.2373 - val_acc: 0.9159\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2113 - acc: 0.9215\n","Epoch 4: val_loss did not improve from 0.23308\n","238/238 [==============================] - 17s 70ms/step - loss: 0.2113 - acc: 0.9215 - val_loss: 0.2355 - val_acc: 0.9164\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1966 - acc: 0.9268\n","Epoch 5: val_loss did not improve from 0.23308\n","238/238 [==============================] - 17s 72ms/step - loss: 0.1966 - acc: 0.9268 - val_loss: 0.2391 - val_acc: 0.9131\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1792 - acc: 0.9343\n","Epoch 6: val_loss did not improve from 0.23308\n","238/238 [==============================] - 17s 72ms/step - loss: 0.1792 - acc: 0.9343 - val_loss: 0.2440 - val_acc: 0.9113\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1561 - acc: 0.9428\n","Epoch 7: val_loss did not improve from 0.23308\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1561 - acc: 0.9428 - val_loss: 0.2630 - val_acc: 0.9043\n","0.8749834787205922 0.5656316193210817 0.9158233670653174 0.43343653250773995\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2146 - acc: 0.9332\n","Epoch 1: val_loss improved from inf to 0.16797, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 20s 73ms/step - loss: 0.2146 - acc: 0.9332 - val_loss: 0.1680 - val_acc: 0.9445\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1735 - acc: 0.9427\n","Epoch 2: val_loss improved from 0.16797 to 0.16792, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1735 - acc: 0.9427 - val_loss: 0.1679 - val_acc: 0.9376\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1592 - acc: 0.9463\n","Epoch 3: val_loss did not improve from 0.16792\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1592 - acc: 0.9463 - val_loss: 0.1717 - val_acc: 0.9427\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1486 - acc: 0.9487\n","Epoch 4: val_loss did not improve from 0.16792\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1486 - acc: 0.9487 - val_loss: 0.1750 - val_acc: 0.9362\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1353 - acc: 0.9526\n","Epoch 5: val_loss did not improve from 0.16792\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1353 - acc: 0.9526 - val_loss: 0.1691 - val_acc: 0.9413\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1183 - acc: 0.9587\n","Epoch 6: val_loss did not improve from 0.16792\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1183 - acc: 0.9587 - val_loss: 0.1842 - val_acc: 0.9367\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1005 - acc: 0.9647\n","Epoch 7: val_loss did not improve from 0.16792\n","238/238 [==============================] - 17s 71ms/step - loss: 0.1005 - acc: 0.9647 - val_loss: 0.1893 - val_acc: 0.9330\n","0.8840910154969802 0.5147733642198871 0.9408923643054278 0.39243498817966904\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6802 - acc: 0.6230\n","Epoch 1: val_loss improved from inf to 0.63339, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 20s 72ms/step - loss: 0.6802 - acc: 0.6230 - val_loss: 0.6334 - val_acc: 0.6622\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6255 - acc: 0.6654\n","Epoch 2: val_loss improved from 0.63339 to 0.61813, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.6255 - acc: 0.6654 - val_loss: 0.6181 - val_acc: 0.6677\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6085 - acc: 0.6803\n","Epoch 3: val_loss improved from 0.61813 to 0.61659, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 69ms/step - loss: 0.6085 - acc: 0.6803 - val_loss: 0.6166 - val_acc: 0.6622\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5938 - acc: 0.6932\n","Epoch 4: val_loss improved from 0.61659 to 0.61481, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 17s 70ms/step - loss: 0.5938 - acc: 0.6932 - val_loss: 0.6148 - val_acc: 0.6696\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5760 - acc: 0.7085\n","Epoch 5: val_loss did not improve from 0.61481\n","238/238 [==============================] - 17s 70ms/step - loss: 0.5760 - acc: 0.7085 - val_loss: 0.6191 - val_acc: 0.6553\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5528 - acc: 0.7248\n","Epoch 6: val_loss did not improve from 0.61481\n","238/238 [==============================] - 17s 71ms/step - loss: 0.5528 - acc: 0.7248 - val_loss: 0.6328 - val_acc: 0.6580\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5217 - acc: 0.7479\n","Epoch 7: val_loss did not improve from 0.61481\n","238/238 [==============================] - 17s 70ms/step - loss: 0.5217 - acc: 0.7479 - val_loss: 0.6527 - val_acc: 0.6520\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4803 - acc: 0.7751\n","Epoch 8: val_loss did not improve from 0.61481\n","238/238 [==============================] - 16s 69ms/step - loss: 0.4803 - acc: 0.7751 - val_loss: 0.6781 - val_acc: 0.6456\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4218 - acc: 0.8091\n","Epoch 9: val_loss did not improve from 0.61481\n","238/238 [==============================] - 15s 65ms/step - loss: 0.4218 - acc: 0.8091 - val_loss: 0.7378 - val_acc: 0.6525\n","0.7099978203916388 0.6549249405733901 0.6667433302667893 0.5583663517220361\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.3017 - acc: 0.9125\n","Epoch 1: val_loss improved from inf to 0.26898, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 19s 68ms/step - loss: 0.3017 - acc: 0.9125 - val_loss: 0.2690 - val_acc: 0.9251\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2580 - acc: 0.9218\n","Epoch 2: val_loss improved from 0.26898 to 0.26076, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2580 - acc: 0.9218 - val_loss: 0.2608 - val_acc: 0.9247\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2438 - acc: 0.9217\n","Epoch 3: val_loss did not improve from 0.26076\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2438 - acc: 0.9217 - val_loss: 0.2632 - val_acc: 0.9247\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2341 - acc: 0.9231\n","Epoch 4: val_loss did not improve from 0.26076\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2341 - acc: 0.9231 - val_loss: 0.2829 - val_acc: 0.9085\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2229 - acc: 0.9248\n","Epoch 5: val_loss did not improve from 0.26076\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2229 - acc: 0.9248 - val_loss: 0.2834 - val_acc: 0.9214\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2079 - acc: 0.9284\n","Epoch 6: val_loss did not improve from 0.26076\n","238/238 [==============================] - 16s 69ms/step - loss: 0.2079 - acc: 0.9284 - val_loss: 0.2741 - val_acc: 0.9187\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1851 - acc: 0.9356\n","Epoch 7: val_loss did not improve from 0.26076\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1851 - acc: 0.9356 - val_loss: 0.2987 - val_acc: 0.9131\n","0.7299356437594289 0.21211889252636928 0.9181232750689973 0.0\n","Iteration number:  6\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2926 - acc: 0.8987\n","Epoch 1: val_loss improved from inf to 0.24884, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.2926 - acc: 0.8987 - val_loss: 0.2488 - val_acc: 0.9140\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2418 - acc: 0.9135\n","Epoch 2: val_loss improved from 0.24884 to 0.23906, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 17s 70ms/step - loss: 0.2418 - acc: 0.9135 - val_loss: 0.2391 - val_acc: 0.9140\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2228 - acc: 0.9190\n","Epoch 3: val_loss improved from 0.23906 to 0.23373, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2228 - acc: 0.9190 - val_loss: 0.2337 - val_acc: 0.9145\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2092 - acc: 0.9227\n","Epoch 4: val_loss did not improve from 0.23373\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2092 - acc: 0.9227 - val_loss: 0.2557 - val_acc: 0.8974\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1926 - acc: 0.9285\n","Epoch 5: val_loss did not improve from 0.23373\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1926 - acc: 0.9285 - val_loss: 0.2508 - val_acc: 0.9034\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1760 - acc: 0.9333\n","Epoch 6: val_loss did not improve from 0.23373\n","238/238 [==============================] - 17s 71ms/step - loss: 0.1760 - acc: 0.9333 - val_loss: 0.2448 - val_acc: 0.9094\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1535 - acc: 0.9450\n","Epoch 7: val_loss did not improve from 0.23373\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1535 - acc: 0.9450 - val_loss: 0.2725 - val_acc: 0.9085\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1241 - acc: 0.9551\n","Epoch 8: val_loss did not improve from 0.23373\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1241 - acc: 0.9551 - val_loss: 0.3061 - val_acc: 0.8928\n","0.869599193761565 0.5618285960586286 0.9162833486660533 0.4518072289156626\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2350 - acc: 0.9242\n","Epoch 1: val_loss improved from inf to 0.18858, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.2350 - acc: 0.9242 - val_loss: 0.1886 - val_acc: 0.9427\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1793 - acc: 0.9431\n","Epoch 2: val_loss improved from 0.18858 to 0.17635, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1793 - acc: 0.9431 - val_loss: 0.1764 - val_acc: 0.9395\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1627 - acc: 0.9461\n","Epoch 3: val_loss improved from 0.17635 to 0.17387, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1627 - acc: 0.9461 - val_loss: 0.1739 - val_acc: 0.9445\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1496 - acc: 0.9502\n","Epoch 4: val_loss improved from 0.17387 to 0.17264, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1496 - acc: 0.9502 - val_loss: 0.1726 - val_acc: 0.9390\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1362 - acc: 0.9539\n","Epoch 5: val_loss did not improve from 0.17264\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1362 - acc: 0.9539 - val_loss: 0.1769 - val_acc: 0.9372\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1203 - acc: 0.9585\n","Epoch 6: val_loss did not improve from 0.17264\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1203 - acc: 0.9585 - val_loss: 0.1861 - val_acc: 0.9358\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1022 - acc: 0.9639\n","Epoch 7: val_loss did not improve from 0.17264\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1022 - acc: 0.9639 - val_loss: 0.2021 - val_acc: 0.9344\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0783 - acc: 0.9734\n","Epoch 8: val_loss did not improve from 0.17264\n","238/238 [==============================] - 17s 71ms/step - loss: 0.0783 - acc: 0.9734 - val_loss: 0.2275 - val_acc: 0.9330\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0600 - acc: 0.9800\n","Epoch 9: val_loss did not improve from 0.17264\n","238/238 [==============================] - 16s 69ms/step - loss: 0.0600 - acc: 0.9800 - val_loss: 0.2650 - val_acc: 0.9307\n","0.8837383148399582 0.5250409431480848 0.93721251149954 0.4739884393063584\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6850 - acc: 0.6232\n","Epoch 1: val_loss improved from inf to 0.62932, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 20s 73ms/step - loss: 0.6850 - acc: 0.6232 - val_loss: 0.6293 - val_acc: 0.6636\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6292 - acc: 0.6621\n","Epoch 2: val_loss improved from 0.62932 to 0.61463, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 69ms/step - loss: 0.6292 - acc: 0.6621 - val_loss: 0.6146 - val_acc: 0.6701\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6092 - acc: 0.6777\n","Epoch 3: val_loss improved from 0.61463 to 0.61427, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 69ms/step - loss: 0.6092 - acc: 0.6777 - val_loss: 0.6143 - val_acc: 0.6701\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5914 - acc: 0.6975\n","Epoch 4: val_loss did not improve from 0.61427\n","238/238 [==============================] - 16s 67ms/step - loss: 0.5914 - acc: 0.6975 - val_loss: 0.6161 - val_acc: 0.6645\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5743 - acc: 0.7081\n","Epoch 5: val_loss did not improve from 0.61427\n","238/238 [==============================] - 17s 71ms/step - loss: 0.5743 - acc: 0.7081 - val_loss: 0.6174 - val_acc: 0.6673\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5557 - acc: 0.7232\n","Epoch 6: val_loss did not improve from 0.61427\n","238/238 [==============================] - 17s 70ms/step - loss: 0.5557 - acc: 0.7232 - val_loss: 0.6178 - val_acc: 0.6682\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5211 - acc: 0.7454\n","Epoch 7: val_loss did not improve from 0.61427\n","238/238 [==============================] - 17s 70ms/step - loss: 0.5211 - acc: 0.7454 - val_loss: 0.6398 - val_acc: 0.6654\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4776 - acc: 0.7752\n","Epoch 8: val_loss did not improve from 0.61427\n","238/238 [==============================] - 17s 71ms/step - loss: 0.4776 - acc: 0.7752 - val_loss: 0.6723 - val_acc: 0.6562\n","0.7059969442322385 0.6478819283742651 0.6672033118675254 0.5661169415292353\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2886 - acc: 0.9181\n","Epoch 1: val_loss improved from inf to 0.26281, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 20s 72ms/step - loss: 0.2886 - acc: 0.9181 - val_loss: 0.2628 - val_acc: 0.9247\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2563 - acc: 0.9215\n","Epoch 2: val_loss improved from 0.26281 to 0.26060, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 18s 74ms/step - loss: 0.2563 - acc: 0.9215 - val_loss: 0.2606 - val_acc: 0.9247\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2444 - acc: 0.9223\n","Epoch 3: val_loss did not improve from 0.26060\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2444 - acc: 0.9223 - val_loss: 0.2667 - val_acc: 0.9242\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2329 - acc: 0.9235\n","Epoch 4: val_loss improved from 0.26060 to 0.25551, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2329 - acc: 0.9235 - val_loss: 0.2555 - val_acc: 0.9247\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2216 - acc: 0.9256\n","Epoch 5: val_loss did not improve from 0.25551\n","238/238 [==============================] - 16s 69ms/step - loss: 0.2216 - acc: 0.9256 - val_loss: 0.2749 - val_acc: 0.9247\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2034 - acc: 0.9292\n","Epoch 6: val_loss did not improve from 0.25551\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2034 - acc: 0.9292 - val_loss: 0.2895 - val_acc: 0.9247\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1835 - acc: 0.9362\n","Epoch 7: val_loss did not improve from 0.25551\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1835 - acc: 0.9362 - val_loss: 0.2941 - val_acc: 0.9140\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1519 - acc: 0.9470\n","Epoch 8: val_loss did not improve from 0.25551\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1519 - acc: 0.9470 - val_loss: 0.3364 - val_acc: 0.9006\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1170 - acc: 0.9602\n","Epoch 9: val_loss did not improve from 0.25551\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1170 - acc: 0.9602 - val_loss: 0.3981 - val_acc: 0.9020\n","0.7335703710792372 0.19950466354549967 0.9153633854645814 0.05641025641025641\n","Iteration number:  7\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2900 - acc: 0.9014\n","Epoch 1: val_loss improved from inf to 0.24743, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 20s 74ms/step - loss: 0.2900 - acc: 0.9014 - val_loss: 0.2474 - val_acc: 0.9159\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2409 - acc: 0.9158\n","Epoch 2: val_loss improved from 0.24743 to 0.23979, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.2409 - acc: 0.9158 - val_loss: 0.2398 - val_acc: 0.9071\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2254 - acc: 0.9183\n","Epoch 3: val_loss improved from 0.23979 to 0.23318, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.2254 - acc: 0.9183 - val_loss: 0.2332 - val_acc: 0.9154\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2094 - acc: 0.9236\n","Epoch 4: val_loss did not improve from 0.23318\n","238/238 [==============================] - 17s 70ms/step - loss: 0.2094 - acc: 0.9236 - val_loss: 0.2359 - val_acc: 0.9182\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1927 - acc: 0.9290\n","Epoch 5: val_loss did not improve from 0.23318\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1927 - acc: 0.9290 - val_loss: 0.2439 - val_acc: 0.9113\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1734 - acc: 0.9352\n","Epoch 6: val_loss did not improve from 0.23318\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1734 - acc: 0.9352 - val_loss: 0.2411 - val_acc: 0.9196\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1479 - acc: 0.9455\n","Epoch 7: val_loss did not improve from 0.23318\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1479 - acc: 0.9455 - val_loss: 0.2736 - val_acc: 0.9053\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1207 - acc: 0.9575\n","Epoch 8: val_loss did not improve from 0.23318\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1207 - acc: 0.9575 - val_loss: 0.2981 - val_acc: 0.9006\n","0.8634026125649836 0.551742558387687 0.9128334866605335 0.3955342902711324\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2212 - acc: 0.9336\n","Epoch 1: val_loss improved from inf to 0.17469, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.2212 - acc: 0.9336 - val_loss: 0.1747 - val_acc: 0.9432\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1756 - acc: 0.9434\n","Epoch 2: val_loss did not improve from 0.17469\n","238/238 [==============================] - 17s 73ms/step - loss: 0.1756 - acc: 0.9434 - val_loss: 0.1762 - val_acc: 0.9409\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1610 - acc: 0.9449\n","Epoch 3: val_loss did not improve from 0.17469\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1610 - acc: 0.9449 - val_loss: 0.1891 - val_acc: 0.9279\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1507 - acc: 0.9489\n","Epoch 4: val_loss did not improve from 0.17469\n","238/238 [==============================] - 17s 71ms/step - loss: 0.1507 - acc: 0.9489 - val_loss: 0.1748 - val_acc: 0.9413\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1355 - acc: 0.9524\n","Epoch 5: val_loss did not improve from 0.17469\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1355 - acc: 0.9524 - val_loss: 0.1876 - val_acc: 0.9395\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1221 - acc: 0.9572\n","Epoch 6: val_loss did not improve from 0.17469\n","238/238 [==============================] - 17s 71ms/step - loss: 0.1221 - acc: 0.9572 - val_loss: 0.1811 - val_acc: 0.9372\n","0.8674629742341244 0.4853537134758902 0.937442502299908 0.3365853658536585\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6811 - acc: 0.6262\n","Epoch 1: val_loss improved from inf to 0.63326, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.6811 - acc: 0.6262 - val_loss: 0.6333 - val_acc: 0.6594\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6299 - acc: 0.6619\n","Epoch 2: val_loss improved from 0.63326 to 0.62559, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 17s 72ms/step - loss: 0.6299 - acc: 0.6619 - val_loss: 0.6256 - val_acc: 0.6636\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6076 - acc: 0.6803\n","Epoch 3: val_loss did not improve from 0.62559\n","238/238 [==============================] - 16s 69ms/step - loss: 0.6076 - acc: 0.6803 - val_loss: 0.6268 - val_acc: 0.6733\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5921 - acc: 0.6953\n","Epoch 4: val_loss improved from 0.62559 to 0.62404, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 69ms/step - loss: 0.5921 - acc: 0.6953 - val_loss: 0.6240 - val_acc: 0.6645\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5796 - acc: 0.7082\n","Epoch 5: val_loss did not improve from 0.62404\n","238/238 [==============================] - 17s 70ms/step - loss: 0.5796 - acc: 0.7082 - val_loss: 0.6323 - val_acc: 0.6627\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5537 - acc: 0.7240\n","Epoch 6: val_loss did not improve from 0.62404\n","238/238 [==============================] - 17s 70ms/step - loss: 0.5537 - acc: 0.7240 - val_loss: 0.6356 - val_acc: 0.6571\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5196 - acc: 0.7474\n","Epoch 7: val_loss did not improve from 0.62404\n","238/238 [==============================] - 16s 68ms/step - loss: 0.5196 - acc: 0.7474 - val_loss: 0.6559 - val_acc: 0.6613\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4801 - acc: 0.7761\n","Epoch 8: val_loss did not improve from 0.62404\n","238/238 [==============================] - 17s 70ms/step - loss: 0.4801 - acc: 0.7761 - val_loss: 0.6834 - val_acc: 0.6488\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4256 - acc: 0.8064\n","Epoch 9: val_loss did not improve from 0.62404\n","238/238 [==============================] - 16s 67ms/step - loss: 0.4256 - acc: 0.8064 - val_loss: 0.7607 - val_acc: 0.6382\n","0.6991829705688131 0.6465259934112113 0.6598436062557498 0.5436593643937055\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2945 - acc: 0.9160\n","Epoch 1: val_loss improved from inf to 0.27394, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 20s 74ms/step - loss: 0.2945 - acc: 0.9160 - val_loss: 0.2739 - val_acc: 0.9233\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2599 - acc: 0.9220\n","Epoch 2: val_loss improved from 0.27394 to 0.26069, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2599 - acc: 0.9220 - val_loss: 0.2607 - val_acc: 0.9238\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2452 - acc: 0.9219\n","Epoch 3: val_loss improved from 0.26069 to 0.25840, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2452 - acc: 0.9219 - val_loss: 0.2584 - val_acc: 0.9233\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2346 - acc: 0.9227\n","Epoch 4: val_loss did not improve from 0.25840\n","238/238 [==============================] - 17s 71ms/step - loss: 0.2346 - acc: 0.9227 - val_loss: 0.2634 - val_acc: 0.9251\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2242 - acc: 0.9238\n","Epoch 5: val_loss did not improve from 0.25840\n","238/238 [==============================] - 17s 69ms/step - loss: 0.2242 - acc: 0.9238 - val_loss: 0.2666 - val_acc: 0.9228\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2073 - acc: 0.9279\n","Epoch 6: val_loss did not improve from 0.25840\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2073 - acc: 0.9279 - val_loss: 0.2847 - val_acc: 0.9164\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1889 - acc: 0.9361\n","Epoch 7: val_loss did not improve from 0.25840\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1889 - acc: 0.9361 - val_loss: 0.2924 - val_acc: 0.9173\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1628 - acc: 0.9426\n","Epoch 8: val_loss did not improve from 0.25840\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1628 - acc: 0.9426 - val_loss: 0.3238 - val_acc: 0.9080\n","0.7137575713224201 0.20396440750354075 0.9181232750689973 0.03783783783783784\n","Iteration number:  8\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2886 - acc: 0.9025\n","Epoch 1: val_loss improved from inf to 0.24065, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.2886 - acc: 0.9025 - val_loss: 0.2406 - val_acc: 0.9099\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2407 - acc: 0.9138\n","Epoch 2: val_loss did not improve from 0.24065\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2407 - acc: 0.9138 - val_loss: 0.2407 - val_acc: 0.9154\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2226 - acc: 0.9185\n","Epoch 3: val_loss improved from 0.24065 to 0.23562, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2226 - acc: 0.9185 - val_loss: 0.2356 - val_acc: 0.9150\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2081 - acc: 0.9225\n","Epoch 4: val_loss did not improve from 0.23562\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2081 - acc: 0.9225 - val_loss: 0.2379 - val_acc: 0.9136\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1919 - acc: 0.9290\n","Epoch 5: val_loss did not improve from 0.23562\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1919 - acc: 0.9290 - val_loss: 0.2417 - val_acc: 0.9150\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1729 - acc: 0.9338\n","Epoch 6: val_loss did not improve from 0.23562\n","238/238 [==============================] - 16s 65ms/step - loss: 0.1729 - acc: 0.9338 - val_loss: 0.2604 - val_acc: 0.9011\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1481 - acc: 0.9441\n","Epoch 7: val_loss did not improve from 0.23562\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1481 - acc: 0.9441 - val_loss: 0.2662 - val_acc: 0.9127\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1203 - acc: 0.9567\n","Epoch 8: val_loss did not improve from 0.23562\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1203 - acc: 0.9567 - val_loss: 0.3162 - val_acc: 0.8891\n","0.8580486166182043 0.5573384356342065 0.9123735050597976 0.37024793388429755\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2110 - acc: 0.9356\n","Epoch 1: val_loss improved from inf to 0.17301, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 19s 71ms/step - loss: 0.2110 - acc: 0.9356 - val_loss: 0.1730 - val_acc: 0.9418\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1725 - acc: 0.9433\n","Epoch 2: val_loss improved from 0.17301 to 0.17120, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1725 - acc: 0.9433 - val_loss: 0.1712 - val_acc: 0.9441\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1600 - acc: 0.9467\n","Epoch 3: val_loss improved from 0.17120 to 0.16725, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1600 - acc: 0.9467 - val_loss: 0.1672 - val_acc: 0.9436\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1480 - acc: 0.9484\n","Epoch 4: val_loss did not improve from 0.16725\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1480 - acc: 0.9484 - val_loss: 0.1726 - val_acc: 0.9409\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1359 - acc: 0.9522\n","Epoch 5: val_loss did not improve from 0.16725\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1359 - acc: 0.9522 - val_loss: 0.1728 - val_acc: 0.9422\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1221 - acc: 0.9574\n","Epoch 6: val_loss did not improve from 0.16725\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1221 - acc: 0.9574 - val_loss: 0.1851 - val_acc: 0.9399\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1038 - acc: 0.9628\n","Epoch 7: val_loss did not improve from 0.16725\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1038 - acc: 0.9628 - val_loss: 0.1874 - val_acc: 0.9367\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0824 - acc: 0.9708\n","Epoch 8: val_loss did not improve from 0.16725\n","238/238 [==============================] - 16s 67ms/step - loss: 0.0824 - acc: 0.9708 - val_loss: 0.2060 - val_acc: 0.9348\n","0.8799647299342978 0.5251905818064466 0.9436522539098436 0.45916114790286977\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6767 - acc: 0.6249\n","Epoch 1: val_loss improved from inf to 0.62619, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 19s 69ms/step - loss: 0.6767 - acc: 0.6249 - val_loss: 0.6262 - val_acc: 0.6687\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6275 - acc: 0.6626\n","Epoch 2: val_loss improved from 0.62619 to 0.62321, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.6275 - acc: 0.6626 - val_loss: 0.6232 - val_acc: 0.6548\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6069 - acc: 0.6816\n","Epoch 3: val_loss improved from 0.62321 to 0.61521, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 15s 65ms/step - loss: 0.6069 - acc: 0.6816 - val_loss: 0.6152 - val_acc: 0.6617\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5919 - acc: 0.6955\n","Epoch 4: val_loss did not improve from 0.61521\n","238/238 [==============================] - 16s 66ms/step - loss: 0.5919 - acc: 0.6955 - val_loss: 0.6203 - val_acc: 0.6664\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5743 - acc: 0.7097\n","Epoch 5: val_loss did not improve from 0.61521\n","238/238 [==============================] - 16s 67ms/step - loss: 0.5743 - acc: 0.7097 - val_loss: 0.6253 - val_acc: 0.6640\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5524 - acc: 0.7268\n","Epoch 6: val_loss did not improve from 0.61521\n","238/238 [==============================] - 15s 64ms/step - loss: 0.5524 - acc: 0.7268 - val_loss: 0.6252 - val_acc: 0.6673\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5201 - acc: 0.7503\n","Epoch 7: val_loss did not improve from 0.61521\n","238/238 [==============================] - 16s 66ms/step - loss: 0.5201 - acc: 0.7503 - val_loss: 0.6587 - val_acc: 0.6590\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4734 - acc: 0.7791\n","Epoch 8: val_loss did not improve from 0.61521\n","238/238 [==============================] - 16s 68ms/step - loss: 0.4734 - acc: 0.7791 - val_loss: 0.6872 - val_acc: 0.6405\n","0.7078295416779964 0.6505686940293678 0.671343146274149 0.5453388482341712\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2941 - acc: 0.9175\n","Epoch 1: val_loss improved from inf to 0.27237, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 19s 69ms/step - loss: 0.2941 - acc: 0.9175 - val_loss: 0.2724 - val_acc: 0.9242\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2570 - acc: 0.9212\n","Epoch 2: val_loss improved from 0.27237 to 0.26465, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2570 - acc: 0.9212 - val_loss: 0.2647 - val_acc: 0.9251\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2442 - acc: 0.9217\n","Epoch 3: val_loss did not improve from 0.26465\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2442 - acc: 0.9217 - val_loss: 0.2665 - val_acc: 0.9247\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2338 - acc: 0.9232\n","Epoch 4: val_loss improved from 0.26465 to 0.25939, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 15s 65ms/step - loss: 0.2338 - acc: 0.9232 - val_loss: 0.2594 - val_acc: 0.9251\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2212 - acc: 0.9239\n","Epoch 5: val_loss did not improve from 0.25939\n","238/238 [==============================] - 15s 63ms/step - loss: 0.2212 - acc: 0.9239 - val_loss: 0.2644 - val_acc: 0.9196\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2054 - acc: 0.9283\n","Epoch 6: val_loss did not improve from 0.25939\n","238/238 [==============================] - 16s 69ms/step - loss: 0.2054 - acc: 0.9283 - val_loss: 0.2685 - val_acc: 0.9238\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1845 - acc: 0.9350\n","Epoch 7: val_loss did not improve from 0.25939\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1845 - acc: 0.9350 - val_loss: 0.2806 - val_acc: 0.9131\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1565 - acc: 0.9458\n","Epoch 8: val_loss did not improve from 0.25939\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1565 - acc: 0.9458 - val_loss: 0.3161 - val_acc: 0.9039\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1227 - acc: 0.9582\n","Epoch 9: val_loss did not improve from 0.25939\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1227 - acc: 0.9582 - val_loss: 0.3900 - val_acc: 0.9201\n","0.725499453964108 0.20427183956060557 0.9167433302667893 0.0717948717948718\n","Iteration number:  9\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2844 - acc: 0.9044\n","Epoch 1: val_loss improved from inf to 0.23668, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.2844 - acc: 0.9044 - val_loss: 0.2367 - val_acc: 0.9131\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2373 - acc: 0.9149\n","Epoch 2: val_loss did not improve from 0.23668\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2373 - acc: 0.9149 - val_loss: 0.2416 - val_acc: 0.9108\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2229 - acc: 0.9216\n","Epoch 3: val_loss improved from 0.23668 to 0.23639, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2229 - acc: 0.9216 - val_loss: 0.2364 - val_acc: 0.9113\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2094 - acc: 0.9231\n","Epoch 4: val_loss improved from 0.23639 to 0.23302, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 17s 70ms/step - loss: 0.2094 - acc: 0.9231 - val_loss: 0.2330 - val_acc: 0.9150\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1945 - acc: 0.9282\n","Epoch 5: val_loss did not improve from 0.23302\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1945 - acc: 0.9282 - val_loss: 0.2433 - val_acc: 0.9020\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1755 - acc: 0.9362\n","Epoch 6: val_loss did not improve from 0.23302\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1755 - acc: 0.9362 - val_loss: 0.2520 - val_acc: 0.9177\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1529 - acc: 0.9436\n","Epoch 7: val_loss did not improve from 0.23302\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1529 - acc: 0.9436 - val_loss: 0.2661 - val_acc: 0.9043\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1224 - acc: 0.9553\n","Epoch 8: val_loss did not improve from 0.23302\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1224 - acc: 0.9553 - val_loss: 0.3129 - val_acc: 0.8905\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0993 - acc: 0.9648\n","Epoch 9: val_loss did not improve from 0.23302\n","238/238 [==============================] - 16s 68ms/step - loss: 0.0993 - acc: 0.9648 - val_loss: 0.3517 - val_acc: 0.9016\n","0.8723764208300291 0.5770896106591307 0.9153633854645814 0.4140127388535032\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2406 - acc: 0.9259\n","Epoch 1: val_loss improved from inf to 0.17913, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 19s 69ms/step - loss: 0.2406 - acc: 0.9259 - val_loss: 0.1791 - val_acc: 0.9432\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1797 - acc: 0.9420\n","Epoch 2: val_loss improved from 0.17913 to 0.17243, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1797 - acc: 0.9420 - val_loss: 0.1724 - val_acc: 0.9432\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1650 - acc: 0.9437\n","Epoch 3: val_loss improved from 0.17243 to 0.17149, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1650 - acc: 0.9437 - val_loss: 0.1715 - val_acc: 0.9409\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1502 - acc: 0.9480\n","Epoch 4: val_loss did not improve from 0.17149\n","238/238 [==============================] - 17s 71ms/step - loss: 0.1502 - acc: 0.9480 - val_loss: 0.1746 - val_acc: 0.9409\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1369 - acc: 0.9529\n","Epoch 5: val_loss did not improve from 0.17149\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1369 - acc: 0.9529 - val_loss: 0.1720 - val_acc: 0.9376\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1211 - acc: 0.9573\n","Epoch 6: val_loss did not improve from 0.17149\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1211 - acc: 0.9573 - val_loss: 0.1838 - val_acc: 0.9321\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1035 - acc: 0.9631\n","Epoch 7: val_loss did not improve from 0.17149\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1035 - acc: 0.9631 - val_loss: 0.1919 - val_acc: 0.9422\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0806 - acc: 0.9716\n","Epoch 8: val_loss did not improve from 0.17149\n","238/238 [==============================] - 16s 68ms/step - loss: 0.0806 - acc: 0.9716 - val_loss: 0.2085 - val_acc: 0.9381\n","0.8822502614042481 0.5178806254578625 0.9422723091076357 0.45076586433260396\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6821 - acc: 0.6197\n","Epoch 1: val_loss improved from inf to 0.62507, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 20s 73ms/step - loss: 0.6821 - acc: 0.6197 - val_loss: 0.6251 - val_acc: 0.6645\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6277 - acc: 0.6621\n","Epoch 2: val_loss improved from 0.62507 to 0.61564, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.6277 - acc: 0.6621 - val_loss: 0.6156 - val_acc: 0.6793\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6078 - acc: 0.6802\n","Epoch 3: val_loss improved from 0.61564 to 0.61335, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.6078 - acc: 0.6802 - val_loss: 0.6133 - val_acc: 0.6848\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5930 - acc: 0.6935\n","Epoch 4: val_loss did not improve from 0.61335\n","238/238 [==============================] - 16s 67ms/step - loss: 0.5930 - acc: 0.6935 - val_loss: 0.6240 - val_acc: 0.6594\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5740 - acc: 0.7084\n","Epoch 5: val_loss did not improve from 0.61335\n","238/238 [==============================] - 16s 67ms/step - loss: 0.5740 - acc: 0.7084 - val_loss: 0.6345 - val_acc: 0.6553\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5562 - acc: 0.7219\n","Epoch 6: val_loss did not improve from 0.61335\n","238/238 [==============================] - 16s 68ms/step - loss: 0.5562 - acc: 0.7219 - val_loss: 0.6196 - val_acc: 0.6714\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5197 - acc: 0.7493\n","Epoch 7: val_loss did not improve from 0.61335\n","238/238 [==============================] - 16s 66ms/step - loss: 0.5197 - acc: 0.7493 - val_loss: 0.6507 - val_acc: 0.6627\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4786 - acc: 0.7781\n","Epoch 8: val_loss did not improve from 0.61335\n","238/238 [==============================] - 16s 67ms/step - loss: 0.4786 - acc: 0.7781 - val_loss: 0.6849 - val_acc: 0.6534\n","0.7050564755948604 0.6485544634721405 0.6662833486660533 0.5511908444169502\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2884 - acc: 0.9189\n","Epoch 1: val_loss improved from inf to 0.26542, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 19s 66ms/step - loss: 0.2884 - acc: 0.9189 - val_loss: 0.2654 - val_acc: 0.9242\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2569 - acc: 0.9215\n","Epoch 2: val_loss improved from 0.26542 to 0.25924, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 15s 64ms/step - loss: 0.2569 - acc: 0.9215 - val_loss: 0.2592 - val_acc: 0.9247\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2440 - acc: 0.9221\n","Epoch 3: val_loss did not improve from 0.25924\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2440 - acc: 0.9221 - val_loss: 0.2633 - val_acc: 0.9242\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2360 - acc: 0.9230\n","Epoch 4: val_loss did not improve from 0.25924\n","238/238 [==============================] - 15s 65ms/step - loss: 0.2360 - acc: 0.9230 - val_loss: 0.2691 - val_acc: 0.9219\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2237 - acc: 0.9234\n","Epoch 5: val_loss did not improve from 0.25924\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2237 - acc: 0.9234 - val_loss: 0.2669 - val_acc: 0.9242\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2081 - acc: 0.9268\n","Epoch 6: val_loss did not improve from 0.25924\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2081 - acc: 0.9268 - val_loss: 0.2787 - val_acc: 0.9154\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1866 - acc: 0.9330\n","Epoch 7: val_loss did not improve from 0.25924\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1866 - acc: 0.9330 - val_loss: 0.2845 - val_acc: 0.9136\n","0.7135373274639166 0.20355096216697027 0.9178932842686293 0.005571030640668524\n","Iteration number:  10\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2898 - acc: 0.9025\n","Epoch 1: val_loss improved from inf to 0.24229, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 19s 68ms/step - loss: 0.2898 - acc: 0.9025 - val_loss: 0.2423 - val_acc: 0.9154\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2376 - acc: 0.9150\n","Epoch 2: val_loss improved from 0.24229 to 0.23475, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2376 - acc: 0.9150 - val_loss: 0.2347 - val_acc: 0.9154\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2239 - acc: 0.9194\n","Epoch 3: val_loss improved from 0.23475 to 0.23166, saving model to avg-word2vec-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2239 - acc: 0.9194 - val_loss: 0.2317 - val_acc: 0.9164\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2061 - acc: 0.9238\n","Epoch 4: val_loss did not improve from 0.23166\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2061 - acc: 0.9238 - val_loss: 0.2413 - val_acc: 0.9076\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1909 - acc: 0.9303\n","Epoch 5: val_loss did not improve from 0.23166\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1909 - acc: 0.9303 - val_loss: 0.2357 - val_acc: 0.9136\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1735 - acc: 0.9367\n","Epoch 6: val_loss did not improve from 0.23166\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1735 - acc: 0.9367 - val_loss: 0.2394 - val_acc: 0.9150\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1525 - acc: 0.9451\n","Epoch 7: val_loss did not improve from 0.23166\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1525 - acc: 0.9451 - val_loss: 0.2699 - val_acc: 0.8997\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1265 - acc: 0.9541\n","Epoch 8: val_loss did not improve from 0.23166\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1265 - acc: 0.9541 - val_loss: 0.2677 - val_acc: 0.9067\n","0.8791479425500045 0.5885841027192942 0.9151333946642134 0.4705882352941176\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2140 - acc: 0.9350\n","Epoch 1: val_loss improved from inf to 0.17318, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 19s 72ms/step - loss: 0.2140 - acc: 0.9350 - val_loss: 0.1732 - val_acc: 0.9441\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1734 - acc: 0.9424\n","Epoch 2: val_loss improved from 0.17318 to 0.16714, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1734 - acc: 0.9424 - val_loss: 0.1671 - val_acc: 0.9459\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1605 - acc: 0.9457\n","Epoch 3: val_loss improved from 0.16714 to 0.16406, saving model to avg-word2vec-mort_icu-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.1605 - acc: 0.9457 - val_loss: 0.1641 - val_acc: 0.9418\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1486 - acc: 0.9492\n","Epoch 4: val_loss did not improve from 0.16406\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1486 - acc: 0.9492 - val_loss: 0.1697 - val_acc: 0.9459\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1358 - acc: 0.9529\n","Epoch 5: val_loss did not improve from 0.16406\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1358 - acc: 0.9529 - val_loss: 0.1675 - val_acc: 0.9441\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1209 - acc: 0.9578\n","Epoch 6: val_loss did not improve from 0.16406\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1209 - acc: 0.9578 - val_loss: 0.1755 - val_acc: 0.9436\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1024 - acc: 0.9646\n","Epoch 7: val_loss did not improve from 0.16406\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1024 - acc: 0.9646 - val_loss: 0.1947 - val_acc: 0.9367\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0816 - acc: 0.9734\n","Epoch 8: val_loss did not improve from 0.16406\n","238/238 [==============================] - 16s 69ms/step - loss: 0.0816 - acc: 0.9734 - val_loss: 0.2123 - val_acc: 0.9284\n","0.8794856188648034 0.5201313321350687 0.9420423183072677 0.4\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6742 - acc: 0.6215\n","Epoch 1: val_loss improved from inf to 0.63465, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 20s 74ms/step - loss: 0.6742 - acc: 0.6215 - val_loss: 0.6346 - val_acc: 0.6571\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6268 - acc: 0.6639\n","Epoch 2: val_loss improved from 0.63465 to 0.62039, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 69ms/step - loss: 0.6268 - acc: 0.6639 - val_loss: 0.6204 - val_acc: 0.6774\n","Epoch 3/100\n","237/238 [============================>.] - ETA: 0s - loss: 0.6100 - acc: 0.6806\n","Epoch 3: val_loss improved from 0.62039 to 0.61754, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 66ms/step - loss: 0.6102 - acc: 0.6805 - val_loss: 0.6175 - val_acc: 0.6687\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5935 - acc: 0.6920\n","Epoch 4: val_loss improved from 0.61754 to 0.61421, saving model to avg-word2vec-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.5935 - acc: 0.6920 - val_loss: 0.6142 - val_acc: 0.6691\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5771 - acc: 0.7057\n","Epoch 5: val_loss did not improve from 0.61421\n","238/238 [==============================] - 17s 72ms/step - loss: 0.5771 - acc: 0.7057 - val_loss: 0.6294 - val_acc: 0.6594\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5539 - acc: 0.7247\n","Epoch 6: val_loss did not improve from 0.61421\n","238/238 [==============================] - 17s 70ms/step - loss: 0.5539 - acc: 0.7247 - val_loss: 0.6375 - val_acc: 0.6594\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5261 - acc: 0.7437\n","Epoch 7: val_loss did not improve from 0.61421\n","238/238 [==============================] - 16s 69ms/step - loss: 0.5261 - acc: 0.7437 - val_loss: 0.6422 - val_acc: 0.6622\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4798 - acc: 0.7742\n","Epoch 8: val_loss did not improve from 0.61421\n","238/238 [==============================] - 16s 67ms/step - loss: 0.4798 - acc: 0.7742 - val_loss: 0.6844 - val_acc: 0.6576\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4284 - acc: 0.8095\n","Epoch 9: val_loss did not improve from 0.61421\n","238/238 [==============================] - 16s 68ms/step - loss: 0.4284 - acc: 0.8095 - val_loss: 0.7392 - val_acc: 0.6497\n","0.7019422252722354 0.641482527531781 0.6653633854645814 0.5719329214474845\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2853 - acc: 0.9189\n","Epoch 1: val_loss improved from inf to 0.26840, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 19s 69ms/step - loss: 0.2853 - acc: 0.9189 - val_loss: 0.2684 - val_acc: 0.9247\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2550 - acc: 0.9218\n","Epoch 2: val_loss improved from 0.26840 to 0.25599, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2550 - acc: 0.9218 - val_loss: 0.2560 - val_acc: 0.9247\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2426 - acc: 0.9232\n","Epoch 3: val_loss did not improve from 0.25599\n","238/238 [==============================] - 16s 69ms/step - loss: 0.2426 - acc: 0.9232 - val_loss: 0.2647 - val_acc: 0.9238\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2354 - acc: 0.9229\n","Epoch 4: val_loss improved from 0.25599 to 0.25561, saving model to avg-word2vec-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2354 - acc: 0.9229 - val_loss: 0.2556 - val_acc: 0.9242\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2221 - acc: 0.9246\n","Epoch 5: val_loss did not improve from 0.25561\n","238/238 [==============================] - 17s 72ms/step - loss: 0.2221 - acc: 0.9246 - val_loss: 0.2634 - val_acc: 0.9224\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2054 - acc: 0.9302\n","Epoch 6: val_loss did not improve from 0.25561\n","238/238 [==============================] - 17s 69ms/step - loss: 0.2054 - acc: 0.9302 - val_loss: 0.2768 - val_acc: 0.9187\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1852 - acc: 0.9349\n","Epoch 7: val_loss did not improve from 0.25561\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1852 - acc: 0.9349 - val_loss: 0.2851 - val_acc: 0.9173\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1552 - acc: 0.9483\n","Epoch 8: val_loss did not improve from 0.25561\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1552 - acc: 0.9483 - val_loss: 0.3319 - val_acc: 0.8877\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1209 - acc: 0.9584\n","Epoch 9: val_loss did not improve from 0.25561\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1209 - acc: 0.9584 - val_loss: 0.3578 - val_acc: 0.8886\n","0.7226517642025624 0.20682137034439865 0.9174333026678932 0.03234501347708895\n","Embedding:  fasttext\n","=============================\n","Iteration number:  1\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2898 - acc: 0.9026\n","Epoch 1: val_loss improved from inf to 0.25362, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 19s 68ms/step - loss: 0.2898 - acc: 0.9026 - val_loss: 0.2536 - val_acc: 0.9140\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2419 - acc: 0.9152\n","Epoch 2: val_loss improved from 0.25362 to 0.23134, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.2419 - acc: 0.9152 - val_loss: 0.2313 - val_acc: 0.9117\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2219 - acc: 0.9188\n","Epoch 3: val_loss did not improve from 0.23134\n","238/238 [==============================] - 17s 70ms/step - loss: 0.2219 - acc: 0.9188 - val_loss: 0.2426 - val_acc: 0.9122\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2093 - acc: 0.9233\n","Epoch 4: val_loss did not improve from 0.23134\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2093 - acc: 0.9233 - val_loss: 0.2319 - val_acc: 0.9145\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1936 - acc: 0.9287\n","Epoch 5: val_loss did not improve from 0.23134\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1936 - acc: 0.9287 - val_loss: 0.2548 - val_acc: 0.9043\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1742 - acc: 0.9360\n","Epoch 6: val_loss did not improve from 0.23134\n","238/238 [==============================] - 17s 71ms/step - loss: 0.1742 - acc: 0.9360 - val_loss: 0.2505 - val_acc: 0.9039\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1547 - acc: 0.9437\n","Epoch 7: val_loss did not improve from 0.23134\n","238/238 [==============================] - 17s 70ms/step - loss: 0.1547 - acc: 0.9437 - val_loss: 0.2567 - val_acc: 0.9090\n","0.871794871794872 0.5684115486287455 0.9165133394664213 0.4557721139430285\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2173 - acc: 0.9351\n","Epoch 1: val_loss improved from inf to 0.17715, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 18s 67ms/step - loss: 0.2173 - acc: 0.9351 - val_loss: 0.1771 - val_acc: 0.9422\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1738 - acc: 0.9445\n","Epoch 2: val_loss improved from 0.17715 to 0.16976, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 17s 71ms/step - loss: 0.1738 - acc: 0.9445 - val_loss: 0.1698 - val_acc: 0.9450\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1591 - acc: 0.9459\n","Epoch 3: val_loss did not improve from 0.16976\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1591 - acc: 0.9459 - val_loss: 0.1700 - val_acc: 0.9422\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1458 - acc: 0.9498\n","Epoch 4: val_loss improved from 0.16976 to 0.16883, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 18s 74ms/step - loss: 0.1458 - acc: 0.9498 - val_loss: 0.1688 - val_acc: 0.9385\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1338 - acc: 0.9533\n","Epoch 5: val_loss did not improve from 0.16883\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1338 - acc: 0.9533 - val_loss: 0.1747 - val_acc: 0.9367\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1211 - acc: 0.9568\n","Epoch 6: val_loss did not improve from 0.16883\n","238/238 [==============================] - 17s 71ms/step - loss: 0.1211 - acc: 0.9568 - val_loss: 0.1796 - val_acc: 0.9418\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1022 - acc: 0.9645\n","Epoch 7: val_loss did not improve from 0.16883\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1022 - acc: 0.9645 - val_loss: 0.1959 - val_acc: 0.9385\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0793 - acc: 0.9742\n","Epoch 8: val_loss did not improve from 0.16883\n","238/238 [==============================] - 17s 71ms/step - loss: 0.0793 - acc: 0.9742 - val_loss: 0.2221 - val_acc: 0.9302\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0609 - acc: 0.9784\n","Epoch 9: val_loss did not improve from 0.16883\n","238/238 [==============================] - 16s 68ms/step - loss: 0.0609 - acc: 0.9784 - val_loss: 0.2629 - val_acc: 0.9261\n","0.8783112505267101 0.5112306823609053 0.9422723091076357 0.47157894736842093\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6804 - acc: 0.6248\n","Epoch 1: val_loss improved from inf to 0.62478, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.6804 - acc: 0.6248 - val_loss: 0.6248 - val_acc: 0.6765\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6285 - acc: 0.6634\n","Epoch 2: val_loss improved from 0.62478 to 0.62041, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 66ms/step - loss: 0.6285 - acc: 0.6634 - val_loss: 0.6204 - val_acc: 0.6668\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6106 - acc: 0.6840\n","Epoch 3: val_loss improved from 0.62041 to 0.61699, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.6106 - acc: 0.6840 - val_loss: 0.6170 - val_acc: 0.6742\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5932 - acc: 0.6953\n","Epoch 4: val_loss improved from 0.61699 to 0.61534, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 17s 72ms/step - loss: 0.5932 - acc: 0.6953 - val_loss: 0.6153 - val_acc: 0.6830\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5747 - acc: 0.7075\n","Epoch 5: val_loss did not improve from 0.61534\n","238/238 [==============================] - 16s 68ms/step - loss: 0.5747 - acc: 0.7075 - val_loss: 0.6226 - val_acc: 0.6691\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5542 - acc: 0.7219\n","Epoch 6: val_loss did not improve from 0.61534\n","238/238 [==============================] - 16s 67ms/step - loss: 0.5542 - acc: 0.7219 - val_loss: 0.6303 - val_acc: 0.6682\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5196 - acc: 0.7477\n","Epoch 7: val_loss did not improve from 0.61534\n","238/238 [==============================] - 16s 69ms/step - loss: 0.5196 - acc: 0.7477 - val_loss: 0.6477 - val_acc: 0.6636\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4772 - acc: 0.7774\n","Epoch 8: val_loss did not improve from 0.61534\n","238/238 [==============================] - 16s 66ms/step - loss: 0.4772 - acc: 0.7774 - val_loss: 0.6850 - val_acc: 0.6405\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4217 - acc: 0.8079\n","Epoch 9: val_loss did not improve from 0.61534\n","238/238 [==============================] - 16s 66ms/step - loss: 0.4217 - acc: 0.8079 - val_loss: 0.7132 - val_acc: 0.6372\n","0.7045013228712131 0.648626885538339 0.6612235510579577 0.5974309920743373\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2879 - acc: 0.9190\n","Epoch 1: val_loss improved from inf to 0.25805, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 19s 68ms/step - loss: 0.2879 - acc: 0.9190 - val_loss: 0.2580 - val_acc: 0.9247\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2555 - acc: 0.9217\n","Epoch 2: val_loss improved from 0.25805 to 0.25759, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 15s 65ms/step - loss: 0.2555 - acc: 0.9217 - val_loss: 0.2576 - val_acc: 0.9238\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2435 - acc: 0.9223\n","Epoch 3: val_loss did not improve from 0.25759\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2435 - acc: 0.9223 - val_loss: 0.2579 - val_acc: 0.9247\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2350 - acc: 0.9231\n","Epoch 4: val_loss did not improve from 0.25759\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2350 - acc: 0.9231 - val_loss: 0.2709 - val_acc: 0.9242\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2223 - acc: 0.9248\n","Epoch 5: val_loss did not improve from 0.25759\n","238/238 [==============================] - 15s 64ms/step - loss: 0.2223 - acc: 0.9248 - val_loss: 0.2695 - val_acc: 0.9196\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2106 - acc: 0.9279\n","Epoch 6: val_loss did not improve from 0.25759\n","238/238 [==============================] - 15s 64ms/step - loss: 0.2106 - acc: 0.9279 - val_loss: 0.2716 - val_acc: 0.9238\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1879 - acc: 0.9336\n","Epoch 7: val_loss did not improve from 0.25759\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1879 - acc: 0.9336 - val_loss: 0.2955 - val_acc: 0.9094\n","0.7323227916507171 0.2220991922678105 0.9174333026678932 0.027100271002710025\n","Iteration number:  2\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2839 - acc: 0.9051\n","Epoch 1: val_loss improved from inf to 0.23544, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 19s 68ms/step - loss: 0.2839 - acc: 0.9051 - val_loss: 0.2354 - val_acc: 0.9140\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2393 - acc: 0.9148\n","Epoch 2: val_loss improved from 0.23544 to 0.23543, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 65ms/step - loss: 0.2393 - acc: 0.9148 - val_loss: 0.2354 - val_acc: 0.9140\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2218 - acc: 0.9185\n","Epoch 3: val_loss did not improve from 0.23543\n","238/238 [==============================] - 15s 64ms/step - loss: 0.2218 - acc: 0.9185 - val_loss: 0.2377 - val_acc: 0.9140\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2093 - acc: 0.9233\n","Epoch 4: val_loss improved from 0.23543 to 0.23240, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2093 - acc: 0.9233 - val_loss: 0.2324 - val_acc: 0.9136\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1920 - acc: 0.9277\n","Epoch 5: val_loss did not improve from 0.23240\n","238/238 [==============================] - 15s 65ms/step - loss: 0.1920 - acc: 0.9277 - val_loss: 0.2474 - val_acc: 0.9034\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1736 - acc: 0.9355\n","Epoch 6: val_loss did not improve from 0.23240\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1736 - acc: 0.9355 - val_loss: 0.2513 - val_acc: 0.9154\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1512 - acc: 0.9466\n","Epoch 7: val_loss did not improve from 0.23240\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1512 - acc: 0.9466 - val_loss: 0.2678 - val_acc: 0.9053\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1213 - acc: 0.9574\n","Epoch 8: val_loss did not improve from 0.23240\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1213 - acc: 0.9574 - val_loss: 0.3205 - val_acc: 0.8831\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0974 - acc: 0.9654\n","Epoch 9: val_loss did not improve from 0.23240\n","238/238 [==============================] - 16s 66ms/step - loss: 0.0974 - acc: 0.9654 - val_loss: 0.3261 - val_acc: 0.8928\n","0.869832143801216 0.5690682030619407 0.9144434222631095 0.425925925925926\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2179 - acc: 0.9315\n","Epoch 1: val_loss improved from inf to 0.17855, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.2179 - acc: 0.9315 - val_loss: 0.1785 - val_acc: 0.9441\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1749 - acc: 0.9424\n","Epoch 2: val_loss improved from 0.17855 to 0.17194, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1749 - acc: 0.9424 - val_loss: 0.1719 - val_acc: 0.9409\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1605 - acc: 0.9470\n","Epoch 3: val_loss improved from 0.17194 to 0.16781, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1605 - acc: 0.9470 - val_loss: 0.1678 - val_acc: 0.9390\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1486 - acc: 0.9497\n","Epoch 4: val_loss did not improve from 0.16781\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1486 - acc: 0.9497 - val_loss: 0.1692 - val_acc: 0.9455\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1348 - acc: 0.9534\n","Epoch 5: val_loss did not improve from 0.16781\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1348 - acc: 0.9534 - val_loss: 0.1756 - val_acc: 0.9404\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1205 - acc: 0.9562\n","Epoch 6: val_loss did not improve from 0.16781\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1205 - acc: 0.9562 - val_loss: 0.1831 - val_acc: 0.9422\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1021 - acc: 0.9641\n","Epoch 7: val_loss did not improve from 0.16781\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1021 - acc: 0.9641 - val_loss: 0.1932 - val_acc: 0.9376\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0803 - acc: 0.9723\n","Epoch 8: val_loss did not improve from 0.16781\n","238/238 [==============================] - 16s 68ms/step - loss: 0.0803 - acc: 0.9723 - val_loss: 0.2269 - val_acc: 0.9362\n","0.8831616648719509 0.5218047815373666 0.9427322907083716 0.4575163398692811\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6860 - acc: 0.6222\n","Epoch 1: val_loss improved from inf to 0.63310, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 18s 68ms/step - loss: 0.6860 - acc: 0.6222 - val_loss: 0.6331 - val_acc: 0.6608\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6271 - acc: 0.6653\n","Epoch 2: val_loss improved from 0.63310 to 0.61928, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 15s 63ms/step - loss: 0.6271 - acc: 0.6653 - val_loss: 0.6193 - val_acc: 0.6738\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6084 - acc: 0.6818\n","Epoch 3: val_loss did not improve from 0.61928\n","238/238 [==============================] - 16s 67ms/step - loss: 0.6084 - acc: 0.6818 - val_loss: 0.6193 - val_acc: 0.6677\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5928 - acc: 0.6938\n","Epoch 4: val_loss did not improve from 0.61928\n","238/238 [==============================] - 16s 67ms/step - loss: 0.5928 - acc: 0.6938 - val_loss: 0.6231 - val_acc: 0.6553\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5765 - acc: 0.7066\n","Epoch 5: val_loss did not improve from 0.61928\n","238/238 [==============================] - 16s 67ms/step - loss: 0.5765 - acc: 0.7066 - val_loss: 0.6249 - val_acc: 0.6664\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5551 - acc: 0.7226\n","Epoch 6: val_loss did not improve from 0.61928\n","238/238 [==============================] - 16s 68ms/step - loss: 0.5551 - acc: 0.7226 - val_loss: 0.6380 - val_acc: 0.6543\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5239 - acc: 0.7449\n","Epoch 7: val_loss did not improve from 0.61928\n","238/238 [==============================] - 16s 66ms/step - loss: 0.5239 - acc: 0.7449 - val_loss: 0.6507 - val_acc: 0.6437\n","0.6997816075582776 0.641599252194841 0.6626034958601656 0.5290529695024078\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2939 - acc: 0.9177\n","Epoch 1: val_loss improved from inf to 0.26495, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 18s 65ms/step - loss: 0.2939 - acc: 0.9177 - val_loss: 0.2649 - val_acc: 0.9247\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2574 - acc: 0.9217\n","Epoch 2: val_loss improved from 0.26495 to 0.25966, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 15s 65ms/step - loss: 0.2574 - acc: 0.9217 - val_loss: 0.2597 - val_acc: 0.9247\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2458 - acc: 0.9219\n","Epoch 3: val_loss did not improve from 0.25966\n","238/238 [==============================] - 15s 63ms/step - loss: 0.2458 - acc: 0.9219 - val_loss: 0.2635 - val_acc: 0.9242\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2339 - acc: 0.9232\n","Epoch 4: val_loss did not improve from 0.25966\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2339 - acc: 0.9232 - val_loss: 0.2737 - val_acc: 0.9219\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2221 - acc: 0.9242\n","Epoch 5: val_loss did not improve from 0.25966\n","238/238 [==============================] - 16s 65ms/step - loss: 0.2221 - acc: 0.9242 - val_loss: 0.2645 - val_acc: 0.9261\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2046 - acc: 0.9292\n","Epoch 6: val_loss did not improve from 0.25966\n","238/238 [==============================] - 15s 63ms/step - loss: 0.2046 - acc: 0.9292 - val_loss: 0.2665 - val_acc: 0.9224\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1802 - acc: 0.9363\n","Epoch 7: val_loss did not improve from 0.25966\n","238/238 [==============================] - 16s 65ms/step - loss: 0.1802 - acc: 0.9363 - val_loss: 0.3052 - val_acc: 0.9168\n","0.7123777048478981 0.1859017848746295 0.9181232750689973 0.0\n","Iteration number:  3\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2955 - acc: 0.8991\n","Epoch 1: val_loss improved from inf to 0.23884, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 19s 69ms/step - loss: 0.2955 - acc: 0.8991 - val_loss: 0.2388 - val_acc: 0.9145\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.9148\n","Epoch 2: val_loss improved from 0.23884 to 0.23716, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2400 - acc: 0.9148 - val_loss: 0.2372 - val_acc: 0.9168\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2234 - acc: 0.9199\n","Epoch 3: val_loss improved from 0.23716 to 0.23472, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 15s 64ms/step - loss: 0.2234 - acc: 0.9199 - val_loss: 0.2347 - val_acc: 0.9140\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2094 - acc: 0.9255\n","Epoch 4: val_loss improved from 0.23472 to 0.23231, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2094 - acc: 0.9255 - val_loss: 0.2323 - val_acc: 0.9164\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1926 - acc: 0.9280\n","Epoch 5: val_loss did not improve from 0.23231\n","238/238 [==============================] - 16s 65ms/step - loss: 0.1926 - acc: 0.9280 - val_loss: 0.2351 - val_acc: 0.9067\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1726 - acc: 0.9369\n","Epoch 6: val_loss did not improve from 0.23231\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1726 - acc: 0.9369 - val_loss: 0.2482 - val_acc: 0.9140\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1508 - acc: 0.9444\n","Epoch 7: val_loss did not improve from 0.23231\n","238/238 [==============================] - 15s 65ms/step - loss: 0.1508 - acc: 0.9444 - val_loss: 0.2590 - val_acc: 0.9067\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1221 - acc: 0.9575\n","Epoch 8: val_loss did not improve from 0.23231\n","238/238 [==============================] - 15s 65ms/step - loss: 0.1221 - acc: 0.9575 - val_loss: 0.3042 - val_acc: 0.9076\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0920 - acc: 0.9687\n","Epoch 9: val_loss did not improve from 0.23231\n","238/238 [==============================] - 16s 68ms/step - loss: 0.0920 - acc: 0.9687 - val_loss: 0.3499 - val_acc: 0.9002\n","0.8764522204599524 0.5760131665048718 0.9149034038638455 0.43251533742331283\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2144 - acc: 0.9359\n","Epoch 1: val_loss improved from inf to 0.17772, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 20s 72ms/step - loss: 0.2144 - acc: 0.9359 - val_loss: 0.1777 - val_acc: 0.9436\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1740 - acc: 0.9421\n","Epoch 2: val_loss improved from 0.17772 to 0.17089, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1740 - acc: 0.9421 - val_loss: 0.1709 - val_acc: 0.9418\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1607 - acc: 0.9460\n","Epoch 3: val_loss improved from 0.17089 to 0.16611, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 15s 65ms/step - loss: 0.1607 - acc: 0.9460 - val_loss: 0.1661 - val_acc: 0.9432\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1471 - acc: 0.9490\n","Epoch 4: val_loss did not improve from 0.16611\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1471 - acc: 0.9490 - val_loss: 0.1664 - val_acc: 0.9422\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1335 - acc: 0.9534\n","Epoch 5: val_loss did not improve from 0.16611\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1335 - acc: 0.9534 - val_loss: 0.1705 - val_acc: 0.9427\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1196 - acc: 0.9582\n","Epoch 6: val_loss did not improve from 0.16611\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1196 - acc: 0.9582 - val_loss: 0.1764 - val_acc: 0.9409\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1000 - acc: 0.9648\n","Epoch 7: val_loss did not improve from 0.16611\n","238/238 [==============================] - 16s 69ms/step - loss: 0.1000 - acc: 0.9648 - val_loss: 0.2065 - val_acc: 0.9288\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0800 - acc: 0.9721\n","Epoch 8: val_loss did not improve from 0.16611\n","238/238 [==============================] - 16s 65ms/step - loss: 0.0800 - acc: 0.9721 - val_loss: 0.2263 - val_acc: 0.9298\n","0.8803728326856751 0.5116362201937308 0.9392824287028518 0.326530612244898\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6829 - acc: 0.6200\n","Epoch 1: val_loss improved from inf to 0.63649, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 19s 68ms/step - loss: 0.6829 - acc: 0.6200 - val_loss: 0.6365 - val_acc: 0.6650\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6282 - acc: 0.6605\n","Epoch 2: val_loss improved from 0.63649 to 0.62807, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.6282 - acc: 0.6605 - val_loss: 0.6281 - val_acc: 0.6650\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6093 - acc: 0.6762\n","Epoch 3: val_loss improved from 0.62807 to 0.61792, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.6093 - acc: 0.6762 - val_loss: 0.6179 - val_acc: 0.6696\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5941 - acc: 0.6908\n","Epoch 4: val_loss did not improve from 0.61792\n","238/238 [==============================] - 16s 67ms/step - loss: 0.5941 - acc: 0.6908 - val_loss: 0.6201 - val_acc: 0.6788\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5770 - acc: 0.7025\n","Epoch 5: val_loss did not improve from 0.61792\n","238/238 [==============================] - 15s 64ms/step - loss: 0.5770 - acc: 0.7025 - val_loss: 0.6220 - val_acc: 0.6664\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5539 - acc: 0.7211\n","Epoch 6: val_loss did not improve from 0.61792\n","238/238 [==============================] - 16s 66ms/step - loss: 0.5539 - acc: 0.7211 - val_loss: 0.6280 - val_acc: 0.6691\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5243 - acc: 0.7451\n","Epoch 7: val_loss did not improve from 0.61792\n","238/238 [==============================] - 16s 66ms/step - loss: 0.5243 - acc: 0.7451 - val_loss: 0.6461 - val_acc: 0.6594\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4831 - acc: 0.7747\n","Epoch 8: val_loss did not improve from 0.61792\n","238/238 [==============================] - 16s 66ms/step - loss: 0.4831 - acc: 0.7747 - val_loss: 0.6897 - val_acc: 0.6479\n","0.6985675009603224 0.6414432284036514 0.6609935602575897 0.5154503616042078\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2961 - acc: 0.9154\n","Epoch 1: val_loss improved from inf to 0.27243, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 19s 69ms/step - loss: 0.2961 - acc: 0.9154 - val_loss: 0.2724 - val_acc: 0.9251\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2575 - acc: 0.9219\n","Epoch 2: val_loss improved from 0.27243 to 0.25687, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2575 - acc: 0.9219 - val_loss: 0.2569 - val_acc: 0.9251\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2449 - acc: 0.9222\n","Epoch 3: val_loss did not improve from 0.25687\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2449 - acc: 0.9222 - val_loss: 0.2645 - val_acc: 0.9251\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2361 - acc: 0.9231\n","Epoch 4: val_loss did not improve from 0.25687\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2361 - acc: 0.9231 - val_loss: 0.2579 - val_acc: 0.9242\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2239 - acc: 0.9238\n","Epoch 5: val_loss did not improve from 0.25687\n","238/238 [==============================] - 16s 69ms/step - loss: 0.2239 - acc: 0.9238 - val_loss: 0.2616 - val_acc: 0.9233\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2067 - acc: 0.9273\n","Epoch 6: val_loss did not improve from 0.25687\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2067 - acc: 0.9273 - val_loss: 0.2705 - val_acc: 0.9187\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1848 - acc: 0.9360\n","Epoch 7: val_loss did not improve from 0.25687\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1848 - acc: 0.9360 - val_loss: 0.2800 - val_acc: 0.9187\n","0.7267984001711287 0.20229770570037678 0.9185832566697332 0.016666666666666666\n","Iteration number:  4\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2863 - acc: 0.9025\n","Epoch 1: val_loss improved from inf to 0.25336, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 18s 67ms/step - loss: 0.2863 - acc: 0.9025 - val_loss: 0.2534 - val_acc: 0.9117\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2395 - acc: 0.9167\n","Epoch 2: val_loss improved from 0.25336 to 0.24126, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 17s 70ms/step - loss: 0.2395 - acc: 0.9167 - val_loss: 0.2413 - val_acc: 0.9122\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2251 - acc: 0.9190\n","Epoch 3: val_loss improved from 0.24126 to 0.23631, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 69ms/step - loss: 0.2251 - acc: 0.9190 - val_loss: 0.2363 - val_acc: 0.9136\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2093 - acc: 0.9255\n","Epoch 4: val_loss improved from 0.23631 to 0.23090, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2093 - acc: 0.9255 - val_loss: 0.2309 - val_acc: 0.9205\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1916 - acc: 0.9299\n","Epoch 5: val_loss did not improve from 0.23090\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1916 - acc: 0.9299 - val_loss: 0.2513 - val_acc: 0.9127\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1719 - acc: 0.9378\n","Epoch 6: val_loss did not improve from 0.23090\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1719 - acc: 0.9378 - val_loss: 0.2524 - val_acc: 0.9104\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1481 - acc: 0.9457\n","Epoch 7: val_loss did not improve from 0.23090\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1481 - acc: 0.9457 - val_loss: 0.2859 - val_acc: 0.8965\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1248 - acc: 0.9552\n","Epoch 8: val_loss did not improve from 0.23090\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1248 - acc: 0.9552 - val_loss: 0.3153 - val_acc: 0.9090\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0939 - acc: 0.9679\n","Epoch 9: val_loss did not improve from 0.23090\n","238/238 [==============================] - 16s 67ms/step - loss: 0.0939 - acc: 0.9679 - val_loss: 0.3512 - val_acc: 0.8965\n","0.8659961780773636 0.573046007348484 0.9158233670653174 0.43343653250773995\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2159 - acc: 0.9346\n","Epoch 1: val_loss improved from inf to 0.17719, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 19s 69ms/step - loss: 0.2159 - acc: 0.9346 - val_loss: 0.1772 - val_acc: 0.9409\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1768 - acc: 0.9432\n","Epoch 2: val_loss improved from 0.17719 to 0.17353, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 15s 65ms/step - loss: 0.1768 - acc: 0.9432 - val_loss: 0.1735 - val_acc: 0.9395\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1610 - acc: 0.9452\n","Epoch 3: val_loss did not improve from 0.17353\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1610 - acc: 0.9452 - val_loss: 0.1746 - val_acc: 0.9404\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1482 - acc: 0.9489\n","Epoch 4: val_loss improved from 0.17353 to 0.16810, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 15s 65ms/step - loss: 0.1482 - acc: 0.9489 - val_loss: 0.1681 - val_acc: 0.9422\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1356 - acc: 0.9517\n","Epoch 5: val_loss did not improve from 0.16810\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1356 - acc: 0.9517 - val_loss: 0.1760 - val_acc: 0.9441\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1191 - acc: 0.9570\n","Epoch 6: val_loss did not improve from 0.16810\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1191 - acc: 0.9570 - val_loss: 0.1806 - val_acc: 0.9399\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1012 - acc: 0.9637\n","Epoch 7: val_loss did not improve from 0.16810\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1012 - acc: 0.9637 - val_loss: 0.2053 - val_acc: 0.9293\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0764 - acc: 0.9750\n","Epoch 8: val_loss did not improve from 0.16810\n","238/238 [==============================] - 15s 65ms/step - loss: 0.0764 - acc: 0.9750 - val_loss: 0.2155 - val_acc: 0.9353\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0605 - acc: 0.9791\n","Epoch 9: val_loss did not improve from 0.16810\n","238/238 [==============================] - 16s 66ms/step - loss: 0.0605 - acc: 0.9791 - val_loss: 0.2271 - val_acc: 0.9274\n","0.8811305148493219 0.5315202816940116 0.9445722171113156 0.4749455337690632\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6818 - acc: 0.6243\n","Epoch 1: val_loss improved from inf to 0.62858, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 18s 66ms/step - loss: 0.6818 - acc: 0.6243 - val_loss: 0.6286 - val_acc: 0.6673\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6304 - acc: 0.6645\n","Epoch 2: val_loss improved from 0.62858 to 0.62193, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.6304 - acc: 0.6645 - val_loss: 0.6219 - val_acc: 0.6645\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6087 - acc: 0.6813\n","Epoch 3: val_loss improved from 0.62193 to 0.61485, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 66ms/step - loss: 0.6087 - acc: 0.6813 - val_loss: 0.6148 - val_acc: 0.6687\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5935 - acc: 0.6934\n","Epoch 4: val_loss did not improve from 0.61485\n","238/238 [==============================] - 15s 65ms/step - loss: 0.5935 - acc: 0.6934 - val_loss: 0.6178 - val_acc: 0.6733\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5732 - acc: 0.7105\n","Epoch 5: val_loss did not improve from 0.61485\n","238/238 [==============================] - 15s 63ms/step - loss: 0.5732 - acc: 0.7105 - val_loss: 0.6258 - val_acc: 0.6631\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5518 - acc: 0.7249\n","Epoch 6: val_loss did not improve from 0.61485\n","238/238 [==============================] - 16s 65ms/step - loss: 0.5518 - acc: 0.7249 - val_loss: 0.6272 - val_acc: 0.6710\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5177 - acc: 0.7477\n","Epoch 7: val_loss did not improve from 0.61485\n","238/238 [==============================] - 15s 65ms/step - loss: 0.5177 - acc: 0.7477 - val_loss: 0.6375 - val_acc: 0.6557\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4755 - acc: 0.7774\n","Epoch 8: val_loss did not improve from 0.61485\n","238/238 [==============================] - 16s 66ms/step - loss: 0.4755 - acc: 0.7774 - val_loss: 0.7051 - val_acc: 0.6308\n","0.7077501262446426 0.6499285538411181 0.6669733210671573 0.5834292289988492\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2896 - acc: 0.9180\n","Epoch 1: val_loss improved from inf to 0.26682, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 18s 66ms/step - loss: 0.2896 - acc: 0.9180 - val_loss: 0.2668 - val_acc: 0.9247\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2568 - acc: 0.9212\n","Epoch 2: val_loss improved from 0.26682 to 0.26201, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 15s 64ms/step - loss: 0.2568 - acc: 0.9212 - val_loss: 0.2620 - val_acc: 0.9247\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2444 - acc: 0.9224\n","Epoch 3: val_loss did not improve from 0.26201\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2444 - acc: 0.9224 - val_loss: 0.2668 - val_acc: 0.9247\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2317 - acc: 0.9233\n","Epoch 4: val_loss did not improve from 0.26201\n","238/238 [==============================] - 15s 64ms/step - loss: 0.2317 - acc: 0.9233 - val_loss: 0.2660 - val_acc: 0.9238\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2220 - acc: 0.9255\n","Epoch 5: val_loss did not improve from 0.26201\n","238/238 [==============================] - 15s 63ms/step - loss: 0.2220 - acc: 0.9255 - val_loss: 0.2630 - val_acc: 0.9224\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2048 - acc: 0.9289\n","Epoch 6: val_loss did not improve from 0.26201\n","238/238 [==============================] - 15s 65ms/step - loss: 0.2048 - acc: 0.9289 - val_loss: 0.2762 - val_acc: 0.9187\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1842 - acc: 0.9348\n","Epoch 7: val_loss did not improve from 0.26201\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1842 - acc: 0.9348 - val_loss: 0.2898 - val_acc: 0.9140\n","0.717684667086983 0.20688955331471276 0.9185832566697332 0.0111731843575419\n","Iteration number:  5\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.3028 - acc: 0.8969\n","Epoch 1: val_loss improved from inf to 0.24203, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 19s 68ms/step - loss: 0.3028 - acc: 0.8969 - val_loss: 0.2420 - val_acc: 0.9150\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2436 - acc: 0.9144\n","Epoch 2: val_loss improved from 0.24203 to 0.23767, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 65ms/step - loss: 0.2436 - acc: 0.9144 - val_loss: 0.2377 - val_acc: 0.9131\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2256 - acc: 0.9178\n","Epoch 3: val_loss improved from 0.23767 to 0.23237, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 15s 64ms/step - loss: 0.2256 - acc: 0.9178 - val_loss: 0.2324 - val_acc: 0.9150\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2081 - acc: 0.9237\n","Epoch 4: val_loss did not improve from 0.23237\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2081 - acc: 0.9237 - val_loss: 0.2383 - val_acc: 0.9090\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1950 - acc: 0.9288\n","Epoch 5: val_loss did not improve from 0.23237\n","238/238 [==============================] - 15s 65ms/step - loss: 0.1950 - acc: 0.9288 - val_loss: 0.2339 - val_acc: 0.9164\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1749 - acc: 0.9366\n","Epoch 6: val_loss did not improve from 0.23237\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1749 - acc: 0.9366 - val_loss: 0.2433 - val_acc: 0.9150\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1504 - acc: 0.9466\n","Epoch 7: val_loss did not improve from 0.23237\n","238/238 [==============================] - 15s 65ms/step - loss: 0.1504 - acc: 0.9466 - val_loss: 0.2906 - val_acc: 0.9196\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1230 - acc: 0.9560\n","Epoch 8: val_loss did not improve from 0.23237\n","238/238 [==============================] - 15s 65ms/step - loss: 0.1230 - acc: 0.9560 - val_loss: 0.3106 - val_acc: 0.8956\n","0.8682714336064852 0.5689804689897683 0.9149034038638455 0.4126984126984128\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2166 - acc: 0.9344\n","Epoch 1: val_loss improved from inf to 0.18111, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 18s 66ms/step - loss: 0.2166 - acc: 0.9344 - val_loss: 0.1811 - val_acc: 0.9395\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1741 - acc: 0.9430\n","Epoch 2: val_loss improved from 0.18111 to 0.17054, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1741 - acc: 0.9430 - val_loss: 0.1705 - val_acc: 0.9432\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1606 - acc: 0.9470\n","Epoch 3: val_loss improved from 0.17054 to 0.16327, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 65ms/step - loss: 0.1606 - acc: 0.9470 - val_loss: 0.1633 - val_acc: 0.9432\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1492 - acc: 0.9486\n","Epoch 4: val_loss did not improve from 0.16327\n","238/238 [==============================] - 15s 63ms/step - loss: 0.1492 - acc: 0.9486 - val_loss: 0.1658 - val_acc: 0.9445\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1338 - acc: 0.9526\n","Epoch 5: val_loss did not improve from 0.16327\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1338 - acc: 0.9526 - val_loss: 0.1756 - val_acc: 0.9385\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1182 - acc: 0.9591\n","Epoch 6: val_loss did not improve from 0.16327\n","238/238 [==============================] - 15s 63ms/step - loss: 0.1182 - acc: 0.9591 - val_loss: 0.1768 - val_acc: 0.9445\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0955 - acc: 0.9671\n","Epoch 7: val_loss did not improve from 0.16327\n","238/238 [==============================] - 15s 63ms/step - loss: 0.0955 - acc: 0.9671 - val_loss: 0.2030 - val_acc: 0.9358\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0744 - acc: 0.9748\n","Epoch 8: val_loss did not improve from 0.16327\n","238/238 [==============================] - 15s 63ms/step - loss: 0.0744 - acc: 0.9748 - val_loss: 0.2213 - val_acc: 0.9316\n","0.886847854924544 0.5268702744425273 0.9408923643054278 0.37163814180929094\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6727 - acc: 0.6291\n","Epoch 1: val_loss improved from inf to 0.63896, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 19s 69ms/step - loss: 0.6727 - acc: 0.6291 - val_loss: 0.6390 - val_acc: 0.6650\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6286 - acc: 0.6675\n","Epoch 2: val_loss improved from 0.63896 to 0.61895, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 66ms/step - loss: 0.6286 - acc: 0.6675 - val_loss: 0.6190 - val_acc: 0.6710\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6089 - acc: 0.6774\n","Epoch 3: val_loss improved from 0.61895 to 0.61854, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 15s 63ms/step - loss: 0.6089 - acc: 0.6774 - val_loss: 0.6185 - val_acc: 0.6774\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5929 - acc: 0.6956\n","Epoch 4: val_loss improved from 0.61854 to 0.61622, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 65ms/step - loss: 0.5929 - acc: 0.6956 - val_loss: 0.6162 - val_acc: 0.6784\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5743 - acc: 0.7083\n","Epoch 5: val_loss did not improve from 0.61622\n","238/238 [==============================] - 15s 64ms/step - loss: 0.5743 - acc: 0.7083 - val_loss: 0.6295 - val_acc: 0.6543\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5531 - acc: 0.7215\n","Epoch 6: val_loss did not improve from 0.61622\n","238/238 [==============================] - 15s 65ms/step - loss: 0.5531 - acc: 0.7215 - val_loss: 0.6248 - val_acc: 0.6627\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5184 - acc: 0.7457\n","Epoch 7: val_loss did not improve from 0.61622\n","238/238 [==============================] - 15s 63ms/step - loss: 0.5184 - acc: 0.7457 - val_loss: 0.6599 - val_acc: 0.6534\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4782 - acc: 0.7730\n","Epoch 8: val_loss did not improve from 0.61622\n","238/238 [==============================] - 15s 64ms/step - loss: 0.4782 - acc: 0.7730 - val_loss: 0.6868 - val_acc: 0.6451\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4261 - acc: 0.8052\n","Epoch 9: val_loss did not improve from 0.61622\n","238/238 [==============================] - 15s 63ms/step - loss: 0.4261 - acc: 0.8052 - val_loss: 0.7536 - val_acc: 0.6534\n","0.7040238591584553 0.6487868695742929 0.656163753449862 0.5734664764621968\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2934 - acc: 0.9169\n","Epoch 1: val_loss improved from inf to 0.27108, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 18s 64ms/step - loss: 0.2934 - acc: 0.9169 - val_loss: 0.2711 - val_acc: 0.9233\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2577 - acc: 0.9214\n","Epoch 2: val_loss improved from 0.27108 to 0.26148, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 15s 64ms/step - loss: 0.2577 - acc: 0.9214 - val_loss: 0.2615 - val_acc: 0.9238\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2441 - acc: 0.9224\n","Epoch 3: val_loss did not improve from 0.26148\n","238/238 [==============================] - 15s 63ms/step - loss: 0.2441 - acc: 0.9224 - val_loss: 0.2617 - val_acc: 0.9242\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2336 - acc: 0.9227\n","Epoch 4: val_loss did not improve from 0.26148\n","238/238 [==============================] - 15s 64ms/step - loss: 0.2336 - acc: 0.9227 - val_loss: 0.2616 - val_acc: 0.9238\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2224 - acc: 0.9236\n","Epoch 5: val_loss did not improve from 0.26148\n","238/238 [==============================] - 15s 63ms/step - loss: 0.2224 - acc: 0.9236 - val_loss: 0.2679 - val_acc: 0.9251\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2055 - acc: 0.9298\n","Epoch 6: val_loss did not improve from 0.26148\n","238/238 [==============================] - 15s 63ms/step - loss: 0.2055 - acc: 0.9298 - val_loss: 0.2778 - val_acc: 0.9159\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1849 - acc: 0.9378\n","Epoch 7: val_loss did not improve from 0.26148\n","238/238 [==============================] - 15s 62ms/step - loss: 0.1849 - acc: 0.9378 - val_loss: 0.3012 - val_acc: 0.9145\n","0.7296686068766748 0.20534374451366577 0.9176632934682613 0.011049723756906077\n","Iteration number:  6\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2903 - acc: 0.9010\n","Epoch 1: val_loss improved from inf to 0.24425, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 18s 65ms/step - loss: 0.2903 - acc: 0.9010 - val_loss: 0.2443 - val_acc: 0.9122\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2416 - acc: 0.9141\n","Epoch 2: val_loss improved from 0.24425 to 0.23867, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 15s 65ms/step - loss: 0.2416 - acc: 0.9141 - val_loss: 0.2387 - val_acc: 0.9104\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2216 - acc: 0.9188\n","Epoch 3: val_loss improved from 0.23867 to 0.23584, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 15s 64ms/step - loss: 0.2216 - acc: 0.9188 - val_loss: 0.2358 - val_acc: 0.9168\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2092 - acc: 0.9233\n","Epoch 4: val_loss did not improve from 0.23584\n","238/238 [==============================] - 15s 63ms/step - loss: 0.2092 - acc: 0.9233 - val_loss: 0.2385 - val_acc: 0.9164\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1940 - acc: 0.9299\n","Epoch 5: val_loss did not improve from 0.23584\n","238/238 [==============================] - 15s 62ms/step - loss: 0.1940 - acc: 0.9299 - val_loss: 0.2367 - val_acc: 0.9150\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1753 - acc: 0.9363\n","Epoch 6: val_loss did not improve from 0.23584\n","238/238 [==============================] - 15s 63ms/step - loss: 0.1753 - acc: 0.9363 - val_loss: 0.2456 - val_acc: 0.9136\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1540 - acc: 0.9445\n","Epoch 7: val_loss did not improve from 0.23584\n","238/238 [==============================] - 15s 63ms/step - loss: 0.1540 - acc: 0.9445 - val_loss: 0.2577 - val_acc: 0.9113\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1234 - acc: 0.9553\n","Epoch 8: val_loss did not improve from 0.23584\n","238/238 [==============================] - 15s 62ms/step - loss: 0.1234 - acc: 0.9553 - val_loss: 0.2965 - val_acc: 0.8965\n","0.8652574015331748 0.5544666345177608 0.9130634774609016 0.4408284023668638\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2142 - acc: 0.9355\n","Epoch 1: val_loss improved from inf to 0.17430, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 19s 69ms/step - loss: 0.2142 - acc: 0.9355 - val_loss: 0.1743 - val_acc: 0.9450\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1753 - acc: 0.9430\n","Epoch 2: val_loss improved from 0.17430 to 0.17084, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 15s 63ms/step - loss: 0.1753 - acc: 0.9430 - val_loss: 0.1708 - val_acc: 0.9427\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1599 - acc: 0.9470\n","Epoch 3: val_loss did not improve from 0.17084\n","238/238 [==============================] - 15s 65ms/step - loss: 0.1599 - acc: 0.9470 - val_loss: 0.1749 - val_acc: 0.9395\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1490 - acc: 0.9480\n","Epoch 4: val_loss improved from 0.17084 to 0.16681, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 65ms/step - loss: 0.1490 - acc: 0.9480 - val_loss: 0.1668 - val_acc: 0.9409\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1369 - acc: 0.9525\n","Epoch 5: val_loss did not improve from 0.16681\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1369 - acc: 0.9525 - val_loss: 0.1707 - val_acc: 0.9381\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1203 - acc: 0.9569\n","Epoch 6: val_loss did not improve from 0.16681\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1203 - acc: 0.9569 - val_loss: 0.1862 - val_acc: 0.9395\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1007 - acc: 0.9645\n","Epoch 7: val_loss did not improve from 0.16681\n","238/238 [==============================] - 15s 63ms/step - loss: 0.1007 - acc: 0.9645 - val_loss: 0.1915 - val_acc: 0.9372\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0736 - acc: 0.9753\n","Epoch 8: val_loss did not improve from 0.16681\n","238/238 [==============================] - 15s 63ms/step - loss: 0.0736 - acc: 0.9753 - val_loss: 0.2172 - val_acc: 0.9395\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0558 - acc: 0.9826\n","Epoch 9: val_loss did not improve from 0.16681\n","238/238 [==============================] - 14s 61ms/step - loss: 0.0558 - acc: 0.9826 - val_loss: 0.2673 - val_acc: 0.9298\n","0.8809151489614059 0.5167828317699134 0.9406623735050598 0.4342105263157895\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6839 - acc: 0.6211\n","Epoch 1: val_loss improved from inf to 0.62940, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 18s 64ms/step - loss: 0.6839 - acc: 0.6211 - val_loss: 0.6294 - val_acc: 0.6687\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6268 - acc: 0.6644\n","Epoch 2: val_loss improved from 0.62940 to 0.62072, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 15s 63ms/step - loss: 0.6268 - acc: 0.6644 - val_loss: 0.6207 - val_acc: 0.6751\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6075 - acc: 0.6802\n","Epoch 3: val_loss did not improve from 0.62072\n","238/238 [==============================] - 15s 64ms/step - loss: 0.6075 - acc: 0.6802 - val_loss: 0.6242 - val_acc: 0.6733\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5927 - acc: 0.6918\n","Epoch 4: val_loss improved from 0.62072 to 0.61848, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 15s 62ms/step - loss: 0.5927 - acc: 0.6918 - val_loss: 0.6185 - val_acc: 0.6779\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5735 - acc: 0.7087\n","Epoch 5: val_loss did not improve from 0.61848\n","238/238 [==============================] - 15s 61ms/step - loss: 0.5735 - acc: 0.7087 - val_loss: 0.6363 - val_acc: 0.6585\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5513 - acc: 0.7238\n","Epoch 6: val_loss did not improve from 0.61848\n","238/238 [==============================] - 15s 62ms/step - loss: 0.5513 - acc: 0.7238 - val_loss: 0.6317 - val_acc: 0.6590\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5205 - acc: 0.7487\n","Epoch 7: val_loss did not improve from 0.61848\n","238/238 [==============================] - 15s 64ms/step - loss: 0.5205 - acc: 0.7487 - val_loss: 0.6523 - val_acc: 0.6548\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4769 - acc: 0.7732\n","Epoch 8: val_loss did not improve from 0.61848\n","238/238 [==============================] - 15s 64ms/step - loss: 0.4769 - acc: 0.7732 - val_loss: 0.6629 - val_acc: 0.6520\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4157 - acc: 0.8124\n","Epoch 9: val_loss did not improve from 0.61848\n","238/238 [==============================] - 15s 62ms/step - loss: 0.4157 - acc: 0.8124 - val_loss: 0.7569 - val_acc: 0.6359\n","0.7010350981686974 0.645489688750196 0.6621435142594296 0.5665388020064915\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2851 - acc: 0.9196\n","Epoch 1: val_loss improved from inf to 0.26503, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 18s 67ms/step - loss: 0.2851 - acc: 0.9196 - val_loss: 0.2650 - val_acc: 0.9247\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2543 - acc: 0.9218\n","Epoch 2: val_loss improved from 0.26503 to 0.26462, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2543 - acc: 0.9218 - val_loss: 0.2646 - val_acc: 0.9242\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2440 - acc: 0.9226\n","Epoch 3: val_loss improved from 0.26462 to 0.25709, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2440 - acc: 0.9226 - val_loss: 0.2571 - val_acc: 0.9251\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2335 - acc: 0.9233\n","Epoch 4: val_loss did not improve from 0.25709\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2335 - acc: 0.9233 - val_loss: 0.2591 - val_acc: 0.9261\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2211 - acc: 0.9254\n","Epoch 5: val_loss did not improve from 0.25709\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2211 - acc: 0.9254 - val_loss: 0.2691 - val_acc: 0.9238\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2052 - acc: 0.9288\n","Epoch 6: val_loss did not improve from 0.25709\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2052 - acc: 0.9288 - val_loss: 0.2734 - val_acc: 0.9187\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1823 - acc: 0.9356\n","Epoch 7: val_loss did not improve from 0.25709\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1823 - acc: 0.9356 - val_loss: 0.2894 - val_acc: 0.9187\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1503 - acc: 0.9475\n","Epoch 8: val_loss did not improve from 0.25709\n","238/238 [==============================] - 16s 65ms/step - loss: 0.1503 - acc: 0.9475 - val_loss: 0.3389 - val_acc: 0.9113\n","0.7305932088896895 0.22443356749698543 0.9190432382704692 0.02222222222222222\n","Iteration number:  7\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2864 - acc: 0.9016\n","Epoch 1: val_loss improved from inf to 0.25921, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 18s 66ms/step - loss: 0.2864 - acc: 0.9016 - val_loss: 0.2592 - val_acc: 0.9039\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2396 - acc: 0.9146\n","Epoch 2: val_loss improved from 0.25921 to 0.22960, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 15s 65ms/step - loss: 0.2396 - acc: 0.9146 - val_loss: 0.2296 - val_acc: 0.9191\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2244 - acc: 0.9183\n","Epoch 3: val_loss did not improve from 0.22960\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2244 - acc: 0.9183 - val_loss: 0.2332 - val_acc: 0.9173\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2083 - acc: 0.9249\n","Epoch 4: val_loss did not improve from 0.22960\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2083 - acc: 0.9249 - val_loss: 0.2347 - val_acc: 0.9136\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1932 - acc: 0.9306\n","Epoch 5: val_loss did not improve from 0.22960\n","238/238 [==============================] - 15s 65ms/step - loss: 0.1932 - acc: 0.9306 - val_loss: 0.2445 - val_acc: 0.9099\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1736 - acc: 0.9366\n","Epoch 6: val_loss did not improve from 0.22960\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1736 - acc: 0.9366 - val_loss: 0.2593 - val_acc: 0.9043\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1485 - acc: 0.9461\n","Epoch 7: val_loss did not improve from 0.22960\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1485 - acc: 0.9461 - val_loss: 0.2764 - val_acc: 0.8937\n","0.8800929597321351 0.5890307836433503 0.9165133394664213 0.45248868778280543\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2149 - acc: 0.9336\n","Epoch 1: val_loss improved from inf to 0.18143, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 18s 66ms/step - loss: 0.2149 - acc: 0.9336 - val_loss: 0.1814 - val_acc: 0.9422\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1748 - acc: 0.9428\n","Epoch 2: val_loss improved from 0.18143 to 0.17047, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 15s 63ms/step - loss: 0.1748 - acc: 0.9428 - val_loss: 0.1705 - val_acc: 0.9409\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1569 - acc: 0.9480\n","Epoch 3: val_loss did not improve from 0.17047\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1569 - acc: 0.9480 - val_loss: 0.1720 - val_acc: 0.9404\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1449 - acc: 0.9509\n","Epoch 4: val_loss did not improve from 0.17047\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1449 - acc: 0.9509 - val_loss: 0.1758 - val_acc: 0.9376\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1337 - acc: 0.9515\n","Epoch 5: val_loss did not improve from 0.17047\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1337 - acc: 0.9515 - val_loss: 0.1715 - val_acc: 0.9422\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1176 - acc: 0.9590\n","Epoch 6: val_loss did not improve from 0.17047\n","238/238 [==============================] - 15s 63ms/step - loss: 0.1176 - acc: 0.9590 - val_loss: 0.1852 - val_acc: 0.9335\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0992 - acc: 0.9656\n","Epoch 7: val_loss did not improve from 0.17047\n","238/238 [==============================] - 15s 63ms/step - loss: 0.0992 - acc: 0.9656 - val_loss: 0.1997 - val_acc: 0.9390\n","0.8809689904333848 0.5202377479337232 0.9415823367065317 0.41203703703703703\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6822 - acc: 0.6236\n","Epoch 1: val_loss improved from inf to 0.62821, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 18s 66ms/step - loss: 0.6822 - acc: 0.6236 - val_loss: 0.6282 - val_acc: 0.6742\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6287 - acc: 0.6637\n","Epoch 2: val_loss improved from 0.62821 to 0.62215, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 66ms/step - loss: 0.6287 - acc: 0.6637 - val_loss: 0.6222 - val_acc: 0.6673\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6081 - acc: 0.6807\n","Epoch 3: val_loss improved from 0.62215 to 0.61452, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 15s 63ms/step - loss: 0.6081 - acc: 0.6807 - val_loss: 0.6145 - val_acc: 0.6742\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5932 - acc: 0.6943\n","Epoch 4: val_loss improved from 0.61452 to 0.61330, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 15s 65ms/step - loss: 0.5932 - acc: 0.6943 - val_loss: 0.6133 - val_acc: 0.6756\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5738 - acc: 0.7096\n","Epoch 5: val_loss did not improve from 0.61330\n","238/238 [==============================] - 15s 64ms/step - loss: 0.5738 - acc: 0.7096 - val_loss: 0.6222 - val_acc: 0.6654\n","Epoch 6/100\n","237/238 [============================>.] - ETA: 0s - loss: 0.5519 - acc: 0.7236\n","Epoch 6: val_loss did not improve from 0.61330\n","238/238 [==============================] - 15s 65ms/step - loss: 0.5519 - acc: 0.7236 - val_loss: 0.6312 - val_acc: 0.6585\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5199 - acc: 0.7483\n","Epoch 7: val_loss did not improve from 0.61330\n","238/238 [==============================] - 16s 65ms/step - loss: 0.5199 - acc: 0.7483 - val_loss: 0.6519 - val_acc: 0.6650\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4800 - acc: 0.7765\n","Epoch 8: val_loss did not improve from 0.61330\n","238/238 [==============================] - 16s 66ms/step - loss: 0.4800 - acc: 0.7765 - val_loss: 0.6844 - val_acc: 0.6511\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4291 - acc: 0.8046\n","Epoch 9: val_loss did not improve from 0.61330\n","238/238 [==============================] - 15s 64ms/step - loss: 0.4291 - acc: 0.8046 - val_loss: 0.7233 - val_acc: 0.6451\n","0.7001663839649881 0.6459270993121577 0.6642134314627415 0.5633971291866029\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2868 - acc: 0.9194\n","Epoch 1: val_loss improved from inf to 0.27293, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 19s 67ms/step - loss: 0.2868 - acc: 0.9194 - val_loss: 0.2729 - val_acc: 0.9247\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2573 - acc: 0.9216\n","Epoch 2: val_loss improved from 0.27293 to 0.26817, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2573 - acc: 0.9216 - val_loss: 0.2682 - val_acc: 0.9247\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2432 - acc: 0.9219\n","Epoch 3: val_loss improved from 0.26817 to 0.25937, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 65ms/step - loss: 0.2432 - acc: 0.9219 - val_loss: 0.2594 - val_acc: 0.9247\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2335 - acc: 0.9230\n","Epoch 4: val_loss did not improve from 0.25937\n","238/238 [==============================] - 15s 65ms/step - loss: 0.2335 - acc: 0.9230 - val_loss: 0.2598 - val_acc: 0.9242\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2227 - acc: 0.9245\n","Epoch 5: val_loss did not improve from 0.25937\n","238/238 [==============================] - 15s 64ms/step - loss: 0.2227 - acc: 0.9245 - val_loss: 0.2686 - val_acc: 0.9214\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2072 - acc: 0.9274\n","Epoch 6: val_loss did not improve from 0.25937\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2072 - acc: 0.9274 - val_loss: 0.2756 - val_acc: 0.9224\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1859 - acc: 0.9341\n","Epoch 7: val_loss did not improve from 0.25937\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1859 - acc: 0.9341 - val_loss: 0.2884 - val_acc: 0.9205\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1552 - acc: 0.9461\n","Epoch 8: val_loss did not improve from 0.25937\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1552 - acc: 0.9461 - val_loss: 0.3287 - val_acc: 0.9173\n","0.7094747782081016 0.1988710134093584 0.9181232750689973 0.01111111111111111\n","Iteration number:  8\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2903 - acc: 0.9012\n","Epoch 1: val_loss improved from inf to 0.24299, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.2903 - acc: 0.9012 - val_loss: 0.2430 - val_acc: 0.9113\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2422 - acc: 0.9144\n","Epoch 2: val_loss did not improve from 0.24299\n","238/238 [==============================] - 15s 65ms/step - loss: 0.2422 - acc: 0.9144 - val_loss: 0.2436 - val_acc: 0.9104\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2243 - acc: 0.9198\n","Epoch 3: val_loss improved from 0.24299 to 0.23874, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 65ms/step - loss: 0.2243 - acc: 0.9198 - val_loss: 0.2387 - val_acc: 0.9136\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2090 - acc: 0.9229\n","Epoch 4: val_loss improved from 0.23874 to 0.23152, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2090 - acc: 0.9229 - val_loss: 0.2315 - val_acc: 0.9173\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1937 - acc: 0.9290\n","Epoch 5: val_loss did not improve from 0.23152\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1937 - acc: 0.9290 - val_loss: 0.2397 - val_acc: 0.9131\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1728 - acc: 0.9361\n","Epoch 6: val_loss did not improve from 0.23152\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1728 - acc: 0.9361 - val_loss: 0.2590 - val_acc: 0.9090\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1521 - acc: 0.9421\n","Epoch 7: val_loss did not improve from 0.23152\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1521 - acc: 0.9421 - val_loss: 0.2955 - val_acc: 0.9011\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1228 - acc: 0.9557\n","Epoch 8: val_loss did not improve from 0.23152\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1228 - acc: 0.9557 - val_loss: 0.2991 - val_acc: 0.9039\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0934 - acc: 0.9668\n","Epoch 9: val_loss did not improve from 0.23152\n","238/238 [==============================] - 16s 66ms/step - loss: 0.0934 - acc: 0.9668 - val_loss: 0.3550 - val_acc: 0.8854\n","0.8724562736805005 0.5764856231126402 0.9139834406623735 0.45\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2319 - acc: 0.9290\n","Epoch 1: val_loss improved from inf to 0.18910, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 19s 68ms/step - loss: 0.2319 - acc: 0.9290 - val_loss: 0.1891 - val_acc: 0.9422\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1808 - acc: 0.9429\n","Epoch 2: val_loss improved from 0.18910 to 0.18210, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1808 - acc: 0.9429 - val_loss: 0.1821 - val_acc: 0.9459\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1637 - acc: 0.9455\n","Epoch 3: val_loss improved from 0.18210 to 0.16642, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 65ms/step - loss: 0.1637 - acc: 0.9455 - val_loss: 0.1664 - val_acc: 0.9436\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1492 - acc: 0.9491\n","Epoch 4: val_loss did not improve from 0.16642\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1492 - acc: 0.9491 - val_loss: 0.1765 - val_acc: 0.9376\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1374 - acc: 0.9522\n","Epoch 5: val_loss did not improve from 0.16642\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1374 - acc: 0.9522 - val_loss: 0.1669 - val_acc: 0.9413\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1196 - acc: 0.9587\n","Epoch 6: val_loss did not improve from 0.16642\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1196 - acc: 0.9587 - val_loss: 0.1890 - val_acc: 0.9381\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1033 - acc: 0.9646\n","Epoch 7: val_loss did not improve from 0.16642\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1033 - acc: 0.9646 - val_loss: 0.1963 - val_acc: 0.9395\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0810 - acc: 0.9726\n","Epoch 8: val_loss did not improve from 0.16642\n","238/238 [==============================] - 16s 67ms/step - loss: 0.0810 - acc: 0.9726 - val_loss: 0.2204 - val_acc: 0.9335\n","0.8848783494857749 0.5254608726570885 0.9422723091076357 0.4555314533622559\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6891 - acc: 0.6186\n","Epoch 1: val_loss improved from inf to 0.63203, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.6891 - acc: 0.6186 - val_loss: 0.6320 - val_acc: 0.6636\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6287 - acc: 0.6644\n","Epoch 2: val_loss improved from 0.63203 to 0.62684, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.6287 - acc: 0.6644 - val_loss: 0.6268 - val_acc: 0.6751\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6091 - acc: 0.6813\n","Epoch 3: val_loss improved from 0.62684 to 0.61849, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.6091 - acc: 0.6813 - val_loss: 0.6185 - val_acc: 0.6687\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5929 - acc: 0.6922\n","Epoch 4: val_loss improved from 0.61849 to 0.61679, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 15s 64ms/step - loss: 0.5929 - acc: 0.6922 - val_loss: 0.6168 - val_acc: 0.6691\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5757 - acc: 0.7075\n","Epoch 5: val_loss improved from 0.61679 to 0.61352, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 15s 65ms/step - loss: 0.5757 - acc: 0.7075 - val_loss: 0.6135 - val_acc: 0.6751\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5505 - acc: 0.7282\n","Epoch 6: val_loss did not improve from 0.61352\n","238/238 [==============================] - 16s 68ms/step - loss: 0.5505 - acc: 0.7282 - val_loss: 0.6344 - val_acc: 0.6590\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5221 - acc: 0.7481\n","Epoch 7: val_loss did not improve from 0.61352\n","238/238 [==============================] - 16s 66ms/step - loss: 0.5221 - acc: 0.7481 - val_loss: 0.6449 - val_acc: 0.6645\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4792 - acc: 0.7784\n","Epoch 8: val_loss did not improve from 0.61352\n","238/238 [==============================] - 16s 67ms/step - loss: 0.4792 - acc: 0.7784 - val_loss: 0.6806 - val_acc: 0.6474\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4188 - acc: 0.8114\n","Epoch 9: val_loss did not improve from 0.61352\n","238/238 [==============================] - 16s 68ms/step - loss: 0.4188 - acc: 0.8114 - val_loss: 0.7561 - val_acc: 0.6262\n","Epoch 10/100\n","238/238 [==============================] - ETA: 0s - loss: 0.3550 - acc: 0.8505\n","Epoch 10: val_loss did not improve from 0.61352\n","238/238 [==============================] - 16s 68ms/step - loss: 0.3550 - acc: 0.8505 - val_loss: 0.7968 - val_acc: 0.6326\n","0.7031155451394733 0.6477194909139746 0.6692732290708372 0.5753101004134672\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2943 - acc: 0.9158\n","Epoch 1: val_loss improved from inf to 0.26537, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 19s 68ms/step - loss: 0.2943 - acc: 0.9158 - val_loss: 0.2654 - val_acc: 0.9247\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2567 - acc: 0.9217\n","Epoch 2: val_loss did not improve from 0.26537\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2567 - acc: 0.9217 - val_loss: 0.2673 - val_acc: 0.9233\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2456 - acc: 0.9226\n","Epoch 3: val_loss did not improve from 0.26537\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2456 - acc: 0.9226 - val_loss: 0.2655 - val_acc: 0.9251\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2342 - acc: 0.9229\n","Epoch 4: val_loss improved from 0.26537 to 0.26096, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2342 - acc: 0.9229 - val_loss: 0.2610 - val_acc: 0.9247\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2219 - acc: 0.9246\n","Epoch 5: val_loss did not improve from 0.26096\n","238/238 [==============================] - 16s 65ms/step - loss: 0.2219 - acc: 0.9246 - val_loss: 0.2686 - val_acc: 0.9219\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2058 - acc: 0.9284\n","Epoch 6: val_loss did not improve from 0.26096\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2058 - acc: 0.9284 - val_loss: 0.2672 - val_acc: 0.9233\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1878 - acc: 0.9347\n","Epoch 7: val_loss did not improve from 0.26096\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1878 - acc: 0.9347 - val_loss: 0.2954 - val_acc: 0.9090\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1583 - acc: 0.9457\n","Epoch 8: val_loss did not improve from 0.26096\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1583 - acc: 0.9457 - val_loss: 0.3165 - val_acc: 0.8979\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1244 - acc: 0.9585\n","Epoch 9: val_loss did not improve from 0.26096\n","238/238 [==============================] - 16s 65ms/step - loss: 0.1244 - acc: 0.9585 - val_loss: 0.3462 - val_acc: 0.8965\n","0.7296158327891739 0.21243453901011655 0.9183532658693653 0.011142061281337047\n","Iteration number:  9\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2916 - acc: 0.9013\n","Epoch 1: val_loss improved from inf to 0.24864, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 18s 66ms/step - loss: 0.2916 - acc: 0.9013 - val_loss: 0.2486 - val_acc: 0.9145\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2398 - acc: 0.9152\n","Epoch 2: val_loss improved from 0.24864 to 0.23102, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2398 - acc: 0.9152 - val_loss: 0.2310 - val_acc: 0.9201\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2222 - acc: 0.9198\n","Epoch 3: val_loss did not improve from 0.23102\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2222 - acc: 0.9198 - val_loss: 0.2345 - val_acc: 0.9210\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2100 - acc: 0.9251\n","Epoch 4: val_loss did not improve from 0.23102\n","238/238 [==============================] - 15s 65ms/step - loss: 0.2100 - acc: 0.9251 - val_loss: 0.2328 - val_acc: 0.9131\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1954 - acc: 0.9275\n","Epoch 5: val_loss did not improve from 0.23102\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1954 - acc: 0.9275 - val_loss: 0.2364 - val_acc: 0.9168\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1765 - acc: 0.9342\n","Epoch 6: val_loss did not improve from 0.23102\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1765 - acc: 0.9342 - val_loss: 0.2533 - val_acc: 0.9168\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1546 - acc: 0.9435\n","Epoch 7: val_loss did not improve from 0.23102\n","238/238 [==============================] - 16s 68ms/step - loss: 0.1546 - acc: 0.9435 - val_loss: 0.2706 - val_acc: 0.9191\n","0.8724303903427615 0.5720617223168813 0.9146734130634775 0.4082934609250399\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2139 - acc: 0.9353\n","Epoch 1: val_loss improved from inf to 0.18576, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 19s 68ms/step - loss: 0.2139 - acc: 0.9353 - val_loss: 0.1858 - val_acc: 0.9432\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1744 - acc: 0.9424\n","Epoch 2: val_loss improved from 0.18576 to 0.17101, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1744 - acc: 0.9424 - val_loss: 0.1710 - val_acc: 0.9409\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1589 - acc: 0.9467\n","Epoch 3: val_loss improved from 0.17101 to 0.16718, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1589 - acc: 0.9467 - val_loss: 0.1672 - val_acc: 0.9413\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1470 - acc: 0.9491\n","Epoch 4: val_loss did not improve from 0.16718\n","238/238 [==============================] - 15s 63ms/step - loss: 0.1470 - acc: 0.9491 - val_loss: 0.1799 - val_acc: 0.9390\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1344 - acc: 0.9531\n","Epoch 5: val_loss did not improve from 0.16718\n","238/238 [==============================] - 15s 65ms/step - loss: 0.1344 - acc: 0.9531 - val_loss: 0.1803 - val_acc: 0.9418\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1188 - acc: 0.9592\n","Epoch 6: val_loss did not improve from 0.16718\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1188 - acc: 0.9592 - val_loss: 0.1808 - val_acc: 0.9445\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0978 - acc: 0.9673\n","Epoch 7: val_loss did not improve from 0.16718\n","238/238 [==============================] - 15s 65ms/step - loss: 0.0978 - acc: 0.9673 - val_loss: 0.2063 - val_acc: 0.9316\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0762 - acc: 0.9748\n","Epoch 8: val_loss did not improve from 0.16718\n","238/238 [==============================] - 16s 66ms/step - loss: 0.0762 - acc: 0.9748 - val_loss: 0.2229 - val_acc: 0.9293\n","0.8796518251478689 0.5185288660634386 0.9415823367065317 0.4279279279279279\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6783 - acc: 0.6176\n","Epoch 1: val_loss improved from inf to 0.63064, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 19s 69ms/step - loss: 0.6783 - acc: 0.6176 - val_loss: 0.6306 - val_acc: 0.6654\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6283 - acc: 0.6629\n","Epoch 2: val_loss improved from 0.63064 to 0.62471, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 15s 65ms/step - loss: 0.6283 - acc: 0.6629 - val_loss: 0.6247 - val_acc: 0.6677\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6080 - acc: 0.6820\n","Epoch 3: val_loss improved from 0.62471 to 0.61479, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 15s 65ms/step - loss: 0.6080 - acc: 0.6820 - val_loss: 0.6148 - val_acc: 0.6738\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5922 - acc: 0.6971\n","Epoch 4: val_loss did not improve from 0.61479\n","238/238 [==============================] - 16s 66ms/step - loss: 0.5922 - acc: 0.6971 - val_loss: 0.6191 - val_acc: 0.6696\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5747 - acc: 0.7092\n","Epoch 5: val_loss did not improve from 0.61479\n","238/238 [==============================] - 15s 65ms/step - loss: 0.5747 - acc: 0.7092 - val_loss: 0.6155 - val_acc: 0.6654\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5540 - acc: 0.7238\n","Epoch 6: val_loss did not improve from 0.61479\n","238/238 [==============================] - 16s 66ms/step - loss: 0.5540 - acc: 0.7238 - val_loss: 0.6295 - val_acc: 0.6668\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5236 - acc: 0.7438\n","Epoch 7: val_loss did not improve from 0.61479\n","238/238 [==============================] - 16s 67ms/step - loss: 0.5236 - acc: 0.7438 - val_loss: 0.6502 - val_acc: 0.6479\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4830 - acc: 0.7700\n","Epoch 8: val_loss did not improve from 0.61479\n","238/238 [==============================] - 16s 68ms/step - loss: 0.4830 - acc: 0.7700 - val_loss: 0.7019 - val_acc: 0.6543\n","0.701304743777326 0.6450020981866645 0.6614535418583257 0.5605970149253732\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2993 - acc: 0.9143\n","Epoch 1: val_loss improved from inf to 0.27188, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 19s 70ms/step - loss: 0.2993 - acc: 0.9143 - val_loss: 0.2719 - val_acc: 0.9251\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2586 - acc: 0.9221\n","Epoch 2: val_loss improved from 0.27188 to 0.26575, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2586 - acc: 0.9221 - val_loss: 0.2658 - val_acc: 0.9242\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2451 - acc: 0.9224\n","Epoch 3: val_loss improved from 0.26575 to 0.26295, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2451 - acc: 0.9224 - val_loss: 0.2630 - val_acc: 0.9228\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2344 - acc: 0.9238\n","Epoch 4: val_loss did not improve from 0.26295\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2344 - acc: 0.9238 - val_loss: 0.2692 - val_acc: 0.9242\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2219 - acc: 0.9259\n","Epoch 5: val_loss improved from 0.26295 to 0.25986, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 66ms/step - loss: 0.2219 - acc: 0.9259 - val_loss: 0.2599 - val_acc: 0.9233\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2071 - acc: 0.9290\n","Epoch 6: val_loss did not improve from 0.25986\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2071 - acc: 0.9290 - val_loss: 0.2745 - val_acc: 0.9136\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1838 - acc: 0.9360\n","Epoch 7: val_loss did not improve from 0.25986\n","238/238 [==============================] - 16s 67ms/step - loss: 0.1838 - acc: 0.9360 - val_loss: 0.2961 - val_acc: 0.9201\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1579 - acc: 0.9455\n","Epoch 8: val_loss did not improve from 0.25986\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1579 - acc: 0.9455 - val_loss: 0.3105 - val_acc: 0.9057\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1250 - acc: 0.9585\n","Epoch 9: val_loss did not improve from 0.25986\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1250 - acc: 0.9585 - val_loss: 0.3556 - val_acc: 0.9025\n","Epoch 10/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0899 - acc: 0.9702\n","Epoch 10: val_loss did not improve from 0.25986\n","238/238 [==============================] - 16s 68ms/step - loss: 0.0899 - acc: 0.9702 - val_loss: 0.4185 - val_acc: 0.9039\n","0.7241663805138367 0.20443394920849162 0.9181232750689973 0.08247422680412371\n","Iteration number:  10\n","Problem type:  mort_hosp\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2857 - acc: 0.9010\n","Epoch 1: val_loss improved from inf to 0.24552, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 19s 69ms/step - loss: 0.2857 - acc: 0.9010 - val_loss: 0.2455 - val_acc: 0.9150\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2389 - acc: 0.9158\n","Epoch 2: val_loss improved from 0.24552 to 0.23818, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2389 - acc: 0.9158 - val_loss: 0.2382 - val_acc: 0.9145\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2219 - acc: 0.9196\n","Epoch 3: val_loss improved from 0.23818 to 0.22818, saving model to avg-fasttext-mort_hosp-best_model.hdf5\n","238/238 [==============================] - 16s 68ms/step - loss: 0.2219 - acc: 0.9196 - val_loss: 0.2282 - val_acc: 0.9187\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2089 - acc: 0.9240\n","Epoch 4: val_loss did not improve from 0.22818\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2089 - acc: 0.9240 - val_loss: 0.2415 - val_acc: 0.9159\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1900 - acc: 0.9296\n","Epoch 5: val_loss did not improve from 0.22818\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1900 - acc: 0.9296 - val_loss: 0.2627 - val_acc: 0.9127\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1694 - acc: 0.9369\n","Epoch 6: val_loss did not improve from 0.22818\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1694 - acc: 0.9369 - val_loss: 0.2582 - val_acc: 0.9062\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1456 - acc: 0.9478\n","Epoch 7: val_loss did not improve from 0.22818\n","238/238 [==============================] - 15s 62ms/step - loss: 0.1456 - acc: 0.9478 - val_loss: 0.2928 - val_acc: 0.9085\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1205 - acc: 0.9568\n","Epoch 8: val_loss did not improve from 0.22818\n","238/238 [==============================] - 15s 62ms/step - loss: 0.1205 - acc: 0.9568 - val_loss: 0.2964 - val_acc: 0.8919\n","0.8746387346902812 0.5723457084458393 0.9144434222631095 0.3779264214046823\n","Problem type:  mort_icu\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2124 - acc: 0.9346\n","Epoch 1: val_loss improved from inf to 0.17374, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 18s 66ms/step - loss: 0.2124 - acc: 0.9346 - val_loss: 0.1737 - val_acc: 0.9427\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1745 - acc: 0.9417\n","Epoch 2: val_loss improved from 0.17374 to 0.17176, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 15s 63ms/step - loss: 0.1745 - acc: 0.9417 - val_loss: 0.1718 - val_acc: 0.9399\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1595 - acc: 0.9470\n","Epoch 3: val_loss improved from 0.17176 to 0.16497, saving model to avg-fasttext-mort_icu-best_model.hdf5\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1595 - acc: 0.9470 - val_loss: 0.1650 - val_acc: 0.9427\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1492 - acc: 0.9486\n","Epoch 4: val_loss did not improve from 0.16497\n","238/238 [==============================] - 16s 65ms/step - loss: 0.1492 - acc: 0.9486 - val_loss: 0.1761 - val_acc: 0.9367\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1335 - acc: 0.9547\n","Epoch 5: val_loss did not improve from 0.16497\n","238/238 [==============================] - 16s 65ms/step - loss: 0.1335 - acc: 0.9547 - val_loss: 0.1719 - val_acc: 0.9395\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1189 - acc: 0.9590\n","Epoch 6: val_loss did not improve from 0.16497\n","238/238 [==============================] - 15s 64ms/step - loss: 0.1189 - acc: 0.9590 - val_loss: 0.1885 - val_acc: 0.9372\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0969 - acc: 0.9666\n","Epoch 7: val_loss did not improve from 0.16497\n","238/238 [==============================] - 15s 64ms/step - loss: 0.0969 - acc: 0.9666 - val_loss: 0.1964 - val_acc: 0.9358\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.0751 - acc: 0.9748\n","Epoch 8: val_loss did not improve from 0.16497\n","238/238 [==============================] - 16s 66ms/step - loss: 0.0751 - acc: 0.9748 - val_loss: 0.2319 - val_acc: 0.9362\n","0.8819030229255428 0.5278645121786731 0.9427322907083716 0.4503311258278146\n","Problem type:  los_3\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6825 - acc: 0.6232\n","Epoch 1: val_loss improved from inf to 0.63223, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 19s 68ms/step - loss: 0.6825 - acc: 0.6232 - val_loss: 0.6322 - val_acc: 0.6728\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.6270 - acc: 0.6642\n","Epoch 2: val_loss improved from 0.63223 to 0.61725, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 67ms/step - loss: 0.6270 - acc: 0.6642 - val_loss: 0.6173 - val_acc: 0.6802\n","Epoch 3/100\n","237/238 [============================>.] - ETA: 0s - loss: 0.6075 - acc: 0.6813\n","Epoch 3: val_loss did not improve from 0.61725\n","238/238 [==============================] - 16s 66ms/step - loss: 0.6075 - acc: 0.6815 - val_loss: 0.6205 - val_acc: 0.6733\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5927 - acc: 0.6930\n","Epoch 4: val_loss did not improve from 0.61725\n","238/238 [==============================] - 15s 65ms/step - loss: 0.5927 - acc: 0.6930 - val_loss: 0.6365 - val_acc: 0.6474\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5772 - acc: 0.7031\n","Epoch 5: val_loss improved from 0.61725 to 0.61718, saving model to avg-fasttext-los_3-best_model.hdf5\n","238/238 [==============================] - 16s 69ms/step - loss: 0.5772 - acc: 0.7031 - val_loss: 0.6172 - val_acc: 0.6613\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5549 - acc: 0.7211\n","Epoch 6: val_loss did not improve from 0.61718\n","238/238 [==============================] - 15s 64ms/step - loss: 0.5549 - acc: 0.7211 - val_loss: 0.6320 - val_acc: 0.6580\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.5258 - acc: 0.7439\n","Epoch 7: val_loss did not improve from 0.61718\n","238/238 [==============================] - 16s 66ms/step - loss: 0.5258 - acc: 0.7439 - val_loss: 0.6471 - val_acc: 0.6590\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4802 - acc: 0.7745\n","Epoch 8: val_loss did not improve from 0.61718\n","238/238 [==============================] - 16s 67ms/step - loss: 0.4802 - acc: 0.7745 - val_loss: 0.6753 - val_acc: 0.6470\n","Epoch 9/100\n","238/238 [==============================] - ETA: 0s - loss: 0.4284 - acc: 0.8064\n","Epoch 9: val_loss did not improve from 0.61718\n","238/238 [==============================] - 16s 67ms/step - loss: 0.4284 - acc: 0.8064 - val_loss: 0.7381 - val_acc: 0.6474\n","Epoch 10/100\n","238/238 [==============================] - ETA: 0s - loss: 0.3662 - acc: 0.8442\n","Epoch 10: val_loss did not improve from 0.61718\n","238/238 [==============================] - 16s 66ms/step - loss: 0.3662 - acc: 0.8442 - val_loss: 0.8122 - val_acc: 0.6460\n","0.700726176449008 0.6444025904433162 0.6621435142594296 0.5433633820329499\n","Problem type:  los_7\n","__________________\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2904 - acc: 0.9156\n","Epoch 1: val_loss improved from inf to 0.26270, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 20s 72ms/step - loss: 0.2904 - acc: 0.9156 - val_loss: 0.2627 - val_acc: 0.9233\n","Epoch 2/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2558 - acc: 0.9217\n","Epoch 2: val_loss did not improve from 0.26270\n","238/238 [==============================] - 16s 67ms/step - loss: 0.2558 - acc: 0.9217 - val_loss: 0.2651 - val_acc: 0.9242\n","Epoch 3/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2443 - acc: 0.9228\n","Epoch 3: val_loss improved from 0.26270 to 0.25637, saving model to avg-fasttext-los_7-best_model.hdf5\n","238/238 [==============================] - 16s 65ms/step - loss: 0.2443 - acc: 0.9228 - val_loss: 0.2564 - val_acc: 0.9242\n","Epoch 4/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2337 - acc: 0.9233\n","Epoch 4: val_loss did not improve from 0.25637\n","238/238 [==============================] - 16s 65ms/step - loss: 0.2337 - acc: 0.9233 - val_loss: 0.2579 - val_acc: 0.9242\n","Epoch 5/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2224 - acc: 0.9251\n","Epoch 5: val_loss did not improve from 0.25637\n","238/238 [==============================] - 16s 65ms/step - loss: 0.2224 - acc: 0.9251 - val_loss: 0.2638 - val_acc: 0.9233\n","Epoch 6/100\n","238/238 [==============================] - ETA: 0s - loss: 0.2067 - acc: 0.9275\n","Epoch 6: val_loss did not improve from 0.25637\n","238/238 [==============================] - 16s 65ms/step - loss: 0.2067 - acc: 0.9275 - val_loss: 0.2795 - val_acc: 0.9233\n","Epoch 7/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1876 - acc: 0.9329\n","Epoch 7: val_loss did not improve from 0.25637\n","238/238 [==============================] - 16s 65ms/step - loss: 0.1876 - acc: 0.9329 - val_loss: 0.2840 - val_acc: 0.9219\n","Epoch 8/100\n","238/238 [==============================] - ETA: 0s - loss: 0.1574 - acc: 0.9459\n","Epoch 8: val_loss did not improve from 0.25637\n","238/238 [==============================] - 16s 66ms/step - loss: 0.1574 - acc: 0.9459 - val_loss: 0.3071 - val_acc: 0.9048\n","0.7297030859471753 0.20923166016534533 0.9176632934682613 0.03763440860215054\n"]}],"source":["#embedding_types = ['concat']\n","#embedding_dict = [ner_concat]\n","#embedding_types = ['word2vec', 'fasttext', 'concat']\n","embedding_dict = [ner_word2vec, ner_fasttext, ner_concat]\n","embedding_types = ['word2vec', 'fasttext']\n","#embedding_dict = [ner_word2vec, ner_fasttext]\n","target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n","\n","\n","num_epoch = 100\n","#num_epoch = 2\n","model_patience = 5\n","monitor_criteria = 'val_loss'\n","batch_size = 64\n","iter_num = 11\n","#unit_sizes = [128, 256]\n","unit_sizes = [256]\n","\n","#layers = [\"LSTM\", \"GRU\"]\n","layers = [\"GRU\"]\n","for each_layer in layers:\n","    print (\"Layer: \", each_layer)\n","    for each_unit_size in unit_sizes:\n","        print (\"Hidden unit: \", each_unit_size)\n","\n","        for embed_dict, embed_name in zip(embedding_dict, embedding_types):    \n","            print (\"Embedding: \", embed_name)\n","            print(\"=============================\")\n","\n","            temp_train_ner = dict((k, ner_word2vec[k]) for k in train_ids)\n","            temp_dev_ner = dict((k, ner_word2vec[k]) for k in dev_ids)\n","            temp_test_ner = dict((k, ner_word2vec[k]) for k in test_ids)\n","\n","            x_train_ner = create_dataset(temp_train_ner)\n","            x_dev_ner = create_dataset(temp_dev_ner)\n","            x_test_ner = create_dataset(temp_test_ner)\n","\n","\n","            for iteration in range(1, iter_num):\n","                print (\"Iteration number: \", iteration)\n","\n","                for each_problem in target_problems:\n","                    print (\"Problem type: \", each_problem)\n","                    print (\"__________________\")\n","\n","                    early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n","                    best_model_name = \"avg-\"+str(embed_name)+\"-\"+str(each_problem)+\"-\"+\"best_model.hdf5\"\n","                    checkpoint = ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1,\n","                        save_best_only=True, mode='min', period=1)\n","\n","\n","                    callbacks = [early_stopping_monitor, checkpoint]\n","\n","                    model = avg_ner_model(each_layer, each_unit_size, embed_name)\n","                    \n","                    model.fit([x_train_lstm, x_train_ner], y_train[each_problem], epochs=num_epoch, verbose=1, \n","                              validation_data=([x_dev_lstm, x_dev_ner], y_dev[each_problem]), callbacks=callbacks, \n","                              batch_size=batch_size )\n","\n","                    model.load_weights(best_model_name)\n","\n","                    probs, predictions = make_prediction_multi_avg(model, [x_test_lstm, x_test_ner])\n","                    \n","                    save_scores_multi_avg(predictions, probs, y_test[each_problem], \n","                                embed_name, each_problem, iteration, each_unit_size, \n","                                each_layer, type_of_ner)\n","                    \n","                    reset_keras(model)\n","                    #del model\n","                    clear_session()\n","                    gc.collect()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"05-08-Represent-Entities-With-Different-Embeddings-No-Concat.ipynb","provenance":[{"file_id":"1p7XCSF22Xv0rAUBi5XshXtqDhowvu4ks","timestamp":1649799600818},{"file_id":"1OAgfEwQSo5jSse74gsIH4cv3aVD3RVIw","timestamp":1649619327189}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}