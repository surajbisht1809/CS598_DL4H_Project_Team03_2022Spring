{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KyythSU9lmu1","executionInfo":{"status":"ok","timestamp":1650817793408,"user_tz":300,"elapsed":645,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}},"outputId":"4af28918-8843-4088-e3f7-2be703fc62cd"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["DATAPATH = \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/\"\n","RESULTPATH = \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/results/\""],"metadata":{"id":"LvJrQ5m_U9wP","executionInfo":{"status":"ok","timestamp":1650817793410,"user_tz":300,"elapsed":8,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"pV-5ETnoliPi","executionInfo":{"status":"ok","timestamp":1650817802694,"user_tz":300,"elapsed":9291,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"84b14bd8-54d3-4a93-b138-6dfa3f8c4011"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: glove-python-binary in /usr/local/lib/python3.7/dist-packages (0.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.4.1)\n","Requirement already satisfied: tensorflow==2.2.0 in /usr/local/lib/python3.7/dist-packages (2.2.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.14.0)\n","Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (2.2.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.3.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.17.3)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.4.1)\n","Collecting keras-preprocessing>=1.1.0\n","  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n","Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (2.10.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.44.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n","Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (2.2.2)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.21.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.37.1)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.0.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.6)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.35.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (57.4.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.6)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.0)\n","Installing collected packages: keras-preprocessing\n","  Attempting uninstall: keras-preprocessing\n","    Found existing installation: Keras-Preprocessing 1.0.1\n","    Uninstalling Keras-Preprocessing-1.0.1:\n","      Successfully uninstalled Keras-Preprocessing-1.0.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","keras 2.2.0 requires keras-preprocessing==1.0.1, but you have keras-preprocessing 1.1.2 which is incompatible.\u001b[0m\n","Successfully installed keras-preprocessing-1.1.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["keras_preprocessing"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Keras==2.2.0 in /usr/local/lib/python3.7/dist-packages (2.2.0)\n","Requirement already satisfied: keras-applications==1.0.2 in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.0) (1.0.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.0) (2.10.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.0) (1.4.1)\n","Collecting keras-preprocessing==1.0.1\n","  Using cached Keras_Preprocessing-1.0.1-py2.py3-none-any.whl (26 kB)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.0) (1.21.6)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.0) (1.15.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.0) (3.13)\n","Installing collected packages: keras-preprocessing\n","  Attempting uninstall: keras-preprocessing\n","    Found existing installation: Keras-Preprocessing 1.1.2\n","    Uninstalling Keras-Preprocessing-1.1.2:\n","      Successfully uninstalled Keras-Preprocessing-1.1.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.2.0 requires keras-preprocessing>=1.1.0, but you have keras-preprocessing 1.0.1 which is incompatible.\u001b[0m\n","Successfully installed keras-preprocessing-1.0.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["keras_preprocessing"]}}},"metadata":{}}],"source":["import pandas as pd\n","import os\n","import numpy as np\n","from gensim.models import Word2Vec, FastText\n","#!python3.6\n","!pip install glove-python-binary\n","!pip install tensorflow==2.2.0\n","!pip install Keras==2.2.0\n","import glove\n","from glove import Corpus\n","\n","import collections\n","import gc \n","\n","import keras\n","from keras import backend as K\n","from keras import regularizers\n","from keras.models import Sequential, Model\n","from keras.layers import Flatten, Dense, Dropout, Input, concatenate, merge, Activation, Concatenate, LSTM, GRU\n","from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\n","from keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D, merge\n","\n","#from keras.optimizers import Adam\n","#from tensorflow.keras.optimizers import Adam\n","\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n","from keras.utils import np_utils\n","#from keras.backend.tensorflow_backend import set_session, clear_session, get_session\n","from keras.backend import set_session, clear_session, get_session\n","import tensorflow as tf\n","\n","\n","from sklearn.utils import class_weight\n","from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","source":["lvl2_train =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/lvl2_imputer_train.pkl\")\n","lvl2_dev =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/lvl2_imputer_dev.pkl\")\n","lvl2_test =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/lvl2_imputer_test.pkl\")\n","\n","Ys =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/Ys.pkl\")\n","Ys_train =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/Ys_train.pkl\")\n","Ys_dev =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/Ys_dev.pkl\")\n","Ys_test =  pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/Ys_test.pkl\")"],"metadata":{"id":"LjmhmV0lmQ6S","executionInfo":{"status":"ok","timestamp":1650817810938,"user_tz":300,"elapsed":8263,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["all_train_ids = set()\n","for i in Ys_train.itertuples():\n","    all_train_ids.add( i.Index[0] )\n","    \n","all_dev_ids = set()\n","for i in Ys_dev.itertuples():\n","    all_dev_ids.add( i.Index[0] )\n","    \n","all_test_ids = set()\n","for i in Ys_test.itertuples():\n","    all_test_ids.add( i.Index[0] )"],"metadata":{"id":"eC_0Wy6jmT0e","executionInfo":{"status":"ok","timestamp":1650817812025,"user_tz":300,"elapsed":12,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"xNTzkdVyliPx","executionInfo":{"status":"ok","timestamp":1650817824662,"user_tz":300,"elapsed":12648,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}}},"outputs":[],"source":["type_of_ner = \"new\"\n","\n","x_train_lstm = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_x_train.pkl\")\n","x_dev_lstm = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_x_dev.pkl\")\n","x_test_lstm = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_x_test.pkl\")\n","\n","\n","y_train = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_y_train.pkl\")\n","y_dev = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_y_dev.pkl\")\n","y_test = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_y_test.pkl\")\n","\n","\n","#ner_word2vec = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_word2vec_limited_dict.pkl\")\n","#ner_fasttext = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_fasttext_limited_dict.pkl\")\n","#ner_concat = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_combined_limited_dict.pkl\")\n","\n","ner_word2vec = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner4_word2vec_limited_dict.pkl\")\n","ner_fasttext = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner4_fasttext_limited_dict.pkl\")\n","ner_concat = pd.read_pickle(\"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/data/new_ner4_combined_limited_dict.pkl\")\n","\n","new_keys = set(ner_word2vec.keys())\n","train_ids = sorted(all_train_ids.intersection(new_keys))\n","dev_ids = sorted(all_dev_ids.intersection(new_keys))\n","test_ids = sorted(all_test_ids.intersection(new_keys))\n","\n","#train_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_train_ids.pkl\")\n","#dev_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_dev_ids.pkl\")\n","#test_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_test_ids.pkl\")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"KjjJchEpliP0","executionInfo":{"status":"ok","timestamp":1650817824819,"user_tz":300,"elapsed":188,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}}},"outputs":[],"source":["def make_prediction_cnn(model, test_data):\n","    probs = model.predict(test_data)\n","    y_pred = [1 if i>=0.5 else 0 for i in probs]\n","    return probs, y_pred\n","\n","def save_scores_cnn(predictions, probs, ground_truth, \n","                          \n","                          embed_name, problem_type, iteration, hidden_unit_size,\n","                          \n","                          sequence_name, type_of_ner):\n","    \n","    auc = roc_auc_score(ground_truth, probs)\n","    auprc = average_precision_score(ground_truth, probs)\n","    acc   = accuracy_score(ground_truth, predictions)\n","    F1    = f1_score(ground_truth, predictions)\n","    \n","    result_dict = {}    \n","    result_dict['auc'] = auc\n","    result_dict['auprc'] = auprc\n","    result_dict['acc'] = acc\n","    result_dict['F1'] = F1\n","\n","    result_path = \"/content/drive/MyDrive/Colab Notebooks/ConvolutionMedicalNer-master/results/cnn/\"\n","    file_name = str(sequence_name)+\"-\"+str(hidden_unit_size)+\"-\"+embed_name\n","    file_name = file_name +\"-\"+problem_type+\"-\"+str(iteration)+\"-\"+type_of_ner+\"-cnn-.p\"\n","    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n","\n","    print(auc, auprc, acc, F1)\n","    \n","def print_scores_cnn(predictions, probs, ground_truth, model_name, problem_type, iteration, hidden_unit_size):\n","    auc = roc_auc_score(ground_truth, probs)\n","    auprc = average_precision_score(ground_truth, probs)\n","    acc   = accuracy_score(ground_truth, predictions)\n","    F1    = f1_score(ground_truth, predictions)\n","    \n","    print (\"AUC: \", auc, \"AUPRC: \", auprc, \"F1: \", F1)\n","    \n","def get_subvector_data(size, embed_name, data):\n","    if embed_name == \"concat\":\n","        vector_size = 200\n","    else:\n","        vector_size = 100\n","\n","    x_data = {}\n","    for k, v in data.items():\n","        number_of_additional_vector = len(v) - size\n","        vector = []\n","        for i in v:\n","            vector.append(i)\n","        if number_of_additional_vector < 0: \n","            number_of_additional_vector = np.abs(number_of_additional_vector)\n","\n","            temp = vector[:size]\n","            for i in range(0, number_of_additional_vector):\n","                temp.append(np.zeros(vector_size))\n","            x_data[k] = np.asarray(temp)\n","        else:\n","            x_data[k] = np.asarray(vector[:size])\n","\n","    return x_data\n","\n","\n","def proposedmodel(layer_name, number_of_unit, embedding_name, ner_limit, num_filter):\n","    if embedding_name == \"concat\":\n","        input_dimension = 200\n","    else:\n","        input_dimension = 100\n","\n","    sequence_input = Input(shape=(24,104))\n","\n","    input_img = Input(shape=(ner_limit, input_dimension), name = \"cnn_input\")\n","\n","    convs = []\n","    filter_sizes = [2,3,4]\n","\n","\n","\n","    text_conv1d = Conv1D(filters=num_filter, kernel_size=3, \n","                 padding = 'valid', strides = 1, dilation_rate=1, activation='relu', \n","                         kernel_initializer=tf.keras.initializers.glorot_normal() )(input_img)\n","    \n","    text_conv1d = Conv1D(filters=num_filter*2, kernel_size=3, \n","                 padding = 'valid', strides = 1, dilation_rate=1, activation='relu',\n","                        kernel_initializer=tf.keras.initializers.glorot_normal())(text_conv1d)   \n","    \n","    text_conv1d = Conv1D(filters=num_filter*3, kernel_size=3, \n","                 padding = 'valid', strides = 1, dilation_rate=1, activation='relu',\n","                        kernel_initializer=tf.keras.initializers.glorot_normal())(text_conv1d)   \n","\n","    \n","    #concat_conv = keras.layers.Concatenate()([text_conv1d, text_conv1d_2, text_conv1d_3])\n","    text_embeddings = GlobalMaxPooling1D()(text_conv1d)\n","    #text_embeddings = Dense(128, activation=\"relu\")(text_embeddings)\n","    \n","    if layer_name == \"GRU\":\n","        x = GRU(number_of_unit)(sequence_input)\n","    elif layer_name == \"LSTM\":\n","        x = LSTM(number_of_unit)(sequence_input)\n","\n","    #concatenated = keras.layers.Concatenate()([x, text_embeddings])\n","    concatenated = merge([x, text_embeddings], mode='concat', concat_axis=1)\n","\n","    concatenated = Dense(512, activation='relu')(concatenated)\n","    concatenated = Dropout(0.2)(concatenated)\n","    #concatenated = Dense(256, activation='relu')(concatenated)\n","    #concatenated = Dense(512, activation='relu')(concatenated)\n","    \n","    #concatenated = Dense(512, activation='relu')(concatenated)\n","    #logits_regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n","    logits_regularizer = tf.keras.regularizers.L2(0.01)\n","    preds = Dense(1, activation='sigmoid',use_bias=False,\n","                         kernel_initializer=tf.keras.initializers.glorot_normal(), \n","                  kernel_regularizer=logits_regularizer)(concatenated)\n","    \n","    \n","    #opt = Adam(lr=1e-4, decay = 0.01)\n","    \n","    #opt = Adam(lr=1e-3, decay = 0.01)\n","    \n","    #opt = Adam(lr=0.001)\n","\n","    model = Model(inputs=[sequence_input, input_img], outputs=preds)\n","    model.compile(loss='binary_crossentropy',\n","                  optimizer='adam',\n","                  metrics=['acc'])\n","    \n","    return model\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"id":"xqmdoU1WliP_","executionInfo":{"status":"error","timestamp":1650817827300,"user_tz":300,"elapsed":2488,"user":{"displayName":"Suraj Bisht","userId":"14494261012766270308"}},"outputId":"4b974ba0-6cda-4392-8e69-d5ff27b1ab55"},"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding:  word2vec\n","=============================\n","Iteration number:  1\n","Problem type:  1\n","__________________\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-fbe1928d4837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m#model = textCNN(sequence_model, sequence_hidden_unit, embed_name, ner_representation_limit)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             model = proposedmodel(sequence_model, sequence_hidden_unit, \n\u001b[0;32m---> 87\u001b[0;31m                                embed_name, ner_representation_limit,filter_number)\n\u001b[0m\u001b[1;32m     88\u001b[0m             model.fit([x_train_lstm, x_train_ner], y_train[each_problem], epochs=num_epoch, verbose=1, \n\u001b[1;32m     89\u001b[0m                       validation_data=([x_dev_lstm, x_dev_ner], y_dev[each_problem]), callbacks=callbacks, batch_size=batch_size)\n","\u001b[0;32m<ipython-input-10-c79650817411>\u001b[0m in \u001b[0;36mproposedmodel\u001b[0;34m(layer_name, number_of_unit, embedding_name, ner_limit, num_filter)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0minput_dimension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0msequence_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m104\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0minput_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cnn_input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[1;32m    174\u001b[0m                              \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                              input_tensor=tensor)\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'input'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \"\"\"\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'"]}],"source":["\n","#x_train_lstm = tf.convert_to_tensor(x_train_lstm, dtype=tf.float32)\n","#x_dev_lstm = tf.convert_to_tensor(x_dev_lstm, dtype=tf.float32)\n","#x_test_lstm = tf.convert_to_tensor(x_test_lstm, dtype=tf.float32)\n","\n","#y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n","#y_dev = tf.convert_to_tensor(y_dev, dtype=tf.float32)\n","#y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n","\n","#embedding_types = ['word2vec', 'fasttext', 'concat']\n","#embedding_dict = [ner_word2vec, ner_fasttext, ner_concat]\n","embedding_dict = [ner_word2vec, ner_fasttext]\n","embedding_types = ['word2vec', 'fasttext']\n","\n","#target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n","target_problems = [1, 2, 3, 4]\n","\n","num_epoch = 1\n","model_patience = 5\n","monitor_criteria = 'val_loss'\n","#monitor_criteria = 'val_acc'\n","batch_size = 64\n","\n","filter_number = 32\n","ner_representation_limit = 64\n","activation_func = \"relu\"\n","\n","sequence_model = \"GRU\"\n","sequence_hidden_unit = 256\n","\n","maxiter = 2\n","for embed_dict, embed_name in zip(embedding_dict, embedding_types):    \n","    print (\"Embedding: \", embed_name)\n","    print(\"=============================\")\n","    \n","    temp_train_ner = dict((k, embed_dict[k]) for k in train_ids)\n","    tem_dev_ner = dict((k, embed_dict[k]) for k in dev_ids)\n","    temp_test_ner = dict((k, embed_dict[k]) for k in test_ids)\n","\n","    x_train_dict = {}\n","    x_dev_dict = {}\n","    x_test_dict = {}\n","\n","    x_train_dict = get_subvector_data(ner_representation_limit, embed_name, temp_train_ner)\n","    x_dev_dict = get_subvector_data(ner_representation_limit, embed_name, tem_dev_ner)\n","    x_test_dict = get_subvector_data(ner_representation_limit, embed_name, temp_test_ner)\n","\n","    x_train_dict_sorted = collections.OrderedDict(sorted(x_train_dict.items()))\n","    x_dev_dict_sorted = collections.OrderedDict(sorted(x_dev_dict.items()))\n","    x_test_dict_sorted = collections.OrderedDict(sorted(x_test_dict.items()))\n","\n","    #x_train_ner = np.asarray(x_train_dict_sorted.values()).astype(np.float32)\n","    #x_dev_ner = np.asarray(x_dev_dict_sorted.values()).astype(np.float32)\n","    #x_test_ner = np.asarray(x_test_dict_sorted.values()).astype(np.float32)\n","\n","    x_train_ner = np.asarray(x_train_dict_sorted.values())\n","    x_dev_ner = np.asarray(x_dev_dict_sorted.values())\n","    x_test_ner = np.asarray(x_test_dict_sorted.values())\n","\n","    #x_train_ner = tf.convert_to_tensor(x_train_ner, dtype=tf.float32)\n","    #x_test_ner = tf.convert_to_tensor(x_test_ner, dtype=tf.float32)\n","        \n","    for iteration in range(1,maxiter):\n","        print (\"Iteration number: \", iteration)\n","    \n","        for each_problem in target_problems:\n","            print (\"Problem type: \", each_problem)\n","            print (\"__________________\")\n","            \n","            \n","            early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n","            \n","            best_model_name = str(ner_representation_limit)+\"-basiccnn1d-\"+str(embed_name)+\"-\"+str(each_problem)+\"-\"+\"best_model.hdf5\"\n","            \n","            checkpoint = ModelCheckpoint(best_model_name, monitor=monitor_criteria, verbose=1,\n","                save_best_only=True, mode='min')\n","            \n","            reduce_lr = ReduceLROnPlateau(monitor=monitor_criteria, factor=0.2,\n","                              patience=2, min_lr=0.00001, epsilon=1e-4, mode='min')\n","            \n","\n","            #callbacks = [early_stopping_monitor, checkpoint, reduce_lr]\n","            callbacks = [early_stopping_monitor, checkpoint]\n","            \n","            #model = textCNN(sequence_model, sequence_hidden_unit, embed_name, ner_representation_limit)\n","            model = proposedmodel(sequence_model, sequence_hidden_unit, \n","                               embed_name, ner_representation_limit,filter_number)\n","            model.fit([x_train_lstm, x_train_ner], y_train[each_problem], epochs=num_epoch, verbose=1, \n","                      validation_data=([x_dev_lstm, x_dev_ner], y_dev[each_problem]), callbacks=callbacks, batch_size=batch_size)\n","            \n","            \n","            probs, predictions = make_prediction_cnn(model, [x_test_lstm, x_test_ner])\n","            print_scores_cnn(predictions, probs, y_test[each_problem], embed_name, each_problem, iteration, sequence_hidden_unit)\n","            \n","            model.load_weights(best_model_name)\n","                      \n","            probs, predictions = make_prediction_cnn(model, [x_test_lstm, x_test_ner])\n","            save_scores_cnn(predictions, probs, y_test[each_problem], embed_name, each_problem, iteration,\n","                            sequence_hidden_unit, sequence_model, type_of_ner)\n","            del model\n","            clear_session()\n","            gc.collect()\n","            \n","    "]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"09-Proposed-Model_Updated.ipynb","provenance":[{"file_id":"1AyMYupSP7erZDh47MeqMMKANk199OGav","timestamp":1650816923171}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}